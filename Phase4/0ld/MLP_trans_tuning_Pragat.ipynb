{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0AG1yF50c0N"
   },
   "source": [
    "# Collinearity Reducer HCDR Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vW-DkS2mTZ69",
    "outputId": "9b725044-9a1c-4c70-ba09-504535db20bb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lj/chp0k_rn1qqf1qs2mtg65xzw0000gn/T/ipykernel_65607/1408506528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDo68G3t0gxq"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4__MiCS81mGz"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-dlcHT1FvM7",
    "outputId": "0d16b99e-a2b4-4933-e98e-c91275cdd49d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/pragatwagle/opt/anaconda3/lib/python3.7/site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/pragatwagle/opt/anaconda3/lib/python3.7/site-packages (from torch) (4.1.1)\n",
      "cuda available?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "# system\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "# data engineering\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# deep learning\n",
    "!pip install torch\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"cuda available?\\n{torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9J-pHtE0jva"
   },
   "source": [
    "### Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "AjaTemryA3e5"
   },
   "outputs": [],
   "source": [
    "# transformer reduces the list of columns by a subset\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "# transformer produces a reduced column list by collinearity reduction\n",
    "class CollinearityReducer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    '''\n",
    "    This class reduces features by measuring collinearity between the input variables and target.\n",
    "    Works on numerical features based on the correlations between each variable pair.\n",
    "    Of the var1iable pairs with absolute correlations above the threshold value...\n",
    "    ...the variables with the lowest target variable correlation are dropped from the input X.\n",
    "    The process is repeated until there are no more colinear pairs with absolute correlations above the threshold.\n",
    "    ...Or max_iter. \n",
    "    \n",
    "    The transformation returns a subset of feature names... \n",
    "    ...to be used with the DataFrameSelector() Class. \n",
    "\n",
    "    This class is meant to be run at the end of the numerical pipeline\n",
    "    PRIOR TO THE ACTUAL PIPELINE - only returns subset for DataFrameSelector().\n",
    "\n",
    "    NOTE! The function receives a dataframe structured with the target variable in first column.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, attribute_names, threshold=0.5, max_iter=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.threshold = threshold\n",
    "        self.max_iter = max_iter\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None): \n",
    "\n",
    "        # TODO: address dataframe error when used with pytorch model       \n",
    "        dataframe = pd.concat([y, pd.DataFrame(X)], axis=1)\n",
    "        \n",
    "        i = 0\n",
    "        while i <= self.max_iter:\n",
    "\n",
    "            # read-in and assign columns\n",
    "            # gets correlation matrix between variables and pivots to a longer df\n",
    "            # identify target variable\n",
    "            # drop same-name and target correlations pairs\n",
    "              \n",
    "            df = dataframe\n",
    "            features = df.iloc[:,1:].columns\n",
    "            target_name = df.iloc[:,0].name\n",
    "\n",
    "            df = pd.melt(abs(df.corr()).reset_index(), id_vars='index', value_vars=features)\n",
    "            targets = df[df['index']==target_name]\n",
    "            df = df[(df['index'] != df['variable']) & (df['index'] != target_name) & (df['variable'] != target_name)]\n",
    "\n",
    "            # combine the correlated variables into ordered pairs\n",
    "            # aggregate the max correlation and sort pairs\n",
    "            # split out the variables from the pair\n",
    "            # join the target variable correlations for each variable pair, rename columns\n",
    "\n",
    "            df['joined'] = df[['index', 'variable']].apply(lambda row: '::'.join(np.sort(row.values.astype(str))), axis=1)\n",
    "\n",
    "            df = df.groupby('joined', as_index=False) \\\n",
    "                   .agg({'value':'max'}) \\\n",
    "                   .sort_values(by='value', ascending=False)\n",
    "\n",
    "            df[['var_1','var_2']] = df['joined'].str.split(\"::\",expand=True).astype(int)\n",
    "\n",
    "            df = df.merge(targets, how='left', left_on='var_1', right_on='variable') \\\n",
    "                   .merge(targets, how='left', left_on='var_2', right_on='variable')\n",
    "            df.rename(columns = {'value_x':'var_pair_corr', 'value_y':'var_1_target_corr', 'value':'var_2_target_corr'}, inplace = True)\n",
    "\n",
    "            # This section takes all variable pairs with a correlation greater than threshold\n",
    "            # tests to determine which variable has a higher correlation with the target.\n",
    "            # The higher of the two gets marked as a win\n",
    "            # While the other gets marked as a loss\n",
    "            # the wins and losses for each variable are then grouped and summed\n",
    "\n",
    "            exceeds = df[df['var_pair_corr']>self.threshold]\n",
    "\n",
    "            # break if none above threshold\n",
    "            if len(exceeds['var_pair_corr'])==0:\n",
    "                break\n",
    "\n",
    "            # \"correlation competition\"\n",
    "            exceeds['var_1_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] >= row[\"var_2_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_1_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] >= row[\"var_1_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_2_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] < row[\"var_2_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_2_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] < row[\"var_1_target_corr\"] else 0, axis=1)\n",
    "\n",
    "            # aggregate scores\n",
    "            var1 = exceeds[['var_1', 'var_1_win', 'var_1_loss']].groupby('var_1', as_index=False) \\\n",
    "                                                                .agg({'var_1_win':'sum', 'var_1_loss':'sum'})\n",
    "            var1.rename(columns = {'var_1':'var', 'var_1_win':'win', 'var_1_loss':'loss'}, inplace=True)\n",
    "\n",
    "            var2 = exceeds[['var_2', 'var_2_win', 'var_2_loss']].groupby('var_2', as_index=False) \\\n",
    "                                                                .agg({'var_2_win':'sum', 'var_2_loss':'sum'})\n",
    "            var2.rename(columns = {'var_2':'var', 'var_2_win':'win', 'var_2_loss':'loss'}, inplace=True)\n",
    "\n",
    "            corrcomps = pd.concat([var1,var2], axis=0).groupby('var', as_index=False) \\\n",
    "                                                      .agg({'win':'sum', 'loss':'sum'})\n",
    "\n",
    "            # drop variables which had 0 wins - IE collinear variables which were always least related to the target\n",
    "            dropvars = corrcomps[corrcomps['win']==0]['var']\n",
    "\n",
    "            dataframe = dataframe.drop(dropvars, axis=1)  \n",
    "\n",
    "            i += 1  \n",
    "        \n",
    "        X = [self.attribute_names[col] for col in dataframe.columns]\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvEZVk640s23"
   },
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "70zdSCUMA30F"
   },
   "outputs": [],
   "source": [
    "# function identifies missing data\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) \n",
    "\n",
    "\n",
    "# function to identify different feature types and summary EDA\n",
    "def id_num_cat_feature(df,text = True):\n",
    "    numerical = df.select_dtypes(include=np.number).columns\n",
    "    categorical = df.select_dtypes(include=['object', 'bool', 'category']).columns\n",
    "    feat_num = list(numerical)\n",
    "    feat_cat = list(categorical)\n",
    "    \n",
    "    id_cols = ['SK_ID_CURR','SK_ID_BUREAU']\n",
    "    \n",
    "    id_cols = [cols for cols in  list(df.columns.intersection(id_cols))] \n",
    "    features = list(set(df.columns) - set(id_cols))\n",
    "\n",
    "    if text == True:\n",
    "          # print eda\n",
    "        print('--------')\n",
    "        print(f\"# of ID's: {len(id_cols)}\")\n",
    "        print(f\" ID's:\")\n",
    "        print(id_cols)\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# All features: {len(features)}\")\n",
    "        print(f\"All features:\")\n",
    "        print(features)\n",
    "        print('')\n",
    "        print(f\"Missing data:\")\n",
    "        print(missing_data(df[features]))\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# of Numerical features: {len(feat_num)}\")\n",
    "        print(f\"Numerical features:\")\n",
    "        print(feat_num)\n",
    "        print('')\n",
    "        print(f\"Numerical Statistical Summary:\")\n",
    "        print('')\n",
    "        print(df[feat_num].describe())\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# of Categorical features: {len(feat_cat)}\")\n",
    "        print(f\"Categorical features:\")\n",
    "        print(feat_cat)\n",
    "        print('')\n",
    "        print(f\"Categorical Statistical Summary:\")\n",
    "        print('')\n",
    "        #print(df[feat_cat].describe(include='all'))\n",
    "        print('')\n",
    "        print(\"Categories:\")\n",
    "        print('')\n",
    "        print(df[feat_cat].apply(lambda col: col.unique()))\n",
    "        print('')\n",
    "        print('--------')\n",
    "        \n",
    "    return id_cols,feat_num,feat_cat,features\n",
    "\n",
    "\n",
    "# https://pythonsimplified.com/how-to-handle-large-datasets-in-python-with-pandas/\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**3\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**3\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "Tu_kiwglMHQa"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_hcdr_model(\n",
    "    hidden_layer_neurons=[32, 16, 8],\n",
    "    opt=optim.SGD,\n",
    "    epochs=5,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    \n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 2  # Output layer neurons - depend on what you're trying to predict, here, 2 classes: 0 and 1\n",
    "    \n",
    "    str_neurons = [str(h) for h in hidden_layer_neurons]\n",
    "    arch_string = f\"{D_in}-{'-'.join(str_neurons)}-{D_out}\"\n",
    "    \n",
    "    layers = [\n",
    "        torch.nn.Linear(D_in, hidden_layer_neurons[0]),  # X.matmul(W1)\n",
    "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
    "    ]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i in range(1, len(hidden_layer_neurons)):\n",
    "        prev, curr = hidden_layer_neurons[i - 1], hidden_layer_neurons[i]\n",
    "        layers.append(torch.nn.Linear(prev, curr))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "    \n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(hidden_layer_neurons[-1], D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
    "    \n",
    "    # Use the nn package to define our model and loss function.\n",
    "    # use the sequential API makes things simple\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # use Cross Entropy and SGD optimizer.\n",
    "    loss_fn = nn.CrossEntropyLoss()  #for classfication \n",
    "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #summary(model, (4, 20))\n",
    "    print('-'*50)\n",
    "    print('Model:')\n",
    "    print(model)\n",
    "    print('-'*50)\n",
    "    \n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        y_prob = []\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        # dataset API gives us pythonic batching \n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            # https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "            inputs, target = data[0], data[1]\n",
    "            target = target.type(torch.LongTensor)   # casting to long\n",
    "            inputs, target = inputs.to(device), target.to(device)        \n",
    "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = model(inputs.float()) #prediction over the input data\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(preds, target)    #mean loss for this batch\n",
    "\n",
    "            loss.backward() #calculate nabla_w\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()  #update W\n",
    "            \n",
    "            # https://stackoverflow.com/questions/60182984/how-to-get-the-predict-probability\n",
    "            y_prob.extend(F.softmax(preds, dim=1)[:,1].tolist())\n",
    "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        loss = np.round(running_loss/count, 3)\n",
    "        \n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum() / correct.size\n",
    "        accuracy = np.round(accuracy, 3)\n",
    "        \n",
    "        # auc\n",
    "        roc_auc = roc_auc_score(np.array(epoch_target),np.array(y_prob))\n",
    "        roc_auc = np.round(roc_auc,3)\n",
    "                           \n",
    "        return loss, accuracy, roc_auc\n",
    "\n",
    "\n",
    "\n",
    "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "        overall_loss = 0.0\n",
    "        count = 0\n",
    "        y_prob = []\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        for i,data in enumerate(data_loader):\n",
    "            # https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "            inputs, target = data[0], data[1]\n",
    "            target = target.type(torch.LongTensor)   # casting to long\n",
    "            inputs, target = inputs.to(device), target.to(device)                \n",
    "            preds = model(inputs.float())      \n",
    "\n",
    "            loss = loss_fn(preds, target)           # compute loss value\n",
    "\n",
    "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
    "            \n",
    "            y_prob.extend(F.softmax(preds, dim=1)[:,1].tolist())\n",
    "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            count += 1\n",
    "\n",
    "        # compute mean loss\n",
    "        loss = np.round(overall_loss/count, 3)\n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum() / correct.size\n",
    "        accuracy = np.round(accuracy, 3)\n",
    "\n",
    "        # auc\n",
    "        roc_auc = roc_auc_score(np.array(epoch_target),np.array(y_prob))\n",
    "        roc_auc = np.round(roc_auc,3)\n",
    "\n",
    "        return loss, accuracy, roc_auc\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):      \n",
    "        train_loss, train_accuracy, train_auc = train_epoch(epoch, model, loss_fn, optimizer, trainloader_hcdr)\n",
    "        valid_loss, valid_accuracy, valid_auc = evaluate_model(epoch, model, loss_fn, optimizer, validloader_hcdr, tag = \"Validation\")\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"----Train Accuracy: {train_accuracy}\\t Validation Accuracy: {valid_accuracy}\")\n",
    "        print(f\"----Train AUC-ROC: {train_auc}\\t Validation AUC-ROC: {valid_auc}\")\n",
    "    print(\"-\"*50)\n",
    "    test_loss, test_accuracy, test_auc = evaluate_model(epoch, model, loss_fn, opt, testloader_hcdr, tag=\"Test\")\n",
    "    \n",
    "    return arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PcEXb_u1uJ2p"
   },
   "outputs": [],
   "source": [
    "#hidden_layer_neurons=[265, 80, 32, 16, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY-aqM5W04hl"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ8dzHAt1pvM"
   },
   "source": [
    "### Read-In and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWvkmBxpA32r",
    "outputId": "b6e06d39-f8ff-472b-b68e-1937af07ecd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "application_test_eng\n",
      "---\n",
      "prevapp_agg_data_tr\n",
      "---\n",
      "bureau_agg_data_transformed\n",
      "---\n",
      "ccb_agg_data_tr\n",
      "---\n",
      "ip_agg_data_tr\n",
      "---\n",
      "pos_agg_data_tr\n"
     ]
    }
   ],
   "source": [
    "# read-in\n",
    "DATA_DIR =  \"/../Data\"\n",
    "\n",
    "ds_names = (\n",
    "    \"application_test_eng\",   \n",
    "  #  \"application_train_eng\", \n",
    "    \"prevapp_agg_data_tr\", \"bureau_agg_data_transformed\",  \n",
    "    \"ccb_agg_data_tr\", \"ip_agg_data_tr\", \"pos_agg_data_tr\"\n",
    ")  \n",
    "\n",
    "datasets_agg = {}\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    print('---')\n",
    "    print(ds_name)\n",
    "    datasets_agg[ds_name] = pd.read_csv(os.getcwd() + DATA_DIR + \"/\" + f'{ds_name}.csv')\n",
    "    #datasets_agg[ds_name] = reduce_mem_usage(datasets_agg[ds_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kAs9q2XTUrqA"
   },
   "outputs": [],
   "source": [
    "# denormalize and clean text\n",
    "for ds_name in datasets_agg:\n",
    "    if ds_name == 'application_train_eng':\n",
    "        agg_data = datasets_agg['application_train_eng'].replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\/', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\(', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\)', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\:', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\,', value='', regex=True)\n",
    "    else:\n",
    "        agg_data = agg_data.merge(datasets_agg[ds_name], on='SK_ID_CURR', how='left')\n",
    "\n",
    "\n",
    "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('Unnamed:')]\n",
    "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('SK_ID_PREV')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data.to_csv(os.getcwd() + DATA_DIR + '/' + 'agg_data_4_eng_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalize and clean text\n",
    "for ds_name in datasets_agg:\n",
    "    if ds_name == 'application_test_eng':\n",
    "        agg_data = datasets_agg['application_test_eng'].replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\/', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\(', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\)', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\:', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\,', value='', regex=True)\n",
    "    else:\n",
    "        agg_data = agg_data.merge(datasets_agg[ds_name], on='SK_ID_CURR', how='left')\n",
    "\n",
    "\n",
    "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('Unnamed:')]\n",
    "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('SK_ID_PREV')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data.to_csv(os.getcwd() + DATA_DIR + '/' +  'agg_data_eng_tr_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pragatwagle/Desktop/AML556FinalProjectFall2022/Phase4/../Data/agg_data_eng_tr_test.csv\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd() + DATA_DIR + '/' + 'agg_data_eng_tr_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in\n",
    "DATA_DIR =  \"/../Data\"\n",
    "datasets_agg = {}\n",
    "datasets_agg[\"agg_data_eng_tr\"] = pd.read_csv(os.getcwd() + DATA_DIR + \"/\" +f'agg_data_eng_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = datasets_agg[\"agg_data_eng_tr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7gyiRiE1JQC"
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0v9NqWR1SgS"
   },
   "source": [
    "### Setup and Collinearity Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-VtWvcq3773",
    "outputId": "f09ce714-15eb-417d-b26c-8bac24d2949c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train           shape: (118084, 1428)\n",
      "X validation      shape: (29521, 1428)\n",
      "X test            shape: (36902, 1428)\n"
     ]
    }
   ],
   "source": [
    "# deep learning model\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create train, validation, and test sets\n",
    "y = agg_data['TARGET']\n",
    "X = agg_data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
    "\n",
    "_, X, _, y = train_test_split(X, y, test_size=0.8, random_state=42, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")\n",
    "\n",
    "# determine feature types\n",
    "id_col, feat_num, feat_cat, feature =  id_num_cat_feature(X, text = False)\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "### Collinear Feature Reduction\n",
    "\n",
    "# reduce numerical features by collinearity reduction\n",
    "\n",
    "# cr = make_pipeline(\n",
    "#     SimpleImputer(strategy='median'),\n",
    "#     StandardScaler(),    \n",
    "#     CollinearityReducer(attribute_names=feat_num, threshold = 0.5, max_iter=25)\n",
    "# )\n",
    "\n",
    "# tic = time.perf_counter()\n",
    "# reduced_feat_num = cr.fit_transform(X_train[feat_num], y_train) \n",
    "# toc = time.perf_counter()\n",
    "\n",
    "# print(f\"Collinearity Reduction completed in {toc - tic:0.4f} seconds.\")\n",
    "# print(f'Reduced numerical column count by {len(reduced_feat_num)/len(feat_num)}% through collinearity reduction.')\n",
    "# print(f'From {len(feat_num)} columns to {len(reduced_feat_num)} columns.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAqUW765mIe-"
   },
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6rXxnMl94W",
    "outputId": "2d2c8dce-f143-4bdd-ba0c-9ca03ccd4dfd"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mall_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6f/zmcxlrs12vd7qxtgld2cqcf80000gn/T/ipykernel_61937/3132334404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Transform validation set with the same constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Transform test set with the same constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mtransformer_to_input_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             raise ValueError(\n\u001b[0;32m--> 412\u001b[0;31m                 \u001b[0;34m\"Specifying the columns using strings is only \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0;34m\"supported for pandas DataFrames\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "### Main Pipeline\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    # ('selector', DataFrameSelector(reduced_feat_num)),\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "data_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, feat_num),\n",
    "        (\"cat_pipeline\", cat_pipeline, feat_cat)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "X_train = data_pipeline.fit_transform(X_train)\n",
    "X_valid = data_pipeline.transform(X_valid) #Transform validation set with the same constants\n",
    "X_test = data_pipeline.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_valid = y_valid.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_valid_tensor = torch.from_numpy(X_valid)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_valid_tensor = torch.from_numpy(y_valid)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "hcdr_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "hcdr_valid = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "hcdr_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YR76icicV_9P",
    "outputId": "2aea49c9-0d31-4e6b-c83f-da6c7b95a85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1545, out_features=265, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=265, out_features=120, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=60, out_features=2, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
      "----Train AUC-ROC: 0.722\t Validation AUC-ROC: 0.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
      "----Train AUC-ROC: 0.756\t Validation AUC-ROC: 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
      "----Train AUC-ROC: 0.767\t Validation AUC-ROC: 0.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
      "----Train AUC-ROC: 0.773\t Validation AUC-ROC: 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
      "----Train AUC-ROC: 0.773\t Validation AUC-ROC: 0.759\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(input_dataset) - description of input data: size_set_transformed_columns_CR\n",
    "(hidden_layers_neurons) - A list of the number of neurons in the hidden layers in order. DEFAULT: [32, 16, 8] => 1st hidden layer: 32 neurons, 2nd: 16, 3rd: 8\n",
    "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "(learning_rate) - The learning rate to take the gradient descent step with\n",
    "'''\n",
    "input_dataset=\"0.6_agg_trans\"\n",
    "hidden_layer_neurons=[265, 120,60]\n",
    "opt=optim.Adam\n",
    "epochs=5\n",
    "learning_rate=5e-3\n",
    "\n",
    "train_batch_size = 76\n",
    "valid_test_batch_size = 64\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "\n",
    "\n",
    "# create dataloader\n",
    "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
    "trainloader_hcdr = torch.utils.data.DataLoader(hcdr_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "validloader_hcdr = torch.utils.data.DataLoader(hcdr_valid, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_hcdr = torch.utils.data.DataLoader(hcdr_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc = run_hcdr_model(\n",
    "    hidden_layer_neurons,\n",
    "    opt,\n",
    "    epochs,\n",
    "    learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcEXb_u1uJ2p"
   },
   "outputs": [],
   "source": [
    "#hidden_layer_neurons=[265, 80, 32, 16, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "BrkIsbVX_0zs"
   },
   "outputs": [],
   "source": [
    "try: hcdrLog2 \n",
    "except : hcdrLog2 = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Dataset\",\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Learning Rate\",\n",
    "        \"Train accuracy\",\n",
    "        \"Valid accuracy\",\n",
    "        \"Test accuracy\",\n",
    "        \"Train auc\",\n",
    "        \"Valid auc\",\n",
    "        \"Test auc\",\n",
    "        \"Train_Batch Size\",\n",
    "        \"Valid Batch Size\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "hcdrLog2.loc[len(hcdrLog2)] = [\n",
    "    input_dataset,\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{learning_rate}\",\n",
    "    f\"{train_accuracy * 100}%\",\n",
    "    f\"{valid_accuracy * 100}%\",\n",
    "    f\"{test_accuracy * 100}%\",\n",
    "    f\"{train_auc * 100}%\",\n",
    "    f\"{valid_auc * 100}%\",\n",
    "    f\"{test_auc * 100}%\",\n",
    "    f\"{train_batch_size}\",\n",
    "   f\"{valid_test_batch_size}\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "DeGGB6KUnsD_",
    "outputId": "e9cda9ed-2b19-42b4-95ce-f87a704d39d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Valid accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>Train auc</th>\n",
       "      <th>Valid auc</th>\n",
       "      <th>Test auc</th>\n",
       "      <th>Train_Batch Size</th>\n",
       "      <th>Valid Batch Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-250-64-32-16-4-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>92.0%</td>\n",
       "      <td>91.8%</td>\n",
       "      <td>91.8%</td>\n",
       "      <td>79.10000000000001%</td>\n",
       "      <td>76.2%</td>\n",
       "      <td>76.3%</td>\n",
       "      <td>96</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-250-64-32-16-4-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>78.10000000000001%</td>\n",
       "      <td>76.0%</td>\n",
       "      <td>76.0%</td>\n",
       "      <td>50</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-100-64-32-16-4-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>78.5%</td>\n",
       "      <td>76.1%</td>\n",
       "      <td>75.8%</td>\n",
       "      <td>50</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-250-64-32-16-4-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>79.0%</td>\n",
       "      <td>76.2%</td>\n",
       "      <td>76.2%</td>\n",
       "      <td>76</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-300-64-32-16-4-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>78.5%</td>\n",
       "      <td>75.9%</td>\n",
       "      <td>76.0%</td>\n",
       "      <td>76</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-265-64-32-16-4-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>79.0%</td>\n",
       "      <td>76.1%</td>\n",
       "      <td>76.1%</td>\n",
       "      <td>76</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-265-80-32-16-4-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>79.10000000000001%</td>\n",
       "      <td>76.0%</td>\n",
       "      <td>76.4%</td>\n",
       "      <td>76</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-265-120-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>78.4%</td>\n",
       "      <td>74.7%</td>\n",
       "      <td>74.8%</td>\n",
       "      <td>76</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6_agg_trans</td>\n",
       "      <td>1545-265-120-60-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>91.9%</td>\n",
       "      <td>77.3%</td>\n",
       "      <td>75.9%</td>\n",
       "      <td>75.9%</td>\n",
       "      <td>76</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Dataset    Architecture string                        Optimizer  \\\n",
       "0  0.6_agg_trans  1545-250-64-32-16-4-2  <class 'torch.optim.adam.Adam'>   \n",
       "1  0.6_agg_trans  1545-250-64-32-16-4-2  <class 'torch.optim.adam.Adam'>   \n",
       "2  0.6_agg_trans  1545-100-64-32-16-4-2  <class 'torch.optim.adam.Adam'>   \n",
       "3  0.6_agg_trans  1545-250-64-32-16-4-2  <class 'torch.optim.adam.Adam'>   \n",
       "4  0.6_agg_trans  1545-300-64-32-16-4-2  <class 'torch.optim.adam.Adam'>   \n",
       "5  0.6_agg_trans  1545-265-64-32-16-4-2  <class 'torch.optim.adam.Adam'>   \n",
       "6  0.6_agg_trans  1545-265-80-32-16-4-2  <class 'torch.optim.adam.Adam'>   \n",
       "7  0.6_agg_trans         1545-265-120-2  <class 'torch.optim.adam.Adam'>   \n",
       "8  0.6_agg_trans      1545-265-120-60-2  <class 'torch.optim.adam.Adam'>   \n",
       "\n",
       "  Epochs Learning Rate Train accuracy Valid accuracy Test accuracy  \\\n",
       "0      5         0.005          92.0%          91.8%         91.8%   \n",
       "1      5         0.005          91.9%          91.9%         91.9%   \n",
       "2      5         0.005          91.9%          91.9%         91.9%   \n",
       "3      5         0.005          91.9%          91.9%         91.9%   \n",
       "4      5         0.005          91.9%          91.9%         91.9%   \n",
       "5      5         0.005          91.9%          91.9%         91.9%   \n",
       "6      5         0.005          91.9%          91.9%         91.9%   \n",
       "7      5         0.005          91.9%          91.9%         91.9%   \n",
       "8      5         0.005          91.9%          91.9%         91.9%   \n",
       "\n",
       "            Train auc Valid auc Test auc Train_Batch Size Valid Batch Size  \n",
       "0  79.10000000000001%     76.2%    76.3%               96               64  \n",
       "1  78.10000000000001%     76.0%    76.0%               50               64  \n",
       "2               78.5%     76.1%    75.8%               50               64  \n",
       "3               79.0%     76.2%    76.2%               76               64  \n",
       "4               78.5%     75.9%    76.0%               76               64  \n",
       "5               79.0%     76.1%    76.1%               76               64  \n",
       "6  79.10000000000001%     76.0%    76.4%               76               64  \n",
       "7               78.4%     74.7%    74.8%               76               64  \n",
       "8               77.3%     75.9%    75.9%               76               64  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcdrLog2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PcEXb_u1uJ2p"
   },
   "outputs": [],
   "source": [
    "#hidden_layer_neurons=[265, 80, 32, 16, 4]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
