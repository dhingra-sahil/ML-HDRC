{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0AG1yF50c0N"
   },
   "source": [
    "# HCDR Pipeline - Phase 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDo68G3t0gxq"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4__MiCS81mGz"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-dlcHT1FvM7",
    "outputId": "7e913ed8-1f32-4889-e160-aca53eb7ede8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "# system\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "import pickle\n",
    "\n",
    "# data engineering\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"cuda available?\\n{torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9J-pHtE0jva"
   },
   "source": [
    "### Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AjaTemryA3e5"
   },
   "outputs": [],
   "source": [
    "# transformer reduces the list of columns by a subset\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "# transformer produces a reduced column list by collinearity reduction\n",
    "class CollinearityReducer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    '''\n",
    "    This class reduces features by measuring collinearity between the input variables and target.\n",
    "    Works on numerical features based on the correlations between each variable pair.\n",
    "    Of the var1iable pairs with absolute correlations above the threshold value...\n",
    "    ...the variables with the lowest target variable correlation are dropped from the input X.\n",
    "    The process is repeated until there are no more colinear pairs with absolute correlations above the threshold.\n",
    "    ...Or max_iter. \n",
    "    \n",
    "    The transformation returns a subset of feature names... \n",
    "    ...to be used with the DataFrameSelector() Class. \n",
    "\n",
    "    This class is meant to be run at the end of the numerical pipeline\n",
    "    PRIOR TO THE ACTUAL PIPELINE - only returns subset for DataFrameSelector().\n",
    "\n",
    "    NOTE! The function receives a dataframe structured with the target variable in first column.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, attribute_names, threshold=0.5, max_iter=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.threshold = threshold\n",
    "        self.max_iter = max_iter\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None): \n",
    "\n",
    "        # TODO: address dataframe error when used with pytorch model       \n",
    "        dataframe = pd.concat([y, pd.DataFrame(X)], axis=1)\n",
    "        \n",
    "        i = 0\n",
    "        while i <= self.max_iter:\n",
    "\n",
    "            # read-in and assign columns\n",
    "            # gets correlation matrix between variables and pivots to a longer df\n",
    "            # identify target variable\n",
    "            # drop same-name and target correlations pairs\n",
    "              \n",
    "            df = dataframe\n",
    "            features = df.iloc[:,1:].columns\n",
    "            target_name = df.iloc[:,0].name\n",
    "\n",
    "            df = pd.melt(abs(df.corr()).reset_index(), id_vars='index', value_vars=features)\n",
    "            targets = df[df['index']==target_name]\n",
    "            df = df[(df['index'] != df['variable']) & (df['index'] != target_name) & (df['variable'] != target_name)]\n",
    "\n",
    "            # combine the correlated variables into ordered pairs\n",
    "            # aggregate the max correlation and sort pairs\n",
    "            # split out the variables from the pair\n",
    "            # join the target variable correlations for each variable pair, rename columns\n",
    "\n",
    "            df['joined'] = df[['index', 'variable']].apply(lambda row: '::'.join(np.sort(row.values.astype(str))), axis=1)\n",
    "\n",
    "            df = df.groupby('joined', as_index=False) \\\n",
    "                   .agg({'value':'max'}) \\\n",
    "                   .sort_values(by='value', ascending=False)\n",
    "\n",
    "            df[['var_1','var_2']] = df['joined'].str.split(\"::\",expand=True).astype(int)\n",
    "\n",
    "            df = df.merge(targets, how='left', left_on='var_1', right_on='variable') \\\n",
    "                   .merge(targets, how='left', left_on='var_2', right_on='variable')\n",
    "            df.rename(columns = {'value_x':'var_pair_corr', 'value_y':'var_1_target_corr', 'value':'var_2_target_corr'}, inplace = True)\n",
    "\n",
    "            # This section takes all variable pairs with a correlation greater than threshold\n",
    "            # tests to determine which variable has a higher correlation with the target.\n",
    "            # The higher of the two gets marked as a win\n",
    "            # While the other gets marked as a loss\n",
    "            # the wins and losses for each variable are then grouped and summed\n",
    "\n",
    "            exceeds = df[df['var_pair_corr']>self.threshold]\n",
    "\n",
    "            # break if none above threshold\n",
    "            if len(exceeds['var_pair_corr'])==0:\n",
    "                break\n",
    "\n",
    "            # \"correlation competition\"\n",
    "            exceeds['var_1_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] >= row[\"var_2_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_1_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] >= row[\"var_1_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_2_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] < row[\"var_2_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_2_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] < row[\"var_1_target_corr\"] else 0, axis=1)\n",
    "\n",
    "            # aggregate scores\n",
    "            var1 = exceeds[['var_1', 'var_1_win', 'var_1_loss']].groupby('var_1', as_index=False) \\\n",
    "                                                                .agg({'var_1_win':'sum', 'var_1_loss':'sum'})\n",
    "            var1.rename(columns = {'var_1':'var', 'var_1_win':'win', 'var_1_loss':'loss'}, inplace=True)\n",
    "\n",
    "            var2 = exceeds[['var_2', 'var_2_win', 'var_2_loss']].groupby('var_2', as_index=False) \\\n",
    "                                                                .agg({'var_2_win':'sum', 'var_2_loss':'sum'})\n",
    "            var2.rename(columns = {'var_2':'var', 'var_2_win':'win', 'var_2_loss':'loss'}, inplace=True)\n",
    "\n",
    "            corrcomps = pd.concat([var1,var2], axis=0).groupby('var', as_index=False) \\\n",
    "                                                      .agg({'win':'sum', 'loss':'sum'})\n",
    "\n",
    "            # drop variables which had 0 wins - IE collinear variables which were always least related to the target\n",
    "            dropvars = corrcomps[corrcomps['win']==0]['var']\n",
    "\n",
    "            dataframe = dataframe.drop(dropvars, axis=1)  \n",
    "\n",
    "            i += 1  \n",
    "        \n",
    "        X = [self.attribute_names[col] for col in dataframe.columns]\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvEZVk640s23"
   },
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "70zdSCUMA30F"
   },
   "outputs": [],
   "source": [
    "# function identifies missing data\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) \n",
    "\n",
    "\n",
    "# function to identify different feature types and summary EDA\n",
    "def id_num_cat_feature(df,text = True):\n",
    "    numerical = df.select_dtypes(include=np.number).columns\n",
    "    categorical = df.select_dtypes(include=['object', 'bool', 'category']).columns\n",
    "    feat_num = list(numerical)\n",
    "    feat_cat = list(categorical)\n",
    "    \n",
    "    id_cols = ['SK_ID_CURR','SK_ID_BUREAU']\n",
    "    \n",
    "    id_cols = [cols for cols in  list(df.columns.intersection(id_cols))] \n",
    "    features = list(set(df.columns) - set(id_cols))\n",
    "\n",
    "    if text == True:\n",
    "          # print eda\n",
    "        print('--------')\n",
    "        print(f\"# of ID's: {len(id_cols)}\")\n",
    "        print(f\" ID's:\")\n",
    "        print(id_cols)\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# All features: {len(features)}\")\n",
    "        print(f\"All features:\")\n",
    "        print(features)\n",
    "        print('')\n",
    "        print(f\"Missing data:\")\n",
    "        print(missing_data(df[features]))\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# of Numerical features: {len(feat_num)}\")\n",
    "        print(f\"Numerical features:\")\n",
    "        print(feat_num)\n",
    "        print('')\n",
    "        print(f\"Numerical Statistical Summary:\")\n",
    "        print('')\n",
    "        print(df[feat_num].describe())\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# of Categorical features: {len(feat_cat)}\")\n",
    "        print(f\"Categorical features:\")\n",
    "        print(feat_cat)\n",
    "        print('')\n",
    "        print(f\"Categorical Statistical Summary:\")\n",
    "        print('')\n",
    "        #print(df[feat_cat].describe(include='all'))\n",
    "        print('')\n",
    "        print(\"Categories:\")\n",
    "        print('')\n",
    "        print(df[feat_cat].apply(lambda col: col.unique()))\n",
    "        print('')\n",
    "        print('--------')\n",
    "        \n",
    "    return id_cols,feat_num,feat_cat,features\n",
    "\n",
    "\n",
    "# https://pythonsimplified.com/how-to-handle-large-datasets-in-python-with-pandas/\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**3\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**3\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Tu_kiwglMHQa"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_hcdr_model(\n",
    "    hidden_layer_neurons=[32, 16, 8],\n",
    "    opt=optim.SGD,\n",
    "    epochs=5,\n",
    "    learning_rate=4e-3,\n",
    "    return_model=False\n",
    "):\n",
    "    \n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 2  # Output layer neurons - depend on what you're trying to predict, here, 2 classes: 0 and 1\n",
    "    \n",
    "    str_neurons = [str(h) for h in hidden_layer_neurons]\n",
    "    arch_string = f\"{D_in}-{'-'.join(str_neurons)}-{D_out}\"\n",
    "    \n",
    "    layers = [\n",
    "        torch.nn.Linear(D_in, hidden_layer_neurons[0]),  # X.matmul(W1)\n",
    "        nn.ReLU()  # ReLU( X.matmul(W1))\n",
    "    ]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i in range(1, len(hidden_layer_neurons)):\n",
    "        prev, curr = hidden_layer_neurons[i - 1], hidden_layer_neurons[i]\n",
    "        if i%2==0:\n",
    "            layers.append(torch.nn.Linear(prev, curr))\n",
    "            layers.append(nn.ReLU())\n",
    "        else:\n",
    "            layers.append(torch.nn.Linear(prev, curr))\n",
    "            layers.append(nn.ReLU())\n",
    "    \n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(hidden_layer_neurons[-1], D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
    "    \n",
    "    # Use the nn package to define our model and loss function.\n",
    "    # use the sequential API makes things simple\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # use Cross Entropy and SGD optimizer.\n",
    "    loss_fn = nn.CrossEntropyLoss()  #for classficationÂ \n",
    "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #summary(model, (4, 20))\n",
    "    print('-'*50)\n",
    "    print('Model:')\n",
    "    print(model)\n",
    "    print('-'*50)\n",
    "    \n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        y_prob = []\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        # dataset API gives us pythonic batching \n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            # https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "            inputs, target = data[0], data[1]\n",
    "            target = target.type(torch.LongTensor)   # casting to long\n",
    "            inputs, target = inputs.to(device), target.to(device)        \n",
    "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = model(inputs.float()) #prediction over the input data\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(preds, target)    #mean loss for this batch\n",
    "\n",
    "            loss.backward() #calculate nabla_w\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()  #update W\n",
    "            \n",
    "            # https://stackoverflow.com/questions/60182984/how-to-get-the-predict-probability\n",
    "            y_prob.extend(F.softmax(preds, dim=1)[:,1].tolist())\n",
    "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        loss = np.round(running_loss/count, 3)\n",
    "        \n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum() / correct.size\n",
    "        accuracy = np.round(accuracy, 3)\n",
    "        \n",
    "        # auc\n",
    "        roc_auc = roc_auc_score(np.array(epoch_target),np.array(y_prob))\n",
    "        roc_auc = np.round(roc_auc,3)\n",
    "                           \n",
    "        return loss, accuracy, roc_auc\n",
    "\n",
    "\n",
    "\n",
    "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "        overall_loss = 0.0\n",
    "        count = 0\n",
    "        y_prob = []\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        for i,data in enumerate(data_loader):\n",
    "            # https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
    "            inputs, target = data[0], data[1]\n",
    "            target = target.type(torch.LongTensor)   # casting to long\n",
    "            inputs, target = inputs.to(device), target.to(device)                \n",
    "            preds = model(inputs.float())      \n",
    "\n",
    "            loss = loss_fn(preds, target)           # compute loss value\n",
    "\n",
    "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
    "            \n",
    "            y_prob.extend(F.softmax(preds, dim=1)[:,1].tolist())\n",
    "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            count += 1\n",
    "\n",
    "        # compute mean loss\n",
    "        loss = np.round(overall_loss/count, 3)\n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum() / correct.size\n",
    "        accuracy = np.round(accuracy, 3)\n",
    "\n",
    "        # auc\n",
    "        roc_auc = roc_auc_score(np.array(epoch_target),np.array(y_prob))\n",
    "        roc_auc = np.round(roc_auc,3)\n",
    "\n",
    "        # recall\n",
    "        recall = recall_score(np.array(epoch_target),np.array(y_pred),average='weighted')\n",
    "        recall = np.round(recall, 3)\n",
    "        \n",
    "        # balanced accuracy\n",
    "        balanced_acc = balanced_accuracy_score(np.array(epoch_target),np.array(y_pred))\n",
    "        balanced_acc = np.round(balanced_acc,3)\n",
    "        \n",
    "        return loss, accuracy, roc_auc, recall, balanced_acc\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):      \n",
    "        train_loss, train_accuracy, train_auc = train_epoch(epoch, model, loss_fn, optimizer, trainloader_hcdr)\n",
    "        valid_loss, valid_accuracy, valid_auc,_,_ = evaluate_model(epoch, model, loss_fn, optimizer, validloader_hcdr, tag = \"Validation\")\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"----Train Accuracy: {train_accuracy}\\t Validation Accuracy: {valid_accuracy}\")\n",
    "        print(f\"----Train AUC-ROC: {train_auc}\\t Validation AUC-ROC: {valid_auc}\")\n",
    "    print(\"-\"*50)\n",
    "    test_loss, test_accuracy, test_auc, recall, balanced_acc = evaluate_model(epoch, model, loss_fn, opt, testloader_hcdr, tag=\"Test\")\n",
    "    \n",
    "    if return_model:\n",
    "        return model, arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc, recall, balanced_acc\n",
    "    else:    \n",
    "        return arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc, recall, balanced_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY-aqM5W04hl"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ8dzHAt1pvM"
   },
   "source": [
    "### Read-In and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWvkmBxpA32r",
    "outputId": "1a7afae1-78c1-422d-d10b-bf86fb9d03d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "application_train\n",
      "Memory usage of dataframe is 0.28 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 79.2%\n",
      "---\n",
      "prevapp_agg_data_tr\n",
      "Memory usage of dataframe is 1.73 MB\n",
      "Memory usage after optimization is: 0.46 MB\n",
      "Decreased by 73.2%\n",
      "---\n",
      "bureau_agg_data_trans_untrans\n",
      "Memory usage of dataframe is 0.72 MB\n",
      "Memory usage after optimization is: 0.21 MB\n",
      "Decreased by 70.6%\n",
      "---\n",
      "ccb_agg_data_tr\n",
      "Memory usage of dataframe is 0.11 MB\n",
      "Memory usage after optimization is: 0.04 MB\n",
      "Decreased by 66.6%\n",
      "---\n",
      "ip_agg_data_tr\n",
      "Memory usage of dataframe is 0.17 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 63.4%\n",
      "---\n",
      "pos_agg_data_tr\n",
      "Memory usage of dataframe is 0.34 MB\n",
      "Memory usage after optimization is: 0.09 MB\n",
      "Decreased by 73.3%\n"
     ]
    }
   ],
   "source": [
    "# read-in\n",
    "DATA_DIR =  \"/\"\n",
    "\n",
    "ds_names = (\n",
    "    # \"application_test\",   \n",
    "    \"application_train\", \"prevapp_agg_data_tr\", \"bureau_agg_data_trans_untrans\",  \n",
    "    \"ccb_agg_data_tr\", \"ip_agg_data_tr\", \"pos_agg_data_tr\"\n",
    ")  \n",
    "\n",
    "datasets_agg = {}\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    print('---')\n",
    "    print(ds_name)\n",
    "    datasets_agg[ds_name] = pd.read_csv(os.getcwd() + DATA_DIR + f'{ds_name}.csv')\n",
    "    datasets_agg[ds_name] = reduce_mem_usage(datasets_agg[ds_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kAs9q2XTUrqA"
   },
   "outputs": [],
   "source": [
    "# denormalize and clean text\n",
    "for ds_name in datasets_agg:\n",
    "    if ds_name == 'application_train':\n",
    "        agg_data = datasets_agg['application_train'].replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\/', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\(', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\)', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\:', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\,', value='', regex=True)\n",
    "    else:\n",
    "        agg_data = agg_data.merge(datasets_agg[ds_name], on='SK_ID_CURR', how='left')\n",
    "\n",
    "\n",
    "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('Unnamed:')]\n",
    "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('SK_ID_PREV')]\n",
    "agg_data = agg_data[\n",
    "    (agg_data.CODE_GENDER!='XNA')&(agg_data.NAME_INCOME_TYPE!='Maternity leave')&(agg_data.NAME_FAMILY_STATUS!='Unknown')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiZElhxhsMe_",
    "outputId": "0058c259-a2eb-4055-a642-1e302b16ae3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER',\n",
       "       'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL',\n",
       "       'AMT_CREDIT', 'AMT_ANNUITY',\n",
       "       ...\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Signed_median',\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Signed_mean',\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Signed_var',\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Completed_median',\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Completed_mean',\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Completed_var',\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Approved_median',\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Approved_mean',\n",
       "       'PS_O_NAME_CONTRACT_STATUS_Approved_var', 'PS_O_SK_ID_PREV_count_y'],\n",
       "      dtype='object', length=1463)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7gyiRiE1JQC"
   },
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0v9NqWR1SgS"
   },
   "source": [
    "### Setup and Collinearity Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-VtWvcq3773",
    "outputId": "6a892a6e-3df8-4d5e-83b7-36967e5a1e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train           shape: (118080, 1461)\n",
      "X validation      shape: (29520, 1461)\n",
      "X test            shape: (36900, 1461)\n"
     ]
    }
   ],
   "source": [
    "# deep learning model\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create train, validation, and test sets\n",
    "y = agg_data['TARGET']\n",
    "X = agg_data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
    "\n",
    "_, X, _, y = train_test_split(X, y, test_size=0.6, random_state=42, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")\n",
    "\n",
    "# determine feature types\n",
    "id_col, feat_num, feat_cat, feature =  id_num_cat_feature(X, text = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ELkXmeHZk0dg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced numerical column count by 0.4844290657439446% through collinearity reduction.\n",
      "From 1445 columns to 700 columns.\n"
     ]
    }
   ],
   "source": [
    "### Collinear Feature Reduction\n",
    "\n",
    "with open ('reduced_feat_num.ob', 'rb') as fp:\n",
    "    reduced_feat_num = pickle.load(fp)\n",
    "\n",
    "# reduce numerical features by collinearity reduction\n",
    "\n",
    "# cr = make_pipeline(\n",
    "#     SimpleImputer(strategy='median'),\n",
    "#     StandardScaler(),    \n",
    "#     CollinearityReducer(attribute_names=feat_num, threshold = 0.5, max_iter=10)\n",
    "# )\n",
    "\n",
    "# tic = time.perf_counter()\n",
    "# reduced_feat_num = cr.fit_transform(X_train[feat_num], y_train) \n",
    "# toc = time.perf_counter()\n",
    "\n",
    "# print(f\"Collinearity Reduction completed in {toc - tic:0.4f} seconds.\")\n",
    "print(f'Reduced numerical column count by {len(reduced_feat_num)/len(feat_num)}% through collinearity reduction.')\n",
    "print(f'From {len(feat_num)} columns to {len(reduced_feat_num)} columns.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAqUW765mIe-"
   },
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6rXxnMl94W",
    "outputId": "9b1d8e13-8f4b-462d-a67a-a28666dc78f2"
   },
   "outputs": [],
   "source": [
    "### Main Pipeline\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(reduced_feat_num)),\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "data_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, feat_num),\n",
    "        (\"cat_pipeline\", cat_pipeline, feat_cat)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6rXxnMl94W",
    "outputId": "9b1d8e13-8f4b-462d-a67a-a28666dc78f2"
   },
   "outputs": [],
   "source": [
    "# prepare data as tensor -> dataloader\n",
    "\n",
    "X_train = data_pipeline.fit_transform(X_train)\n",
    "X_valid = data_pipeline.transform(X_valid) #Transform validation set with the same constants\n",
    "X_test = data_pipeline.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_valid = y_valid.to_numpy()\n",
    "y_test = y_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6rXxnMl94W",
    "outputId": "9b1d8e13-8f4b-462d-a67a-a28666dc78f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (108548, 838)\n",
      "class 1: (9532, 838)\n"
     ]
    }
   ],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\n",
    "\n",
    "train_imb = np.c_[y_train, X_train]\n",
    "\n",
    "# class count\n",
    "class_count_0, class_count_1 = pd.DataFrame(y_train).value_counts()\n",
    "\n",
    "# Separate class\n",
    "class_0 = train_imb[train_imb[:,0] == 0]\n",
    "class_1 = train_imb[train_imb[:,0] == 1]  # print the shape of the class\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6rXxnMl94W",
    "outputId": "9b1d8e13-8f4b-462d-a67a-a28666dc78f2"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m class_0_under \u001b[38;5;241m=\u001b[39m class_0\u001b[38;5;241m.\u001b[39msample(class_count_1)\n\u001b[1;32m      3\u001b[0m test_under \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([class_0_under, class_1], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal class of 1 and0:\u001b[39m\u001b[38;5;124m\"\u001b[39m,test_under[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\u001b[38;5;66;03m# plot the count after under-sampeling\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "class_0_under = class_0.sample(class_count_1)\n",
    "\n",
    "test_under = pd.concat([class_0_under, class_1], axis=0)\n",
    "\n",
    "print(\"total class of 1 and0:\",test_under['Class'].value_counts())# plot the count after under-sampeling\n",
    "test_under['Class'].value_counts().plot(kind='bar', title='count (target)')\n",
    "      \n",
    "# # import library\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# rus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\n",
    "# X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# print('original dataset shape:', Counter(y_train))\n",
    "# print('Resample dataset shape', Counter(y_train_rus))\n",
    "\n",
    "# print(\"total class of 1 and 0:\",y_train_rus.value_counts())# plot the count after under-sampeling\n",
    "# print(y_train_rus.value_counts().plot(kind='bar', title='count (TARGET)'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6rXxnMl94W",
    "outputId": "9b1d8e13-8f4b-462d-a67a-a28666dc78f2"
   },
   "outputs": [],
   "source": [
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train_rus)\n",
    "X_valid_tensor = torch.from_numpy(X_valid)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train_rus)\n",
    "y_valid_tensor = torch.from_numpy(y_valid)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "hcdr_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "hcdr_valid = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "hcdr_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# create dataloader\n",
    "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 64\n",
    "trainloader_hcdr = torch.utils.data.DataLoader(hcdr_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "validloader_hcdr = torch.utils.data.DataLoader(hcdr_valid, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_hcdr = torch.utils.data.DataLoader(hcdr_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "YR76icicV_9P",
    "outputId": "4c8759a1-17fc-465e-815d-017b1446f606"
   },
   "outputs": [],
   "source": [
    "\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(input_dataset) - description of input data: size_set_transformed_columns_CR\n",
    "(hidden_layers_neurons) - A list of the number of neurons in the hidden layers in order. DEFAULT: [32, 16, 8] => 1st hidden layer: 32 neurons, 2nd: 16, 3rd: 8\n",
    "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "(learning_rate) - The learning rate to take the gradient descent step with\n",
    "'''\n",
    "input_dataset=f\"0.6_agg_orig+trans_bal_{X_train.shape[1]}_CR:0.5-10_ReLU\"\n",
    "hidden_layer_neurons=[128, 64, 32, 16, 8, 4]\n",
    "opt=optim.Adagrad\n",
    "epochs=5\n",
    "learning_rate=5e-3\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "model, arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc, recall, balanced_accuracy = run_hcdr_model(\n",
    "    hidden_layer_neurons,\n",
    "    opt,\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    return_model=True\n",
    ")\n",
    "    \n",
    "\n",
    "try: hcdrLog \n",
    "except : hcdrLog = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Dataset\",\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Learning Rate\",\n",
    "        \"Train accuracy\",\n",
    "        \"Valid accuracy\",\n",
    "        \"Test accuracy\",\n",
    "        \"Train auc\",\n",
    "        \"Valid auc\",\n",
    "        \"Test auc\", \n",
    "        \"Recall\",\n",
    "        \"Balanced Accuracy\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "hcdrLog.loc[len(hcdrLog)] = [\n",
    "    input_dataset,\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{learning_rate}\",\n",
    "    f\"{train_accuracy}\",\n",
    "    f\"{valid_accuracy}\",\n",
    "    f\"{test_accuracy}\",\n",
    "    f\"{train_auc}\",\n",
    "    f\"{valid_auc}\",\n",
    "    f\"{test_auc}\",\n",
    "    f\"{recall}\",\n",
    "    f\"{balanced_accuracy}\"\n",
    "]\n",
    "\n",
    "hcdrLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "DeGGB6KUnsD_",
    "outputId": "c15fa08f-7d99-43a7-cdbc-16e1606c72b4"
   },
   "outputs": [],
   "source": [
    "hcdrLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrkIsbVX_0zs"
   },
   "outputs": [],
   "source": [
    "# hcdrLog = pd.DataFrame(\n",
    "#     columns=[\n",
    "#         \"Dataset\",\n",
    "#         \"Architecture string\", \n",
    "#         \"Optimizer\", \n",
    "#         \"Epochs\", \n",
    "#         \"Learning Rate\",\n",
    "#         \"Train accuracy\",\n",
    "#         \"Valid accuracy\",\n",
    "#         \"Test accuracy\",\n",
    "#         \"Train auc\",\n",
    "#         \"Valid auc\",\n",
    "#         \"Test auc\", \n",
    "#         \"Recall\",\n",
    "#         \"Balanced Accuracy\"\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33686747/save-a-list-to-a-txt-file\n",
    "\n",
    "import pickle\n",
    "with open('reduced_feat_num.ob', 'wb') as fp:\n",
    "    pickle.dump(reduced_feat_num, fp)\n",
    "    \n",
    "torch.save(model, f\"{input_dataset}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Data with Trained Model\n",
    "\n",
    "When running this section, restart the kernel and re-run section 1 - Setup cells first. \n",
    "\n",
    "### Read-in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvrG5rkx12sj",
    "outputId": "f3632b57-0b6f-48c2-f06d-dca9b652bb98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "application_test\n",
      "Memory usage of dataframe is 0.04 MB\n",
      "Memory usage after optimization is: 0.01 MB\n",
      "Decreased by 79.1%\n",
      "---\n",
      "prevapp_agg_data\n",
      "Memory usage of dataframe is 1.49 MB\n",
      "Memory usage after optimization is: 0.40 MB\n",
      "Decreased by 73.2%\n",
      "---\n",
      "bureau_agg_data\n",
      "Memory usage of dataframe is 0.66 MB\n",
      "Memory usage after optimization is: 0.19 MB\n",
      "Decreased by 71.0%\n",
      "---\n",
      "ccb_agg_data\n",
      "Memory usage of dataframe is 0.13 MB\n",
      "Memory usage after optimization is: 0.05 MB\n",
      "Decreased by 63.8%\n",
      "---\n",
      "ip_agg_data\n",
      "Memory usage of dataframe is 0.13 MB\n",
      "Memory usage after optimization is: 0.05 MB\n",
      "Decreased by 63.2%\n",
      "---\n",
      "pos_agg_data\n",
      "Memory usage of dataframe is 0.22 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 73.0%\n"
     ]
    }
   ],
   "source": [
    "# read-in\n",
    "DATA_DIR =  \"/\"\n",
    "\n",
    "test_ds_names = (\n",
    "    # \"application_train\",   \n",
    "    \"application_test\", \"prevapp_agg_data_tr\", \"bureau_agg_data_trans_untrans\",  \n",
    "    \"ccb_agg_data_tr\", \"ip_agg_data_tr\", \"pos_agg_data_tr\"\n",
    ")  \n",
    "\n",
    "datasets_test = {}\n",
    "\n",
    "for ds_name in test_ds_names:\n",
    "    print('---')\n",
    "    print(ds_name)\n",
    "    datasets_test[ds_name] = pd.read_csv(os.getcwd() + DATA_DIR + f'{ds_name}.csv')\n",
    "    datasets_test[ds_name] = reduce_mem_usage(datasets_test[ds_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dQXDmOgXUVxo"
   },
   "outputs": [],
   "source": [
    "# denormalize and clean text\n",
    "for ds_name in datasets_test:\n",
    "    if ds_name == 'application_test':\n",
    "        test_data = datasets_test['application_test'].replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\/', value='_', regex=True) \\\n",
    "                                                    .replace(to_replace='\\(', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\)', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\:', value='', regex=True) \\\n",
    "                                                    .replace(to_replace='\\,', value='', regex=True)\n",
    "    else:\n",
    "        test_data = test_data.merge(datasets_test[ds_name], on='SK_ID_CURR', how='left')\n",
    "\n",
    "\n",
    "test_data = test_data.loc[:,~test_data.columns.str.startswith('Unnamed:')]\n",
    "test_data = test_data.loc[:,~test_data.columns.str.startswith('SK_ID_PREV')]\n",
    "# test_data = test_data[\n",
    "#     (test_data.CODE_GENDER!='XNA')&(test_data.NAME_INCOME_TYPE!='Maternity leave')&(test_data.NAME_FAMILY_STATUS!='Unknown')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHE Discrepancy EDA\n",
    "\n",
    "It was observed when trying to train the model that there were some missing columns. This indicated missing category variables in the categorical columns of application_test. Below you can see the features and their missing attributes.\n",
    "\n",
    "```python\n",
    "app_test = datasets_test['application_test']  \n",
    "app_train = pd.read_csv(os.getcwd() + DATA_DIR + 'application_train.csv')\n",
    "app_train = reduce_mem_usage(app_train)\n",
    "\n",
    "cat_cols = ['CODE_GENDER', 'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS']\n",
    "# cat_cols = feat_cat\n",
    "\n",
    "print(app_train[cat_cols].apply(lambda col: col.unique()).to_markdown())\n",
    "print('')\n",
    "print(app_test[cat_cols].apply(lambda col: col.unique()).to_markdown())\n",
    "```\n",
    "[OUT]:\n",
    "\n",
    "|                    | 0    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| CODE_GENDER        | ['M', 'F', 'XNA']                                                                                                                                    |\n",
    "|                    | Categories (3, object): ['F', 'M', 'XNA']                                                                                                            |\n",
    "| NAME_INCOME_TYPE   | ['Working', 'State servant', 'Commercial associate', 'Pensioner', 'Unemployed', 'Student', 'Businessman', 'Maternity leave']                         |\n",
    "|                    | Categories (8, object): ['Businessman', 'Commercial associate', 'Maternity leave', 'Pensioner', 'State servant', 'Student', 'Unemployed', 'Working'] |\n",
    "| NAME_FAMILY_STATUS | ['Single / not married', 'Married', 'Civil marriage', 'Widow', 'Separated', 'Unknown']                                                               |\n",
    "|                    | Categories (6, object): ['Civil marriage', 'Married', 'Separated', 'Single / not married', 'Unknown', 'Widow']                                       |\n",
    "\n",
    "|                    | 0    |\n",
    "|:-------------------|:----------------------------------------------------------------------------------------------------------------------------------|\n",
    "| CODE_GENDER        | ['F', 'M']                                                                                                                        |\n",
    "|                    | Categories (2, object): ['F', 'M']                                                                                                |\n",
    "| NAME_INCOME_TYPE   | ['Working', 'State servant', 'Pensioner', 'Commercial associate', 'Businessman', 'Student', 'Unemployed']                         |\n",
    "|                    | Categories (7, object): ['Businessman', 'Commercial associate', 'Pensioner', 'State servant', 'Student', 'Unemployed', 'Working'] |\n",
    "| NAME_FAMILY_STATUS | ['Married', 'Single / not married', 'Civil marriage', 'Widow', 'Separated']                                                       |\n",
    "|                    | Categories (5, object): ['Civil marriage', 'Married', 'Separated', 'Single / not married', 'Widow']                               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0lpM_mQYUWZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set           shape: (48744, 1296)\n"
     ]
    }
   ],
   "source": [
    "# create test sets\n",
    "X_t = test_data.drop(['SK_ID_CURR'], axis = 1) #drop some features with questionable value\n",
    "\n",
    "X_t.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"test set           shape: {X_t.shape}\")\n",
    "\n",
    "# determine feature types\n",
    "id_col, feat_num, feat_cat, feature =  id_num_cat_feature(X_t, text = False)\n",
    "\n",
    "with open ('reduced_feat_num.ob', 'rb') as fp:\n",
    "    reduced_feat_num = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Pipeline\n",
    "\n",
    "should be exact same as Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zU6rXxnMl94W",
    "outputId": "9b1d8e13-8f4b-462d-a67a-a28666dc78f2"
   },
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(reduced_feat_num)),\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "data_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, feat_num),\n",
    "        (\"cat_pipeline\", cat_pipeline, feat_cat)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0lpM_mQYUWZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48744, 723])\n"
     ]
    }
   ],
   "source": [
    "X_t = data_pipeline.fit_transform(X_t) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_t_tensor = torch.from_numpy(X_t)\n",
    "\n",
    "print(X_t_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6nvfsOOK9gp4"
   },
   "outputs": [],
   "source": [
    "model = torch.load(f\"0.6_agg_orig_723_CR:0.5-10_ReLU.pt\")\n",
    "model.eval()\n",
    "\n",
    "t_preds = model(X_t_tensor.float())\n",
    "t_probs = F.softmax(t_preds, dim=1)[:,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.043284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100005</td>\n",
       "      <td>0.281598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100013</td>\n",
       "      <td>0.032688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100028</td>\n",
       "      <td>0.049001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.239915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR    TARGET\n",
       "0      100001  0.043284\n",
       "1      100005  0.281598\n",
       "2      100013  0.032688\n",
       "3      100028  0.049001\n",
       "4      100038  0.239915"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df = datasets_test[\"application_test\"][['SK_ID_CURR']]\n",
    "submit_df['TARGET'] = t_probs\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
