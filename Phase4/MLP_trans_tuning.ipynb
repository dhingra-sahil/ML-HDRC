{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Collinearity Reducer HCDR Pipeline"
      ],
      "metadata": {
        "id": "O0AG1yF50c0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vW-DkS2mTZ69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6b39dd-b48b-4c62-e2a7-a1ba51692954"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "TDo68G3t0gxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "4__MiCS81mGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p-dlcHT1FvM7"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "\n",
        "# system\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "\n",
        "# data engineering\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from pandas.plotting import scatter_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# machine learning\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# deep learning\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Classes"
      ],
      "metadata": {
        "id": "l9J-pHtE0jva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer reduces the list of columns by a subset\n",
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names):\n",
        "        self.attribute_names = attribute_names\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[self.attribute_names].values\n",
        "\n",
        "# transformer produces a reduced column list by collinearity reduction\n",
        "class CollinearityReducer(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    '''\n",
        "    This class reduces features by measuring collinearity between the input variables and target.\n",
        "    Works on numerical features based on the correlations between each variable pair.\n",
        "    Of the var1iable pairs with absolute correlations above the threshold value...\n",
        "    ...the variables with the lowest target variable correlation are dropped from the input X.\n",
        "    The process is repeated until there are no more colinear pairs with absolute correlations above the threshold.\n",
        "    ...Or max_iter. \n",
        "    \n",
        "    The transformation returns a subset of feature names... \n",
        "    ...to be used with the DataFrameSelector() Class. \n",
        "\n",
        "    This class is meant to be run at the end of the numerical pipeline\n",
        "    PRIOR TO THE ACTUAL PIPELINE - only returns subset for DataFrameSelector().\n",
        "\n",
        "    NOTE! The function receives a dataframe structured with the target variable in first column.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, attribute_names, threshold=0.5, max_iter=None):\n",
        "        self.attribute_names = attribute_names\n",
        "        self.threshold = threshold\n",
        "        self.max_iter = max_iter\n",
        "            \n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None): \n",
        "        \n",
        "        dataframe = pd.concat([y, pd.DataFrame(X)], axis=1)\n",
        "        \n",
        "        i = 0\n",
        "        while i <= self.max_iter:\n",
        "\n",
        "            # read-in and assign columns\n",
        "            # gets correlation matrix between variables and pivots to a longer df\n",
        "            # identify target variable\n",
        "            # drop same-name and target correlations pairs\n",
        "              \n",
        "            df = dataframe\n",
        "            features = df.iloc[:,1:].columns\n",
        "            target_name = df.iloc[:,0].name\n",
        "\n",
        "            df = pd.melt(abs(df.corr()).reset_index(), id_vars='index', value_vars=features)\n",
        "            targets = df[df['index']==target_name]\n",
        "            df = df[(df['index'] != df['variable']) & (df['index'] != target_name) & (df['variable'] != target_name)]\n",
        "\n",
        "            # combine the correlated variables into ordered pairs\n",
        "            # aggregate the max correlation and sort pairs\n",
        "            # split out the variables from the pair\n",
        "            # join the target variable correlations for each variable pair, rename columns\n",
        "\n",
        "            df['joined'] = df[['index', 'variable']].apply(lambda row: '::'.join(np.sort(row.values.astype(str))), axis=1)\n",
        "\n",
        "            df = df.groupby('joined', as_index=False) \\\n",
        "                   .agg({'value':'max'}) \\\n",
        "                   .sort_values(by='value', ascending=False)\n",
        "\n",
        "            df[['var_1','var_2']] = df['joined'].str.split(\"::\",expand=True).astype(int)\n",
        "\n",
        "            df = df.merge(targets, how='left', left_on='var_1', right_on='variable') \\\n",
        "                   .merge(targets, how='left', left_on='var_2', right_on='variable')\n",
        "            df.rename(columns = {'value_x':'var_pair_corr', 'value_y':'var_1_target_corr', 'value':'var_2_target_corr'}, inplace = True)\n",
        "\n",
        "            # This section takes all variable pairs with a correlation greater than threshold\n",
        "            # tests to determine which variable has a higher correlation with the target.\n",
        "            # The higher of the two gets marked as a win\n",
        "            # While the other gets marked as a loss\n",
        "            # the wins and losses for each variable are then grouped and summed\n",
        "\n",
        "            exceeds = df[df['var_pair_corr']>self.threshold]\n",
        "\n",
        "            # break if none above threshold\n",
        "            if len(exceeds['var_pair_corr'])==0:\n",
        "                break\n",
        "\n",
        "            # \"correlation competition\"\n",
        "            exceeds['var_1_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] >= row[\"var_2_target_corr\"] else 0, axis=1)\n",
        "            exceeds['var_1_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] >= row[\"var_1_target_corr\"] else 0, axis=1)\n",
        "            exceeds['var_2_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] < row[\"var_2_target_corr\"] else 0, axis=1)\n",
        "            exceeds['var_2_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] < row[\"var_1_target_corr\"] else 0, axis=1)\n",
        "\n",
        "            # aggregate scores\n",
        "            var1 = exceeds[['var_1', 'var_1_win', 'var_1_loss']].groupby('var_1', as_index=False) \\\n",
        "                                                                .agg({'var_1_win':'sum', 'var_1_loss':'sum'})\n",
        "            var1.rename(columns = {'var_1':'var', 'var_1_win':'win', 'var_1_loss':'loss'}, inplace=True)\n",
        "\n",
        "            var2 = exceeds[['var_2', 'var_2_win', 'var_2_loss']].groupby('var_2', as_index=False) \\\n",
        "                                                                .agg({'var_2_win':'sum', 'var_2_loss':'sum'})\n",
        "            var2.rename(columns = {'var_2':'var', 'var_2_win':'win', 'var_2_loss':'loss'}, inplace=True)\n",
        "\n",
        "            corrcomps = pd.concat([var1,var2], axis=0).groupby('var', as_index=False) \\\n",
        "                                                      .agg({'win':'sum', 'loss':'sum'})\n",
        "\n",
        "            # drop variables which had 0 wins - IE collinear variables which were always least related to the target\n",
        "            dropvars = corrcomps[corrcomps['win']==0]['var']\n",
        "\n",
        "            dataframe = dataframe.drop(dropvars, axis=1)  \n",
        "\n",
        "            i += 1  \n",
        "        \n",
        "        X = [self.attribute_names[col] for col in dataframe.columns]\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "AjaTemryA3e5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Functions"
      ],
      "metadata": {
        "id": "FvEZVk640s23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function identifies missing data\n",
        "def missing_data(data):\n",
        "    total = data.isnull().sum().sort_values(ascending = False)\n",
        "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
        "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) \n",
        "\n",
        "\n",
        "# function to identify different feature types and summary EDA\n",
        "def id_num_cat_feature(df,text = True):\n",
        "    numerical = df.select_dtypes(include=np.number).columns\n",
        "    categorical = df.select_dtypes(include=['object', 'bool', 'category']).columns\n",
        "    feat_num = list(numerical)\n",
        "    feat_cat = list(categorical)\n",
        "    \n",
        "    id_cols = ['SK_ID_CURR','SK_ID_BUREAU']\n",
        "    \n",
        "    id_cols = [cols for cols in  list(df.columns.intersection(id_cols))] \n",
        "    features = list(set(df.columns) - set(id_cols))\n",
        "\n",
        "    if text == True:\n",
        "          # print eda\n",
        "        print('--------')\n",
        "        print(f\"# of ID's: {len(id_cols)}\")\n",
        "        print(f\" ID's:\")\n",
        "        print(id_cols)\n",
        "        print('')\n",
        "        print('--------')\n",
        "        print(f\"# All features: {len(features)}\")\n",
        "        print(f\"All features:\")\n",
        "        print(features)\n",
        "        print('')\n",
        "        print(f\"Missing data:\")\n",
        "        print(missing_data(df[features]))\n",
        "        print('')\n",
        "        print('--------')\n",
        "        print(f\"# of Numerical features: {len(feat_num)}\")\n",
        "        print(f\"Numerical features:\")\n",
        "        print(feat_num)\n",
        "        print('')\n",
        "        print(f\"Numerical Statistical Summary:\")\n",
        "        print('')\n",
        "        print(df[feat_num].describe())\n",
        "        print('')\n",
        "        print('--------')\n",
        "        print(f\"# of Categorical features: {len(feat_cat)}\")\n",
        "        print(f\"Categorical features:\")\n",
        "        print(feat_cat)\n",
        "        print('')\n",
        "        print(f\"Categorical Statistical Summary:\")\n",
        "        print('')\n",
        "        #print(df[feat_cat].describe(include='all'))\n",
        "        print('')\n",
        "        print(\"Categories:\")\n",
        "        print('')\n",
        "        print(df[feat_cat].apply(lambda col: col.unique()))\n",
        "        print('')\n",
        "        print('--------')\n",
        "        \n",
        "    return id_cols,feat_num,feat_cat,features\n",
        "\n",
        "\n",
        "# https://pythonsimplified.com/how-to-handle-large-datasets-in-python-with-pandas/\n",
        "\n",
        "def reduce_mem_usage(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024**3\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**3\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "70zdSCUMA30F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "IY-aqM5W04hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read-In and Merge"
      ],
      "metadata": {
        "id": "xZ8dzHAt1pvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read-in\n",
        "DATA_DIR =  \"/drive/MyDrive/ColabNotebooks/\"\n",
        "\n",
        "ds_names = (\n",
        "    # \"application_test\",   \n",
        "    \"application_train\", \"prevapp_agg_data_tr\", \"bureau_agg_data_trans_untrans\",  \n",
        "    \"ccb_agg_data_tr\", \"ip_agg_data_tr\", \"pos_agg_data_tr\"\n",
        ")  \n",
        "\n",
        "datasets_agg = {}\n",
        "\n",
        "for ds_name in ds_names:\n",
        "    print('---')\n",
        "    print(ds_name)\n",
        "    datasets_agg[ds_name] = pd.read_csv(os.getcwd() + DATA_DIR + f'{ds_name}.csv')\n",
        "    datasets_agg[ds_name] = reduce_mem_usage(datasets_agg[ds_name])"
      ],
      "metadata": {
        "id": "VWvkmBxpA32r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935f1baf-af42-4775-b6ad-f2286d41823f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "application_train\n",
            "Memory usage of dataframe is 0.28 MB\n",
            "Memory usage after optimization is: 0.06 MB\n",
            "Decreased by 79.2%\n",
            "---\n",
            "prevapp_agg_data_tr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# denormalize and clean text\n",
        "for ds_name in datasets_agg:\n",
        "    if ds_name == 'application_train':\n",
        "        agg_data = datasets_agg['application_train'].replace(to_replace='\\s+', value='_', regex=True) \\\n",
        "                                                    .replace(to_replace='\\-', value='_', regex=True) \\\n",
        "                                                    .replace(to_replace='\\/', value='_', regex=True) \\\n",
        "                                                    .replace(to_replace='\\(', value='', regex=True) \\\n",
        "                                                    .replace(to_replace='\\)', value='', regex=True) \\\n",
        "                                                    .replace(to_replace='\\:', value='', regex=True) \\\n",
        "                                                    .replace(to_replace='\\,', value='', regex=True)\n",
        "    else:\n",
        "        agg_data = agg_data.merge(datasets_agg[ds_name], on='SK_ID_CURR', how='left')\n",
        "\n",
        "\n",
        "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('Unnamed:')]\n",
        "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('SK_ID_PREV')]\n"
      ],
      "metadata": {
        "id": "kAs9q2XTUrqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_data.columns"
      ],
      "metadata": {
        "id": "LiZElhxhsMe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "J7gyiRiE1JQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Model Function"
      ],
      "metadata": {
        "id": "o90foRoMMIuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Method to create, define and run a deep neural network model\n",
        "#\n",
        "def run_hcdr_model(\n",
        "    hidden_layer_neurons=[32, 16, 8],\n",
        "    opt=optim.SGD,\n",
        "    epochs=5,\n",
        "    learning_rate=1e-3\n",
        "):\n",
        "    \n",
        "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
        "    D_out = 2  # Output layer neurons - depend on what you're trying to predict, here, 2 classes: 0 and 1\n",
        "    \n",
        "    str_neurons = [str(h) for h in hidden_layer_neurons]\n",
        "    arch_string = f\"{D_in}-{'-'.join(str_neurons)}-{D_out}\"\n",
        "    \n",
        "    layers = [\n",
        "        torch.nn.Linear(D_in, hidden_layer_neurons[0]),  # X.matmul(W1)\n",
        "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
        "    ]\n",
        "    \n",
        "    # Add hidden layers\n",
        "    for i in range(1, len(hidden_layer_neurons)):\n",
        "        prev, curr = hidden_layer_neurons[i - 1], hidden_layer_neurons[i]\n",
        "        layers.append(torch.nn.Linear(prev, curr))\n",
        "        layers.append(nn.ReLU())\n",
        "        \n",
        "    \n",
        "    # Add final layer\n",
        "    layers.append(nn.Linear(hidden_layer_neurons[-1], D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
        "    \n",
        "    # Use the nn package to define our model and loss function.\n",
        "    # use the sequential API makes things simple\n",
        "    model = torch.nn.Sequential(*layers)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # use Cross Entropy and SGD optimizer.\n",
        "    loss_fn = nn.CrossEntropyLoss()  #for classfication \n",
        "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    #summary(model, (4, 20))\n",
        "    print('-'*50)\n",
        "    print('Model:')\n",
        "    print(model)\n",
        "    print('-'*50)\n",
        "    \n",
        "    '''\n",
        "    Training Process:\n",
        "        Load a batch of data.\n",
        "        Zero the grad.\n",
        "        Predict the batch of the data through net i.e forward pass.\n",
        "        Calculate the loss value by predict value and true value.\n",
        "        Backprop i.e get the gradient with respect to parameters\n",
        "        Update optimizer i.e gradient update\n",
        "    '''\n",
        "\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
        "        running_loss = 0.0\n",
        "        count = 0\n",
        "        y_prob = []\n",
        "        y_pred = []\n",
        "        epoch_target = []\n",
        "        # dataset API gives us pythonic batching \n",
        "        for batch_id, data in enumerate(train_loader):\n",
        "            # https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
        "            inputs, target = data[0], data[1]\n",
        "            target = target.type(torch.LongTensor)   # casting to long\n",
        "            inputs, target = inputs.to(device), target.to(device)        \n",
        "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
        "            opt.zero_grad()\n",
        "            preds = model(inputs.float()) #prediction over the input data\n",
        "\n",
        "            # compute loss and gradients\n",
        "            loss = loss_fn(preds, target)    #mean loss for this batch\n",
        "\n",
        "            loss.backward() #calculate nabla_w\n",
        "            loss_history.append(loss.item())\n",
        "            opt.step()  #update W\n",
        "            \n",
        "            # https://stackoverflow.com/questions/60182984/how-to-get-the-predict-probability\n",
        "            y_prob.extend(F.softmax(preds, dim=1)[:,1].tolist())\n",
        "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
        "            epoch_target.extend(target.tolist())\n",
        "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "        loss = np.round(running_loss/count, 3)\n",
        "        \n",
        "        #accuracy\n",
        "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
        "        accuracy = correct.sum() / correct.size\n",
        "        accuracy = np.round(accuracy, 3)\n",
        "        \n",
        "        # auc\n",
        "        roc_auc = roc_auc_score(np.array(epoch_target),np.array(y_prob))\n",
        "        roc_auc = np.round(roc_auc,3)\n",
        "                           \n",
        "        return loss, accuracy, roc_auc\n",
        "\n",
        "\n",
        "\n",
        "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
        "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
        "        overall_loss = 0.0\n",
        "        count = 0\n",
        "        y_prob = []\n",
        "        y_pred = []\n",
        "        epoch_target = []\n",
        "        for i,data in enumerate(data_loader):\n",
        "            # https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
        "            inputs, target = data[0], data[1]\n",
        "            target = target.type(torch.LongTensor)   # casting to long\n",
        "            inputs, target = inputs.to(device), target.to(device)                \n",
        "            preds = model(inputs.float())      \n",
        "\n",
        "            loss = loss_fn(preds, target)           # compute loss value\n",
        "\n",
        "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
        "            \n",
        "            y_prob.extend(F.softmax(preds, dim=1)[:,1].tolist())\n",
        "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
        "            epoch_target.extend(target.tolist())\n",
        "            count += 1\n",
        "\n",
        "        # compute mean loss\n",
        "        loss = np.round(overall_loss/count, 3)\n",
        "        #accuracy\n",
        "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
        "        accuracy = correct.sum() / correct.size\n",
        "        accuracy = np.round(accuracy, 3)\n",
        "\n",
        "        # auc\n",
        "        roc_auc = roc_auc_score(np.array(epoch_target),np.array(y_prob))\n",
        "        roc_auc = np.round(roc_auc,3)\n",
        "\n",
        "        return loss, accuracy, roc_auc\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):      \n",
        "        train_loss, train_accuracy, train_auc = train_epoch(epoch, model, loss_fn, optimizer, trainloader_hcdr)\n",
        "        valid_loss, valid_accuracy, valid_auc = evaluate_model(epoch, model, loss_fn, optimizer, validloader_hcdr, tag = \"Validation\")\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        print(f\"----Train Accuracy: {train_accuracy}\\t Validation Accuracy: {valid_accuracy}\")\n",
        "        print(f\"----Train AUC-ROC: {train_auc}\\t Validation AUC-ROC: {valid_auc}\")\n",
        "    print(\"-\"*50)\n",
        "    test_loss, test_accuracy, test_auc = evaluate_model(epoch, model, loss_fn, opt, testloader_hcdr, tag=\"Test\")\n",
        "    \n",
        "    return arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc"
      ],
      "metadata": {
        "id": "Tu_kiwglMHQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Pipeline"
      ],
      "metadata": {
        "id": "m0v9NqWR1SgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deep learning model\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# create train, validation, and test sets\n",
        "y = agg_data['TARGET']\n",
        "X = agg_data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
        "\n",
        "_, X, _, y = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "print(f\"X train           shape: {X_train.shape}\")\n",
        "print(f\"X validation      shape: {X_valid.shape}\")\n",
        "print(f\"X test            shape: {X_test.shape}\")\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "### Collinear Feature Reduction\n",
        "\n",
        "# determine feature types, reduce numerical features by collinearity reduction\n",
        "id_col, feat_num, feat_cat, feature =  id_num_cat_feature(X, text = False)\n",
        "\n",
        "# cr = make_pipeline(\n",
        "#     SimpleImputer(strategy='median'),\n",
        "#     StandardScaler(),    \n",
        "#     CollinearityReducer(attribute_names=feat_num, threshold = 0.5, max_iter=50)\n",
        "# )\n",
        "\n",
        "# tic = time.perf_counter()\n",
        "# reduced_feat_num = cr.fit_transform(X_train[feat_num], y_train) \n",
        "# toc = time.perf_counter()\n",
        "\n",
        "# print(f\"Collinearity Reduction completed in {toc - tic:0.4f} seconds.\")\n",
        "# print(f'Reduced numerical column count by {len(reduced_feat_num)/len(feat_num)}% through collinearity reduction.')\n",
        "# print(f'From {len(feat_num)} columns to {len(reduced_feat_num)} columns.')\n",
        "\n",
        "### Main Pipeline\n",
        "\n",
        "# Pipeline\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    # ('selector', DataFrameSelector(reduced_feat_num)),\n",
        "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
        "    ('std_scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "data_pipeline = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num_pipeline\", num_pipeline, feat_num),\n",
        "        (\"cat_pipeline\", cat_pipeline, feat_cat)\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "X_train = data_pipeline.fit_transform(X_train)\n",
        "X_valid = data_pipeline.transform(X_valid) #Transform validation set with the same constants\n",
        "X_test = data_pipeline.transform(X_test) #Transform test set with the same constants\n",
        "\n",
        "y_train = y_train.to_numpy()\n",
        "y_valid = y_valid.to_numpy()\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "# convert numpy arrays to tensors\n",
        "X_train_tensor = torch.from_numpy(X_train)\n",
        "X_valid_tensor = torch.from_numpy(X_valid)\n",
        "X_test_tensor = torch.from_numpy(X_test)\n",
        "y_train_tensor = torch.from_numpy(y_train)\n",
        "y_valid_tensor = torch.from_numpy(y_valid)\n",
        "y_test_tensor = torch.from_numpy(y_test)\n",
        "\n",
        "# create TensorDataset in PyTorch\n",
        "hcdr_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "hcdr_valid = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "hcdr_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "# create dataloader\n",
        "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
        "train_batch_size = 96\n",
        "valid_test_batch_size = 64\n",
        "trainloader_hcdr = torch.utils.data.DataLoader(hcdr_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
        "validloader_hcdr = torch.utils.data.DataLoader(hcdr_valid, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
        "testloader_hcdr = torch.utils.data.DataLoader(hcdr_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "w-VtWvcq3773"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#==================================================#\n",
        "#    Modify START   #\n",
        "#==================================================#\n",
        "'''\n",
        "(hidden_layers_neurons) - A list of the number of neurons in the hidden layers in order. DEFAULT: [32, 16, 8] => 1st hidden layer: 32 neurons, 2nd: 16, 3rd: 8\n",
        "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
        "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
        "(learning_rate) - The learning rate to take the gradient descent step with\n",
        "'''\n",
        "\n",
        "hidden_layer_neurons=[128, 64, 32, 16, 4]\n",
        "opt=optim.SGD\n",
        "epochs=10\n",
        "learning_rate=5e-3\n",
        "\n",
        "#==================================================#\n",
        "#    Modify END #\n",
        "#==================================================#\n",
        "\n",
        "arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc = run_hcdr_model(\n",
        "    hidden_layer_neurons,\n",
        "    opt,\n",
        "    epochs,\n",
        "    learning_rate\n",
        ")\n",
        "    \n",
        "\n",
        "try: hcdrLog \n",
        "except : hcdrLog = pd.DataFrame(\n",
        "    columns=[\n",
        "        \"Architecture string\", \n",
        "        \"Optimizer\", \n",
        "        \"Epochs\", \n",
        "        \"Train accuracy\",\n",
        "        \"Valid accuracy\",\n",
        "        \"Test accuracy\",\n",
        "        \"Train auc\",\n",
        "        \"Valid auc\",\n",
        "        \"Test auc\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "hcdrLog.loc[len(hcdrLog)] = [\n",
        "    arch_string, \n",
        "    f\"{opt}\", \n",
        "    f\"{epochs}\", \n",
        "    f\"{train_accuracy * 100}%\",\n",
        "    f\"{valid_accuracy * 100}%\",\n",
        "    f\"{test_accuracy * 100}%\",\n",
        "    f\"{train_auc * 100}%\",\n",
        "    f\"{valid_auc * 100}%\",\n",
        "    f\"{test_auc * 100}%\",\n",
        "]\n",
        "\n",
        "hcdrLog"
      ],
      "metadata": {
        "id": "YR76icicV_9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hcdrLog = pd.DataFrame(\n",
        "#     columns=[\n",
        "#         \"Architecture string\", \n",
        "#         \"Optimizer\", \n",
        "#         \"Epochs\", \n",
        "#         \"Train accuracy\",\n",
        "#         \"Valid accuracy\",\n",
        "#         \"Test accuracy\",\n",
        "#         \"Train auc\",\n",
        "#         \"Valid auc\",\n",
        "#         \"Test auc\"\n",
        "#     ]\n",
        "# )"
      ],
      "metadata": {
        "id": "BrkIsbVX_0zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "vtUlJdMRLyZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}