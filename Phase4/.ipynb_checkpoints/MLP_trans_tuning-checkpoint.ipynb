{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Collinearity Reducer HCDR Pipeline"
      ],
      "metadata": {
        "id": "O0AG1yF50c0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vW-DkS2mTZ69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b725044-9a1c-4c70-ba09-504535db20bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "TDo68G3t0gxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "4__MiCS81mGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p-dlcHT1FvM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d16b99e-a2b4-4933-e98e-c91275cdd49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available?\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "\n",
        "# system\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "\n",
        "# data engineering\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from pandas.plotting import scatter_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# machine learning\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# deep learning\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"cuda available?\\n{torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Classes"
      ],
      "metadata": {
        "id": "l9J-pHtE0jva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer reduces the list of columns by a subset\n",
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names):\n",
        "        self.attribute_names = attribute_names\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[self.attribute_names].values\n",
        "\n",
        "# transformer produces a reduced column list by collinearity reduction\n",
        "class CollinearityReducer(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    '''\n",
        "    This class reduces features by measuring collinearity between the input variables and target.\n",
        "    Works on numerical features based on the correlations between each variable pair.\n",
        "    Of the var1iable pairs with absolute correlations above the threshold value...\n",
        "    ...the variables with the lowest target variable correlation are dropped from the input X.\n",
        "    The process is repeated until there are no more colinear pairs with absolute correlations above the threshold.\n",
        "    ...Or max_iter. \n",
        "    \n",
        "    The transformation returns a subset of feature names... \n",
        "    ...to be used with the DataFrameSelector() Class. \n",
        "\n",
        "    This class is meant to be run at the end of the numerical pipeline\n",
        "    PRIOR TO THE ACTUAL PIPELINE - only returns subset for DataFrameSelector().\n",
        "\n",
        "    NOTE! The function receives a dataframe structured with the target variable in first column.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, attribute_names, threshold=0.5, max_iter=None):\n",
        "        self.attribute_names = attribute_names\n",
        "        self.threshold = threshold\n",
        "        self.max_iter = max_iter\n",
        "            \n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None): \n",
        "\n",
        "        # TODO: address dataframe error when used with pytorch model       \n",
        "        dataframe = pd.concat([y, pd.DataFrame(X)], axis=1)\n",
        "        \n",
        "        i = 0\n",
        "        while i <= self.max_iter:\n",
        "\n",
        "            # read-in and assign columns\n",
        "            # gets correlation matrix between variables and pivots to a longer df\n",
        "            # identify target variable\n",
        "            # drop same-name and target correlations pairs\n",
        "              \n",
        "            df = dataframe\n",
        "            features = df.iloc[:,1:].columns\n",
        "            target_name = df.iloc[:,0].name\n",
        "\n",
        "            df = pd.melt(abs(df.corr()).reset_index(), id_vars='index', value_vars=features)\n",
        "            targets = df[df['index']==target_name]\n",
        "            df = df[(df['index'] != df['variable']) & (df['index'] != target_name) & (df['variable'] != target_name)]\n",
        "\n",
        "            # combine the correlated variables into ordered pairs\n",
        "            # aggregate the max correlation and sort pairs\n",
        "            # split out the variables from the pair\n",
        "            # join the target variable correlations for each variable pair, rename columns\n",
        "\n",
        "            df['joined'] = df[['index', 'variable']].apply(lambda row: '::'.join(np.sort(row.values.astype(str))), axis=1)\n",
        "\n",
        "            df = df.groupby('joined', as_index=False) \\\n",
        "                   .agg({'value':'max'}) \\\n",
        "                   .sort_values(by='value', ascending=False)\n",
        "\n",
        "            df[['var_1','var_2']] = df['joined'].str.split(\"::\",expand=True).astype(int)\n",
        "\n",
        "            df = df.merge(targets, how='left', left_on='var_1', right_on='variable') \\\n",
        "                   .merge(targets, how='left', left_on='var_2', right_on='variable')\n",
        "            df.rename(columns = {'value_x':'var_pair_corr', 'value_y':'var_1_target_corr', 'value':'var_2_target_corr'}, inplace = True)\n",
        "\n",
        "            # This section takes all variable pairs with a correlation greater than threshold\n",
        "            # tests to determine which variable has a higher correlation with the target.\n",
        "            # The higher of the two gets marked as a win\n",
        "            # While the other gets marked as a loss\n",
        "            # the wins and losses for each variable are then grouped and summed\n",
        "\n",
        "            exceeds = df[df['var_pair_corr']>self.threshold]\n",
        "\n",
        "            # break if none above threshold\n",
        "            if len(exceeds['var_pair_corr'])==0:\n",
        "                break\n",
        "\n",
        "            # \"correlation competition\"\n",
        "            exceeds['var_1_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] >= row[\"var_2_target_corr\"] else 0, axis=1)\n",
        "            exceeds['var_1_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] >= row[\"var_1_target_corr\"] else 0, axis=1)\n",
        "            exceeds['var_2_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] < row[\"var_2_target_corr\"] else 0, axis=1)\n",
        "            exceeds['var_2_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] < row[\"var_1_target_corr\"] else 0, axis=1)\n",
        "\n",
        "            # aggregate scores\n",
        "            var1 = exceeds[['var_1', 'var_1_win', 'var_1_loss']].groupby('var_1', as_index=False) \\\n",
        "                                                                .agg({'var_1_win':'sum', 'var_1_loss':'sum'})\n",
        "            var1.rename(columns = {'var_1':'var', 'var_1_win':'win', 'var_1_loss':'loss'}, inplace=True)\n",
        "\n",
        "            var2 = exceeds[['var_2', 'var_2_win', 'var_2_loss']].groupby('var_2', as_index=False) \\\n",
        "                                                                .agg({'var_2_win':'sum', 'var_2_loss':'sum'})\n",
        "            var2.rename(columns = {'var_2':'var', 'var_2_win':'win', 'var_2_loss':'loss'}, inplace=True)\n",
        "\n",
        "            corrcomps = pd.concat([var1,var2], axis=0).groupby('var', as_index=False) \\\n",
        "                                                      .agg({'win':'sum', 'loss':'sum'})\n",
        "\n",
        "            # drop variables which had 0 wins - IE collinear variables which were always least related to the target\n",
        "            dropvars = corrcomps[corrcomps['win']==0]['var']\n",
        "\n",
        "            dataframe = dataframe.drop(dropvars, axis=1)  \n",
        "\n",
        "            i += 1  \n",
        "        \n",
        "        X = [self.attribute_names[col] for col in dataframe.columns]\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "AjaTemryA3e5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Functions"
      ],
      "metadata": {
        "id": "FvEZVk640s23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function identifies missing data\n",
        "def missing_data(data):\n",
        "    total = data.isnull().sum().sort_values(ascending = False)\n",
        "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
        "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) \n",
        "\n",
        "\n",
        "# function to identify different feature types and summary EDA\n",
        "def id_num_cat_feature(df,text = True):\n",
        "    numerical = df.select_dtypes(include=np.number).columns\n",
        "    categorical = df.select_dtypes(include=['object', 'bool', 'category']).columns\n",
        "    feat_num = list(numerical)\n",
        "    feat_cat = list(categorical)\n",
        "    \n",
        "    id_cols = ['SK_ID_CURR','SK_ID_BUREAU']\n",
        "    \n",
        "    id_cols = [cols for cols in  list(df.columns.intersection(id_cols))] \n",
        "    features = list(set(df.columns) - set(id_cols))\n",
        "\n",
        "    if text == True:\n",
        "          # print eda\n",
        "        print('--------')\n",
        "        print(f\"# of ID's: {len(id_cols)}\")\n",
        "        print(f\" ID's:\")\n",
        "        print(id_cols)\n",
        "        print('')\n",
        "        print('--------')\n",
        "        print(f\"# All features: {len(features)}\")\n",
        "        print(f\"All features:\")\n",
        "        print(features)\n",
        "        print('')\n",
        "        print(f\"Missing data:\")\n",
        "        print(missing_data(df[features]))\n",
        "        print('')\n",
        "        print('--------')\n",
        "        print(f\"# of Numerical features: {len(feat_num)}\")\n",
        "        print(f\"Numerical features:\")\n",
        "        print(feat_num)\n",
        "        print('')\n",
        "        print(f\"Numerical Statistical Summary:\")\n",
        "        print('')\n",
        "        print(df[feat_num].describe())\n",
        "        print('')\n",
        "        print('--------')\n",
        "        print(f\"# of Categorical features: {len(feat_cat)}\")\n",
        "        print(f\"Categorical features:\")\n",
        "        print(feat_cat)\n",
        "        print('')\n",
        "        print(f\"Categorical Statistical Summary:\")\n",
        "        print('')\n",
        "        #print(df[feat_cat].describe(include='all'))\n",
        "        print('')\n",
        "        print(\"Categories:\")\n",
        "        print('')\n",
        "        print(df[feat_cat].apply(lambda col: col.unique()))\n",
        "        print('')\n",
        "        print('--------')\n",
        "        \n",
        "    return id_cols,feat_num,feat_cat,features\n",
        "\n",
        "\n",
        "# https://pythonsimplified.com/how-to-handle-large-datasets-in-python-with-pandas/\n",
        "\n",
        "def reduce_mem_usage(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024**3\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**3\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "70zdSCUMA30F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Method to create, define and run a deep neural network model\n",
        "#\n",
        "def run_hcdr_model(\n",
        "    hidden_layer_neurons=[32, 16, 8],\n",
        "    opt=optim.SGD,\n",
        "    epochs=5,\n",
        "    learning_rate=1e-3\n",
        "):\n",
        "    \n",
        "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
        "    D_out = 2  # Output layer neurons - depend on what you're trying to predict, here, 2 classes: 0 and 1\n",
        "    \n",
        "    str_neurons = [str(h) for h in hidden_layer_neurons]\n",
        "    arch_string = f\"{D_in}-{'-'.join(str_neurons)}-{D_out}\"\n",
        "    \n",
        "    layers = [\n",
        "        torch.nn.Linear(D_in, hidden_layer_neurons[0]),  # X.matmul(W1)\n",
        "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
        "    ]\n",
        "    \n",
        "    # Add hidden layers\n",
        "    for i in range(1, len(hidden_layer_neurons)):\n",
        "        prev, curr = hidden_layer_neurons[i - 1], hidden_layer_neurons[i]\n",
        "        layers.append(torch.nn.Linear(prev, curr))\n",
        "        layers.append(nn.ReLU())\n",
        "        \n",
        "    \n",
        "    # Add final layer\n",
        "    layers.append(nn.Linear(hidden_layer_neurons[-1], D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
        "    \n",
        "    # Use the nn package to define our model and loss function.\n",
        "    # use the sequential API makes things simple\n",
        "    model = torch.nn.Sequential(*layers)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # use Cross Entropy and SGD optimizer.\n",
        "    loss_fn = nn.CrossEntropyLoss()  #for classfication \n",
        "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    #summary(model, (4, 20))\n",
        "    print('-'*50)\n",
        "    print('Model:')\n",
        "    print(model)\n",
        "    print('-'*50)\n",
        "    \n",
        "    '''\n",
        "    Training Process:\n",
        "        Load a batch of data.\n",
        "        Zero the grad.\n",
        "        Predict the batch of the data through net i.e forward pass.\n",
        "        Calculate the loss value by predict value and true value.\n",
        "        Backprop i.e get the gradient with respect to parameters\n",
        "        Update optimizer i.e gradient update\n",
        "    '''\n",
        "\n",
        "    loss_history = []\n",
        "    acc_history = []\n",
        "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
        "        running_loss = 0.0\n",
        "        count = 0\n",
        "        y_prob = []\n",
        "        y_pred = []\n",
        "        epoch_target = []\n",
        "        # dataset API gives us pythonic batching \n",
        "        for batch_id, data in enumerate(train_loader):\n",
        "            # https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
        "            inputs, target = data[0], data[1]\n",
        "            target = target.type(torch.LongTensor)   # casting to long\n",
        "            inputs, target = inputs.to(device), target.to(device)        \n",
        "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
        "            opt.zero_grad()\n",
        "            preds = model(inputs.float()) #prediction over the input data\n",
        "\n",
        "            # compute loss and gradients\n",
        "            loss = loss_fn(preds, target)    #mean loss for this batch\n",
        "\n",
        "            loss.backward() #calculate nabla_w\n",
        "            loss_history.append(loss.item())\n",
        "            opt.step()  #update W\n",
        "            \n",
        "            # https://stackoverflow.com/questions/60182984/how-to-get-the-predict-probability\n",
        "            y_prob.extend(F.softmax(preds, dim=1)[:,1].tolist())\n",
        "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
        "            epoch_target.extend(target.tolist())\n",
        "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "        loss = np.round(running_loss/count, 3)\n",
        "        \n",
        "        #accuracy\n",
        "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
        "        accuracy = correct.sum() / correct.size\n",
        "        accuracy = np.round(accuracy, 3)\n",
        "        \n",
        "        # auc\n",
        "        roc_auc = roc_auc_score(np.array(epoch_target),np.array(y_prob))\n",
        "        roc_auc = np.round(roc_auc,3)\n",
        "                           \n",
        "        return loss, accuracy, roc_auc\n",
        "\n",
        "\n",
        "\n",
        "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
        "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
        "        overall_loss = 0.0\n",
        "        count = 0\n",
        "        y_prob = []\n",
        "        y_pred = []\n",
        "        epoch_target = []\n",
        "        for i,data in enumerate(data_loader):\n",
        "            # https://stackoverflow.com/questions/69742930/runtimeerror-nll-loss-forward-reduce-cuda-kernel-2d-index-not-implemented-for\n",
        "            inputs, target = data[0], data[1]\n",
        "            target = target.type(torch.LongTensor)   # casting to long\n",
        "            inputs, target = inputs.to(device), target.to(device)                \n",
        "            preds = model(inputs.float())      \n",
        "\n",
        "            loss = loss_fn(preds, target)           # compute loss value\n",
        "\n",
        "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
        "            \n",
        "            y_prob.extend(F.softmax(preds, dim=1)[:,1].tolist())\n",
        "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
        "            epoch_target.extend(target.tolist())\n",
        "            count += 1\n",
        "\n",
        "        # compute mean loss\n",
        "        loss = np.round(overall_loss/count, 3)\n",
        "        #accuracy\n",
        "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
        "        accuracy = correct.sum() / correct.size\n",
        "        accuracy = np.round(accuracy, 3)\n",
        "\n",
        "        # auc\n",
        "        roc_auc = roc_auc_score(np.array(epoch_target),np.array(y_prob))\n",
        "        roc_auc = np.round(roc_auc,3)\n",
        "\n",
        "        return loss, accuracy, roc_auc\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):      \n",
        "        train_loss, train_accuracy, train_auc = train_epoch(epoch, model, loss_fn, optimizer, trainloader_hcdr)\n",
        "        valid_loss, valid_accuracy, valid_auc = evaluate_model(epoch, model, loss_fn, optimizer, validloader_hcdr, tag = \"Validation\")\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        print(f\"----Train Accuracy: {train_accuracy}\\t Validation Accuracy: {valid_accuracy}\")\n",
        "        print(f\"----Train AUC-ROC: {train_auc}\\t Validation AUC-ROC: {valid_auc}\")\n",
        "    print(\"-\"*50)\n",
        "    test_loss, test_accuracy, test_auc = evaluate_model(epoch, model, loss_fn, opt, testloader_hcdr, tag=\"Test\")\n",
        "    \n",
        "    return arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc"
      ],
      "metadata": {
        "id": "Tu_kiwglMHQa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "IY-aqM5W04hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read-In and Merge"
      ],
      "metadata": {
        "id": "xZ8dzHAt1pvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read-in\n",
        "DATA_DIR =  \"/drive/MyDrive/ColabNotebooks/\"\n",
        "\n",
        "ds_names = (\n",
        "    # \"application_test\",   \n",
        "    \"application_train\", \"prevapp_agg_data_tr\", \"bureau_agg_data_trans_untrans\",  \n",
        "    \"ccb_agg_data_tr\", \"ip_agg_data_tr\", \"pos_agg_data_tr\"\n",
        ")  \n",
        "\n",
        "datasets_agg = {}\n",
        "\n",
        "for ds_name in ds_names:\n",
        "    print('---')\n",
        "    print(ds_name)\n",
        "    datasets_agg[ds_name] = pd.read_csv(os.getcwd() + DATA_DIR + f'{ds_name}.csv')\n",
        "    datasets_agg[ds_name] = reduce_mem_usage(datasets_agg[ds_name])"
      ],
      "metadata": {
        "id": "VWvkmBxpA32r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e06d39-f8ff-472b-b68e-1937af07ecd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "application_train\n",
            "Memory usage of dataframe is 0.28 MB\n",
            "Memory usage after optimization is: 0.06 MB\n",
            "Decreased by 79.2%\n",
            "---\n",
            "prevapp_agg_data_tr\n",
            "Memory usage of dataframe is 1.73 MB\n",
            "Memory usage after optimization is: 0.46 MB\n",
            "Decreased by 73.2%\n",
            "---\n",
            "bureau_agg_data_trans_untrans\n",
            "Memory usage of dataframe is 0.72 MB\n",
            "Memory usage after optimization is: 0.21 MB\n",
            "Decreased by 70.6%\n",
            "---\n",
            "ccb_agg_data_tr\n",
            "Memory usage of dataframe is 0.11 MB\n",
            "Memory usage after optimization is: 0.04 MB\n",
            "Decreased by 66.6%\n",
            "---\n",
            "ip_agg_data_tr\n",
            "Memory usage of dataframe is 0.17 MB\n",
            "Memory usage after optimization is: 0.06 MB\n",
            "Decreased by 63.4%\n",
            "---\n",
            "pos_agg_data_tr\n",
            "Memory usage of dataframe is 0.34 MB\n",
            "Memory usage after optimization is: 0.09 MB\n",
            "Decreased by 73.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# denormalize and clean text\n",
        "for ds_name in datasets_agg:\n",
        "    if ds_name == 'application_train':\n",
        "        agg_data = datasets_agg['application_train'].replace(to_replace='\\s+', value='_', regex=True) \\\n",
        "                                                    .replace(to_replace='\\-', value='_', regex=True) \\\n",
        "                                                    .replace(to_replace='\\/', value='_', regex=True) \\\n",
        "                                                    .replace(to_replace='\\(', value='', regex=True) \\\n",
        "                                                    .replace(to_replace='\\)', value='', regex=True) \\\n",
        "                                                    .replace(to_replace='\\:', value='', regex=True) \\\n",
        "                                                    .replace(to_replace='\\,', value='', regex=True)\n",
        "    else:\n",
        "        agg_data = agg_data.merge(datasets_agg[ds_name], on='SK_ID_CURR', how='left')\n",
        "\n",
        "\n",
        "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('Unnamed:')]\n",
        "agg_data = agg_data.loc[:,~agg_data.columns.str.startswith('SK_ID_PREV')]\n"
      ],
      "metadata": {
        "id": "kAs9q2XTUrqA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_data.columns"
      ],
      "metadata": {
        "id": "LiZElhxhsMe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a61276d-373e-4aa0-a8a7-47995fae7bee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER',\n",
              "       'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL',\n",
              "       'AMT_CREDIT', 'AMT_ANNUITY',\n",
              "       ...\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Signed_median',\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Signed_mean',\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Signed_var',\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Completed_median',\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Completed_mean',\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Completed_var',\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Approved_median',\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Approved_mean',\n",
              "       'PS_O_NAME_CONTRACT_STATUS_Approved_var', 'PS_O_SK_ID_PREV_count_y'],\n",
              "      dtype='object', length=1463)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "J7gyiRiE1JQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Collinearity Reduction"
      ],
      "metadata": {
        "id": "m0v9NqWR1SgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deep learning model\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# create train, validation, and test sets\n",
        "y = agg_data['TARGET']\n",
        "X = agg_data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
        "\n",
        "_, X, _, y = train_test_split(X, y, test_size=0.6, random_state=42, stratify=y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "print(f\"X train           shape: {X_train.shape}\")\n",
        "print(f\"X validation      shape: {X_valid.shape}\")\n",
        "print(f\"X test            shape: {X_test.shape}\")\n",
        "\n",
        "# determine feature types\n",
        "id_col, feat_num, feat_cat, feature =  id_num_cat_feature(X, text = False)\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "### Collinear Feature Reduction\n",
        "\n",
        "# reduce numerical features by collinearity reduction\n",
        "\n",
        "# cr = make_pipeline(\n",
        "#     SimpleImputer(strategy='median'),\n",
        "#     StandardScaler(),    \n",
        "#     CollinearityReducer(attribute_names=feat_num, threshold = 0.5, max_iter=25)\n",
        "# )\n",
        "\n",
        "# tic = time.perf_counter()\n",
        "# reduced_feat_num = cr.fit_transform(X_train[feat_num], y_train) \n",
        "# toc = time.perf_counter()\n",
        "\n",
        "# print(f\"Collinearity Reduction completed in {toc - tic:0.4f} seconds.\")\n",
        "# print(f'Reduced numerical column count by {len(reduced_feat_num)/len(feat_num)}% through collinearity reduction.')\n",
        "# print(f'From {len(feat_num)} columns to {len(reduced_feat_num)} columns.')\n"
      ],
      "metadata": {
        "id": "w-VtWvcq3773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09ce714-15eb-417d-b26c-8bac24d2949c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train           shape: (118084, 1461)\n",
            "X validation      shape: (29521, 1461)\n",
            "X test            shape: (36902, 1461)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Pipeline"
      ],
      "metadata": {
        "id": "FAqUW765mIe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Main Pipeline\n",
        "\n",
        "# Pipeline\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    # ('selector', DataFrameSelector(reduced_feat_num)),\n",
        "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
        "    ('std_scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "data_pipeline = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num_pipeline\", num_pipeline, feat_num),\n",
        "        (\"cat_pipeline\", cat_pipeline, feat_cat)\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "X_train = data_pipeline.fit_transform(X_train)\n",
        "X_valid = data_pipeline.transform(X_valid) #Transform validation set with the same constants\n",
        "X_test = data_pipeline.transform(X_test) #Transform test set with the same constants\n",
        "\n",
        "y_train = y_train.to_numpy()\n",
        "y_valid = y_valid.to_numpy()\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "# convert numpy arrays to tensors\n",
        "X_train_tensor = torch.from_numpy(X_train)\n",
        "X_valid_tensor = torch.from_numpy(X_valid)\n",
        "X_test_tensor = torch.from_numpy(X_test)\n",
        "y_train_tensor = torch.from_numpy(y_train)\n",
        "y_valid_tensor = torch.from_numpy(y_valid)\n",
        "y_test_tensor = torch.from_numpy(y_test)\n",
        "\n",
        "# create TensorDataset in PyTorch\n",
        "hcdr_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "hcdr_valid = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "hcdr_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "# create dataloader\n",
        "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
        "train_batch_size = 96\n",
        "valid_test_batch_size = 64\n",
        "trainloader_hcdr = torch.utils.data.DataLoader(hcdr_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
        "validloader_hcdr = torch.utils.data.DataLoader(hcdr_valid, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
        "testloader_hcdr = torch.utils.data.DataLoader(hcdr_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "zU6rXxnMl94W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2c8dce-f143-4bdd-ba0c-9ca03ccd4dfd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118084, 1585) (118084,) (36902, 1585) (36902,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#==================================================#\n",
        "#    Modify START   #\n",
        "#==================================================#\n",
        "'''\n",
        "(input_dataset) - description of input data: size_set_transformed_columns_CR\n",
        "(hidden_layers_neurons) - A list of the number of neurons in the hidden layers in order. DEFAULT: [32, 16, 8] => 1st hidden layer: 32 neurons, 2nd: 16, 3rd: 8\n",
        "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
        "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
        "(learning_rate) - The learning rate to take the gradient descent step with\n",
        "'''\n",
        "input_dataset=\"0.6_agg_trans+orig_1585_noCR\"\n",
        "hidden_layer_neurons=[128, 64, 32, 16, 4]\n",
        "opt=optim.Adam\n",
        "epochs=10\n",
        "learning_rate=5e-3\n",
        "\n",
        "#==================================================#\n",
        "#    Modify END #\n",
        "#==================================================#\n",
        "\n",
        "arch_string, train_accuracy, valid_accuracy, test_accuracy, train_auc, valid_auc, test_auc = run_hcdr_model(\n",
        "    hidden_layer_neurons,\n",
        "    opt,\n",
        "    epochs,\n",
        "    learning_rate\n",
        ")\n",
        "    \n",
        "\n",
        "try: hcdrLog \n",
        "except : hcdrLog = pd.DataFrame(\n",
        "    columns=[\n",
        "        \"Dataset\",\n",
        "        \"Architecture string\", \n",
        "        \"Optimizer\", \n",
        "        \"Epochs\", \n",
        "        \"Learning Rate\",\n",
        "        \"Train accuracy\",\n",
        "        \"Valid accuracy\",\n",
        "        \"Test accuracy\",\n",
        "        \"Train auc\",\n",
        "        \"Valid auc\",\n",
        "        \"Test auc\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "hcdrLog.loc[len(hcdrLog)] = [\n",
        "    input_dataset,\n",
        "    arch_string, \n",
        "    f\"{opt}\", \n",
        "    f\"{epochs}\", \n",
        "    f\"{learning_rate}\",\n",
        "    f\"{train_accuracy * 100}%\",\n",
        "    f\"{valid_accuracy * 100}%\",\n",
        "    f\"{test_accuracy * 100}%\",\n",
        "    f\"{train_auc * 100}%\",\n",
        "    f\"{valid_auc * 100}%\",\n",
        "    f\"{test_auc * 100}%\",\n",
        "]\n",
        "\n",
        "hcdrLog"
      ],
      "metadata": {
        "id": "YR76icicV_9P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2aea49c9-0d31-4e6b-c83f-da6c7b95a85e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Model:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=1585, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (7): ReLU()\n",
            "  (8): Linear(in_features=16, out_features=4, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=4, out_features=2, bias=True)\n",
            ")\n",
            "--------------------------------------------------\n",
            "Epoch 1\n",
            "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
            "----Train AUC-ROC: 0.714\t Validation AUC-ROC: 0.745\n",
            "Epoch 2\n",
            "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
            "----Train AUC-ROC: 0.758\t Validation AUC-ROC: 0.755\n",
            "Epoch 3\n",
            "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
            "----Train AUC-ROC: 0.769\t Validation AUC-ROC: 0.762\n",
            "Epoch 4\n",
            "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
            "----Train AUC-ROC: 0.78\t Validation AUC-ROC: 0.759\n",
            "Epoch 5\n",
            "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
            "----Train AUC-ROC: 0.791\t Validation AUC-ROC: 0.752\n",
            "Epoch 6\n",
            "----Train Accuracy: 0.919\t Validation Accuracy: 0.919\n",
            "----Train AUC-ROC: 0.798\t Validation AUC-ROC: 0.751\n",
            "Epoch 7\n",
            "----Train Accuracy: 0.921\t Validation Accuracy: 0.917\n",
            "----Train AUC-ROC: 0.808\t Validation AUC-ROC: 0.75\n",
            "Epoch 8\n",
            "----Train Accuracy: 0.923\t Validation Accuracy: 0.914\n",
            "----Train AUC-ROC: 0.822\t Validation AUC-ROC: 0.747\n",
            "Epoch 9\n",
            "----Train Accuracy: 0.924\t Validation Accuracy: 0.915\n",
            "----Train AUC-ROC: 0.834\t Validation AUC-ROC: 0.739\n",
            "Epoch 10\n",
            "----Train Accuracy: 0.926\t Validation Accuracy: 0.914\n",
            "----Train AUC-ROC: 0.847\t Validation AUC-ROC: 0.73\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Dataset    Architecture string  \\\n",
              "0  0.6_agg_trans+orig_1585_noCR  1585-128-64-32-16-4-2   \n",
              "1  0.6_agg_trans+orig_1585_noCR  1585-128-64-32-16-4-2   \n",
              "\n",
              "                         Optimizer Epochs Learning Rate      Train accuracy  \\\n",
              "0    <class 'torch.optim.sgd.SGD'>     10         0.005               91.9%   \n",
              "1  <class 'torch.optim.adam.Adam'>     10         0.005  92.60000000000001%   \n",
              "\n",
              "  Valid accuracy Test accuracy           Train auc Valid auc Test auc  \n",
              "0          91.9%         91.9%  65.10000000000001%     64.5%    64.9%  \n",
              "1          91.4%         91.4%               84.7%     73.0%    73.8%  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00ed9f87-d829-4757-a83f-c704612bd6a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Architecture string</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Train accuracy</th>\n",
              "      <th>Valid accuracy</th>\n",
              "      <th>Test accuracy</th>\n",
              "      <th>Train auc</th>\n",
              "      <th>Valid auc</th>\n",
              "      <th>Test auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.6_agg_trans+orig_1585_noCR</td>\n",
              "      <td>1585-128-64-32-16-4-2</td>\n",
              "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
              "      <td>10</td>\n",
              "      <td>0.005</td>\n",
              "      <td>91.9%</td>\n",
              "      <td>91.9%</td>\n",
              "      <td>91.9%</td>\n",
              "      <td>65.10000000000001%</td>\n",
              "      <td>64.5%</td>\n",
              "      <td>64.9%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.6_agg_trans+orig_1585_noCR</td>\n",
              "      <td>1585-128-64-32-16-4-2</td>\n",
              "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
              "      <td>10</td>\n",
              "      <td>0.005</td>\n",
              "      <td>92.60000000000001%</td>\n",
              "      <td>91.4%</td>\n",
              "      <td>91.4%</td>\n",
              "      <td>84.7%</td>\n",
              "      <td>73.0%</td>\n",
              "      <td>73.8%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00ed9f87-d829-4757-a83f-c704612bd6a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00ed9f87-d829-4757-a83f-c704612bd6a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00ed9f87-d829-4757-a83f-c704612bd6a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hcdrLog = pd.DataFrame(\n",
        "#     columns=[\n",
        "#         \"Dataset\",\n",
        "#         \"Architecture string\", \n",
        "#         \"Optimizer\", \n",
        "#         \"Epochs\", \n",
        "#         \"Learning Rate\",\n",
        "#         \"Train accuracy\",\n",
        "#         \"Valid accuracy\",\n",
        "#         \"Test accuracy\",\n",
        "#         \"Train auc\",\n",
        "#         \"Valid auc\",\n",
        "#         \"Test auc\"\n",
        "#     ]\n",
        "# )"
      ],
      "metadata": {
        "id": "BrkIsbVX_0zs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hcdrLog"
      ],
      "metadata": {
        "id": "DeGGB6KUnsD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "e9cda9ed-2b19-42b4-95ce-f87a704d39d7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Dataset    Architecture string  \\\n",
              "0  0.6_agg_trans+orig_1585_noCR  1585-128-64-32-16-4-2   \n",
              "1  0.6_agg_trans+orig_1585_noCR  1585-128-64-32-16-4-2   \n",
              "\n",
              "                         Optimizer Epochs Learning Rate      Train accuracy  \\\n",
              "0    <class 'torch.optim.sgd.SGD'>     10         0.005               91.9%   \n",
              "1  <class 'torch.optim.adam.Adam'>     10         0.005  92.60000000000001%   \n",
              "\n",
              "  Valid accuracy Test accuracy           Train auc Valid auc Test auc  \n",
              "0          91.9%         91.9%  65.10000000000001%     64.5%    64.9%  \n",
              "1          91.4%         91.4%               84.7%     73.0%    73.8%  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da115e49-17e2-4fc2-9f3c-86d8ebb8820e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Architecture string</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Train accuracy</th>\n",
              "      <th>Valid accuracy</th>\n",
              "      <th>Test accuracy</th>\n",
              "      <th>Train auc</th>\n",
              "      <th>Valid auc</th>\n",
              "      <th>Test auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.6_agg_trans+orig_1585_noCR</td>\n",
              "      <td>1585-128-64-32-16-4-2</td>\n",
              "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
              "      <td>10</td>\n",
              "      <td>0.005</td>\n",
              "      <td>91.9%</td>\n",
              "      <td>91.9%</td>\n",
              "      <td>91.9%</td>\n",
              "      <td>65.10000000000001%</td>\n",
              "      <td>64.5%</td>\n",
              "      <td>64.9%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.6_agg_trans+orig_1585_noCR</td>\n",
              "      <td>1585-128-64-32-16-4-2</td>\n",
              "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
              "      <td>10</td>\n",
              "      <td>0.005</td>\n",
              "      <td>92.60000000000001%</td>\n",
              "      <td>91.4%</td>\n",
              "      <td>91.4%</td>\n",
              "      <td>84.7%</td>\n",
              "      <td>73.0%</td>\n",
              "      <td>73.8%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da115e49-17e2-4fc2-9f3c-86d8ebb8820e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da115e49-17e2-4fc2-9f3c-86d8ebb8820e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da115e49-17e2-4fc2-9f3c-86d8ebb8820e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PcEXb_u1uJ2p"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}