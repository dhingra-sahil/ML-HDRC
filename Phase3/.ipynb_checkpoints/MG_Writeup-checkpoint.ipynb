{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63c77e1-08a7-4af6-adb3-0a9e70c9d62a",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines\n",
    "\n",
    "Many models were run and compared using different combinations of data pre-processing techniques, degrees of feature selection, and hyperparameter tunes. This section outlines the experiments and interprets their results. The main challenge of this section was developing models which could *actually* run and would not crash the python kernel. The computational cost of these algorithms is a common theme and will be revisited often in discussion.\n",
    "\n",
    "## Pre-Processing Techniques\n",
    "\n",
    "Several techniques were employed to pre process data in various ways, and for different purposes. These are discussed in the sections below.\n",
    "\n",
    "### Feature Engineering - UPDATE WITH SECTION XXX info \n",
    "\n",
    "See section XXX for detailed explanations of the methods used to re-engineer and transform features in the various source tables. Untransformed and transformed datasets were one comparison the ML experiments addressed. \n",
    "\n",
    "### Data Type Optimization\n",
    "\n",
    "When aggregated, the dataset is very large, approximately 2.5 gb of space. When reading these data in as-is, the large memory required to work with the dataset made it a cumbersome object on which to operate ML pipelines. The `reduce_mem_usage()` function is used immediately upon data read-in to counter this large memory requirement. Where possible, this function changes a given column's default datatype to a datatype with a smaller memory footprint. For example, an `int64` column composed of only 1s and 0s (IE OHE columns...) might be converted to an `int8` column with no data loss, and significant memory reduction.\n",
    "\n",
    "This operation typically led to a ~70% memory size reduction of the imported dataset making it much easier to work with. The function is provided below with an example output. Credit to the publisher: https://pythonsimplified.com/how-to-handle-large-datasets-in-python-with-pandas/\n",
    "\n",
    "```python\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**3\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**3\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "[Example Output]:\n",
    "\n",
    "<img src=\"../images/mem_reducer.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "### Collinearity Reduction\n",
    "\n",
    "During Phase 2 exploratory data anlysis, multicollinearity between input variables was shown to be prevalent in most of the data tables. Multicollinearity was further increased during the aggregation process as numeric input variables were proliferated into their mean, median, variance counterparts (amongst other aggregation types). This introduction of new (possibly) redundant variables to the ML algorithms significantly increases computation time and often reduces algorithm accuracy/effectiveness. \n",
    "\n",
    "Used in conjunction with the `DataframeSelector()` transformer class, the `CollinearityReducer()` transformer class combats multicollinearity by removing the *most* (multi-) collinear columns which are *least* correlated to the target variable. The algorithm steps are described below: \n",
    "\n",
    "1. Given input variable *X* and target variable *y*, calculate the correlation matrix\n",
    "2. Pivot the correlation matrix into a long dataframe of correlation pairs and values (input variable 1, input variable 2, absolute correlation) and drop the following variable pairs:\n",
    "   + any pair including the target variable\n",
    "   + any pair with two of the same input variable\n",
    "   + any pair with an absolute correlation value below a specified threshold value (default is 0.5)\n",
    "3. For each variable pair, compare to see which variable is *more* correlated to the target variable. These input variables are given a 'win' while the input variable in the pair which is *less* correlated to the target variable is given a 'loss'. \n",
    "4. Count the total 'wins' and 'losses' for each variable and drop from the original dataframe any variable with 0 wins. \n",
    "5. Repeat steps 1 to 4 until there are no more input variable pairs with correlations above the threshold, no more input variables scoring 0 wins, or the specified maximum number of iterations has been reached. \n",
    "\n",
    "The `CollinearityReducer()` thus applies a common multicollinearity solution - drop the variable which is least correlated to the target (Introduction to Statistical Learning Chapter 3). This is no simple task for human to perform on a high-dimensional dataset, but this class provides a mechanical solution so it does not have to be done manually. the `transform` method of this class creates a list of attribute names from the original dataframe which are *to be kept* - multicollinear classes culled by the `CollinearityReducer()` are *not* included in the output. This output list is then used as the input for the `DataframeSelector()` class in the pipeline. Thus, the `CollinearityReducer()` selects columns based only on the training data and there is no leakage from the validation or test data. \n",
    "\n",
    "Notably, this algorithm is only applied to numerical variables as it is assumed there should natrually be some elevated degree of collinearity between one-hot-encoded categorical variables. Furthermore, the algorithm should be applied to numerical data which has been subjected to the same scaling and imputation strategy as applied in the actual pipeline. \n",
    "\n",
    "Both the `CollinearityReducer()` and `DataframeSelector()` classes are shown below along with a basic example of their usage. \n",
    "\n",
    "```python\n",
    "# transformer reduces the list of columns by a subset\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "# transformer produces a reduced column list by collinearity reduction\n",
    "class CollinearityReducer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    '''\n",
    "    This class reduces features by measuring collinearity between the input variables and target.\n",
    "    Works on numerical features based on the correlations between each variable pair.\n",
    "    Of the var1iable pairs with absolute correlations above the threshold value the variables with the lowest target variable correlation are dropped from the input X.\n",
    "    Repeat until no more collinear pairs with absolute correlations above the threshold or max_iter. \n",
    "    \n",
    "    Inputs:\n",
    "       X (numpy array) - input variables\n",
    "       y (numpy array) - target variable\n",
    "       attribute_names (list) - column names of the input variables (from original dataframe order\n",
    "       threshold (int) - the absolute correlation threshold above which variable pairs are subject to the 'correlation competition'\n",
    "       max_iter (int) - the maximum number of iterations to cut off the algorithm\n",
    "       \n",
    "    Output:\n",
    "       list - attribute_names to be used by DataframeSelector class\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, attribute_names, threshold=0.5, max_iter=None):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.threshold = threshold\n",
    "        self.max_iter = max_iter\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None): \n",
    "        \n",
    "        dataframe = pd.concat([y, pd.DataFrame(X)], axis=1)\n",
    "        \n",
    "        i = 0\n",
    "        while i <= self.max_iter:\n",
    "\n",
    "            # read-in and assign columns\n",
    "            # gets correlation matrix between variables and pivots to a longer df\n",
    "            # identify target variable\n",
    "            # drop same-name and target correlations pairs\n",
    "              \n",
    "            df = dataframe\n",
    "            features = df.iloc[:,1:].columns\n",
    "            target_name = df.iloc[:,0].name\n",
    "\n",
    "            df = pd.melt(abs(df.corr()).reset_index(), id_vars='index', value_vars=features)\n",
    "            targets = df[df['index']==target_name]\n",
    "            df = df[(df['index'] != df['variable']) & (df['index'] != target_name) & (df['variable'] != target_name)]\n",
    "\n",
    "            # combine the correlated variables into ordered pairs\n",
    "            # aggregate the max correlation and sort pairs\n",
    "            # split out the variables from the pair\n",
    "            # join the target variable correlations for each variable pair\n",
    "\n",
    "            df['joined'] = df[['index', 'variable']].apply(lambda row: '::'.join(np.sort(row.values.astype(str))), axis=1)\n",
    "\n",
    "            df = df.groupby('joined', as_index=False) \\\n",
    "                   .agg({'value':'max'}) \\\n",
    "                   .sort_values(by='value', ascending=False)\n",
    "\n",
    "            df[['var_1','var_2']] = df['joined'].str.split(\"::\",expand=True).astype(int)\n",
    "\n",
    "            df = df.merge(targets, how='left', left_on='var_1', right_on='variable') \\\n",
    "                   .merge(targets, how='left', left_on='var_2', right_on='variable')\n",
    "            df.rename(columns = {'value_x':'var_pair_corr', 'value_y':'var_1_target_corr', 'value':'var_2_target_corr'}, inplace = True)\n",
    "\n",
    "            # Take only variable pairs with a correlation greater than threshold\n",
    "            # determine which variable has a higher correlation with the target.\n",
    "            # The higher of the two gets marked as a win\n",
    "            # While the other gets marked as a loss\n",
    "            # the wins and losses for each variable are then grouped and summed\n",
    "\n",
    "            exceeds = df[df['var_pair_corr']>self.threshold]\n",
    "\n",
    "            # break if none above threshold\n",
    "            if len(exceeds['var_pair_corr'])==0:\n",
    "                break\n",
    "\n",
    "            # \"correlation competition\"\n",
    "            exceeds['var_1_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] >= row[\"var_2_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_1_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] >= row[\"var_1_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_2_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] < row[\"var_2_target_corr\"] else 0, axis=1)\n",
    "            exceeds['var_2_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] < row[\"var_1_target_corr\"] else 0, axis=1)\n",
    "\n",
    "            # aggregate scores\n",
    "            var1 = exceeds[['var_1', 'var_1_win', 'var_1_loss']].groupby('var_1', as_index=False) \\\n",
    "                                                                .agg({'var_1_win':'sum', 'var_1_loss':'sum'})\n",
    "            var1.rename(columns = {'var_1':'var', 'var_1_win':'win', 'var_1_loss':'loss'}, inplace=True)\n",
    "\n",
    "            var2 = exceeds[['var_2', 'var_2_win', 'var_2_loss']].groupby('var_2', as_index=False) \\\n",
    "                                                                .agg({'var_2_win':'sum', 'var_2_loss':'sum'})\n",
    "            var2.rename(columns = {'var_2':'var', 'var_2_win':'win', 'var_2_loss':'loss'}, inplace=True)\n",
    "\n",
    "            corrcomps = pd.concat([var1,var2], axis=0).groupby('var', as_index=False) \\\n",
    "                                                      .agg({'win':'sum', 'loss':'sum'})\n",
    "\n",
    "            # drop variables which had 0 wins\n",
    "            # IE collinear variables which were always least related to the target\n",
    "            dropvars = corrcomps[corrcomps['win']==0]['var']\n",
    "\n",
    "            dataframe = dataframe.drop(dropvars, axis=1)  \n",
    "\n",
    "            i += 1  \n",
    "        \n",
    "        X = [self.attribute_names[col] for col in dataframe.columns]\n",
    "\n",
    "        return X\n",
    "    \n",
    "    \n",
    "### Example Usage ###\n",
    "\n",
    "# determine feature types, reduce numerical features by collinearity reduction\n",
    "id_col, feat_num, feat_cat, feature =  id_num_cat_feature(X_train, text = False)\n",
    "\n",
    "cr = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(),    \n",
    "    CollinearityReducer(attribute_names=feat_num, threshold = 0.7, max_iter=2)\n",
    ")\n",
    "\n",
    "reduced_feat_num = cr.fit_transform(X_train[feat_num], y_train)\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(reduced_feat_num)),\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875be59-8127-4fb1-a6da-84de58b34706",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning was co,nducted using `GridSearchCV` to methodically test different combinations of hyperparameters. Identical pipelines were run for various combinations of `CollinearityReducer()` hyperparameters to tune this transformer also. Oftentimes, only a subset of the data was used to train and tune models due to the immensity of the dataset - this alleviated computation time headaches. By the Law-of-Large-Numbers, optimal hyperparameters found on these 'micro pipes' are good proxies for optimal parameters on the pipelines using the full dataset. \n",
    "\n",
    "An example of a \"base\" pipeline upon which various parameters and models were tuned is shown below. The Area-Under-the-Curve is the scoring metric used at is provides a better measure of fit than the Accuracy score. Where an AUC-ROC score of 0.5 indicates a model composed of randomly guessing, an AUC-ROC score of 1 represents a perfect model. For reference, the baseline logistic regression model scored an AUC-ROC score of ~0.76.\n",
    "\n",
    "```python\n",
    "# example basic pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(reduced_feat_num)),  # use only if CollinearityReducer() implemented\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "data_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, feat_num),\n",
    "        (\"cat_pipeline\", cat_pipeline, feat_cat)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "    (\"preparation\", data_pipeline),\n",
    "    (\"classifier\", Classifier())  # ML classifier\n",
    "])\n",
    "\n",
    "param_1 = [5, 10, 25, 50, 100]\n",
    "param_2 = [0.1, 1, 10]\n",
    "\n",
    "parameters = dict(\n",
    "    classifier__param_1 = param_1,\n",
    "    classifier__param_2 = param_2\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    full_pipeline_with_predictor, param_grid = parameters, \n",
    "    cv = 3, n_jobs = 4, scoring = 'roc_auc', verbose = 2\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Logistic Regression Model Tuning\n",
    "\n",
    "The baseline pipeline is a Logistic Regression pipeline with numerical and categorical column transformers. A number of pipeline experiments were conducted and the results of these are shown below. For the sake of brevity, code is only included for the pipelines which gave the most impactful insights into hyperparameter tuning - otherwise only results of the experimental round are show along with bulleted explanations of the pipeline parameters and results.\n",
    "\n",
    "#### Logistic Regression with Bureau and Application Data\n",
    "\n",
    "+ These pipelines were tested using only data aggregated from `bureau.csv` and `bureau balance.csv`, and from `application_train.csv`\n",
    "+ Used 40% of available data\n",
    "+ Both pipelines used logistic regression with L1 regularization\n",
    "+ **Comparisons:**\n",
    "   + untransformed vs transformed variables \n",
    "   + L1 vs L2 regularization\n",
    "+ **Interpretations:** \n",
    "   + The pipeline *without* the transformed data performs better\n",
    "   + L1 was preferred regularization parameter for *both* pipelines\n",
    "\n",
    "<img src=\"../images/tunes/LR_appbureau_L1_v_L2_trans_v_untrans.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "#### Logistic Regression with All Untransformed Data - Regularization\n",
    "\n",
    "+ These pipelines were tested using data aggregated from all the child datasets, and from `application_train.csv`\n",
    "+ Used 40% of available data\n",
    "+ Both pipelines used untransformed data (not feature engineered) with collinearity reduction `(threshold=0.7, max_iter=2)`\n",
    "+ **Comparisons:**\n",
    "   + L1 vs L2 regularization\n",
    "+ **Interpretations:** \n",
    "   + Pipeline with L1 regularization parameter performed better\n",
    "\n",
    "<img src=\"../images/tunes/LR_agg1293_CR_noreg_v_l1.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "#### Logistic Regression with All Untransformed Data - Collinearity Reduction\n",
    "\n",
    "+ These pipelines were tested using data aggregated from all the child datasets, and from `application_train.csv`\n",
    "+ Used 40% of available data\n",
    "+ All pipelines used untransformed data (not feature engineered) with L1 regularization\n",
    "+ **Comparisons:**\n",
    "   + Collinearity Reduction\n",
    "+ **Interpretations:** \n",
    "   + Best performing Pipeline is the one with `CollinearityReducer(threshold=0.5, max_iter=10)`\n",
    "\n",
    "<img src=\"../images/tunes/LR_agg1293_L1_CR_comparison.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf39af-90ac-4fa7-8b3c-5f5175a76c8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nonparametric Model Tuning\n",
    "\n",
    "Nonparametric classification models (as opposed to *parametric* classification models like logistic regression) do not rely on underlying assumptions on our dataset. This can be useful for high dimensional data. Decision Tree (\"DT\") models build trees of binary decisions by which to classify values in a feature space into its target classifications. Used in ensemble (many trees used together), these models can be very powerful! In this project, we explore two types of nonparametric models to compare and tune: *Random Forest* DT models and *Gradient Boosted* DT models.\n",
    "\n",
    "> + In **Random Forest Models**, the trees are grown independently on random samples of the observations. However, each split on each tree is performed using a random subset of the features, thereby decorrelating the trees, and leading to a more thorough exploration of model space relative to bagging. This algorithm combines the output of multiple (randomly created) Decision Trees to generate the final output.  In this algorithm, each node in the decision tree is grown based on a random subset of features and subset of the input features.\n",
    "> + In **Gradient Boosted Models**, we only use the original data, and do not draw any random samples. The trees are grown successively, using a “slow” learning approach: each new tree is fit to the signal that is left over from the earlier trees, and shrunken down before it is used. These trees incrementally added to an ensemble by training each new instance to emphasize the training instances previously mis-modeled.\n",
    "> \n",
    "> *description excerpts from course notes*\n",
    "\n",
    "Tuning and testing of these models are explored below.\n",
    "\n",
    "#### Random Forests and Gradient Boosting with Bureau and Application Data\n",
    "\n",
    "+ These pipelines were tested using data aggregated from `bureau.csv` and `bureau balance.csv`, and from `application_train.csv`\n",
    "+ Used 40% of available data for the first 4 experiments, and 80% of data for the last two experiments\n",
    "+ **Comparisons:**\n",
    "   + Random Forest and XGB (gradient boosted) classifiers\n",
    "   + Collinearity Reduction (RF only)\n",
    "   + Transformed vs Untransformed\n",
    "   + training size \n",
    "   + Random Forest Hyperparameters\n",
    "+ **Interpretations:** \n",
    "   + Best performing Pipeline is the one with `CollinearityReducer(threshold=0.5, max_iter=10)`, Random Forest Algorithm on Transformed data with {'rf__max_depth': 25, 'rf__min_samples_leaf': 25}\n",
    "   + The `CollinearityReducer()` helped the Random Forest algorithm\n",
    "   + Transformed data tended to do better than untransformed data for both Random Forest and XGB algorithms\n",
    "   + XGB generally outperformed Random Forest\n",
    "   + Larger training dataset improved the XGB scores\n",
    "\n",
    "<img src=\"../images/tunes/RF_XGB_appbureau_comp_CR_trans.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "#### Random Forests and Gradient Boosting with All Data Except Credit Card\n",
    "\n",
    "+ These pipelines were tested using data aggregated from all the child datasets (except credit card), and from `application_train.csv`\n",
    "+ Used 10% of available data\n",
    "+ Used transformed data\n",
    "+ **Comparisons:**\n",
    "   + Random Forest and XGB (gradient boosted) classifiers\n",
    "   + Collinearity Reduction\n",
    "   + Random Forest Hyperparameters\n",
    "+ **Interpretations:** \n",
    "   + Best performing Pipeline is the XGB algorithm {'xgb__subsample':0.8} with `CollinearityReducer(threshold=0.5, max_iter=25)`\n",
    "   + The `CollinearityReducer()` generally helped the XGB algorithm but generally hurt the Random Forest algorithm\n",
    "   + XGB generally outperformed Random Forest\n",
    "\n",
    "<img src=\"../images/tunes/RF_XGB_agg_noccb_trans_comp_CR.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "#### Gradient Boosting with All Data Except Credit Card\n",
    "\n",
    "+ These pipelines were tested using data aggregated from all the child datasets (except credit card), and from `application_train.csv`\n",
    "+ Used 10% of available data\n",
    "+ **Comparisons:**\n",
    "   + Collinearity Reduction\n",
    "   + Transformed vs untransformed data\n",
    "+ **Interpretations:** \n",
    "   + Best performing Pipeline is the XGB algorithm {'xgb__subsample':0.8} with no Collinearity Reduction on untransformed data\n",
    "   + The `CollinearityReducer()` generally improved performance with more iterations, but generally did not perform as well as models with a higher subsample parameter value *without* the `CollinearityReducer()`\n",
    "   + Untransformed data outperformed transformed data\n",
    "\n",
    "<img src=\"../images/tunes/XGB_agg_trans_comp_CR.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "<img src=\"../images/tunes/XGB_agg_comp_CR.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d815b2c-951e-4c26-9ca9-ee8a9e3d6586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
