{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<h2 style=\"text-align:center\">Phase 3: Exploratory Data Analysis</h2>**\n",
    "\n",
    "<h3 style=\"text-align:center\">Home Credit Default Risk</h3>\n",
    "\n",
    "**<h4 style=\"text-align:center\">Group 10</h4>**\n",
    "\n",
    "| | |\n",
    "|:-----:|:-----:|\n",
    "| Mark Green | Pragat Wagle |\n",
    "| [margree@iu.edu](mailto:margree@iu.edu) | [pwagle@iupui.edu](mailto:pwagle@iupui.edu) |\n",
    "| <img src=\"../images/picMark.png\" alt=\"drawing\" width=\"200\"/> | <img src=\"../images/picPragat.jpeg\" alt=\"drawing\" width=\"210\"/> |\n",
    "| <img src=\"../images/picSahil.jpg\" alt=\"drawing\" width=\"200\"/> | <img src=\"../images/picKunal.jpg\" alt=\"drawing\" width=\"230\"/> |\n",
    "| Sahil Dhingra | Kunal Singh |\n",
    "| [sahdhin@iu.edu](mailto:sahdhin@iu.edu) | [singhku@iu.edu](mailto:singhku@iu.edu) |\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "---\n",
    "\n",
    "**<h4 style=\"text-align:left\">Table of Contents</h4>**\n",
    "\n",
    "1. [Phase Leader Table](#phase)\n",
    "2. [Credit Assignment Plan](#credit)\n",
    "3. [Abstract](#abstract)\n",
    "4. [Introduction](#intro)\n",
    "5. [Data Description](#datadesc)\n",
    "6. [Exploratory Data Analysis](#EDA)\n",
    "7. [Visual EDA](#EDAV)\n",
    "8. [Baseline Machine Learning Pipelines](#pipelines)\n",
    "9. [Results and Discussion](#results)\n",
    "10. [Conclusion](#conclusion)\n",
    "11. [Bibliography](#bibliography)\n",
    "12. [Attachment 1 - Data Dictionary](#datadict)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Leader Table <a name='phase'></a>\n",
    "\n",
    "| Phase | Leader | Phase Description | Completed |\n",
    "|-------|:------:|:------------------|-----------|\n",
    "| 1     | Mark   | **Project Proposal**: Project Plan, Data description, algorithms and metrics, baseline models, pipelines | *11/15/22* | \n",
    "| 2     | Kunal  | **Baseline Report**: EDA, baseline pipeline, feature engineering, video presentation | *11/29/22* | \n",
    "| 3     | Pragat | **Finetuning Model**: Hyperparameter tuning, feature selection, ensemble methods, video presentation | 12/06/22 | \n",
    "| 4     | Sahil  | **Final Report**: Implement neural network, advanced models and loss functions, video presentation | TBD |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Assignment Table <a name='credit'></a>\n",
    "\n",
    "| Phase | Person | Hours | Effort |\n",
    "|:-----:|:-----:|:------:|:-----:|\n",
    "| 3 |   Mark   | 24 | Mark's goal was (1) to transform (IE denormalize) and perform EDA on the `bureau` and `bureau_balance` datasets, (2) to explore multicollinearity between variables in the aggregated dataset, (3) to facilitate production of the document deliverable, and (4) to help develop the summary video. Mark achieved his goal to complete these tasks by 11/29/2022. Mark accomplished these goals by writing code to aggregate and explore the `bureau` and `bureau_bal` tables, combining work from multiple documents into a singular report document and editing/formatting to produce a final draft document in html, pdf, and ipynb formats, and engaging in zoom meetings to facilitate workflow and develop the video. Accomplishing these goals completes a significant piece of the data preparation and provides insights into the `bureau` data and the \"rolled-up\" denormalized data. Accomplishing these goals also ensures we have high-quality deliverables that are suitable for reading by audiences with varied ML backgrounds. |\n",
    "| 3 |   Sahil    | 42 | Sahil’s goal was to write reusable functions like aggregation, summarization, ETL-transformations for EDA and functions like Num plots, categorical plots, NULL count, Dendrograms, histograms, and unique feature plots for EDA visualization. These goals were required to facilitate creation of new features, pre-processing, remove data below a threshold value(0.7), aggregation of hundreds of attributes for POS (POS-CASH Balance) and PREVAPP (previous applications) tables, and then rolling them to the application train and test tables. Helper functions were used for both EDA and visual EDA. Another goal was to publish details which could be added w.r.t functions. Last goal was to have the results and provide the discussion considering the feature engineering and score recorded in the last phase and doing a comparative analysis. This is all to be done before 11/29/2022. Sahil accomplished these goals by running and understanding the starter notebook, research on helpful EDA, EDA visualization, and feature engineering techniques which could be used across tables. Accomplishing these goals will enable Sahil to have better understanding of data visually and statistically. Accomplishing these goals will also provide insight for the next phase of targeted feature engineering and hyper-parameter tuning. |\n",
    "| 3   |  Pragat     | 34 | Pragat’s goal was to create visualizations including correlation plots and numerical plots such as a histogram, barplot, and kernel density tables. Another goal was to roll up the credit card balance table by summarizing and aggregating that data and then merging into the train data. Lastly, to run, train the pipeline on the final aggregated table and test it on the final aggregated test table to measure accuracy of the model on the test set, and submit to Kaggle. This is all to be done before 11/29/2022. Pragat will accomplish this goal by looking at the existing code, reading python documentation, and by using the web to find existing implementations. This goal will help with EDA to eventually create a more improved model from phase 1 to improve test set accuracy on the dataset provided by kaggle. |\n",
    "| 3    | Kunal      | 20 | Kunals goal was to load , transform and perform EDA on “installments payments” table. He explored the collinearity between the variables in the dataset. He also worked for preparing the documentation for Team and Plan updates, Project Abstract and description. He also helped develop the summary video. |\n",
    "| | | |\n",
    "| 2 |   Mark   | 24 | Mark's goal was (1) to transform (IE denormalize) and perform EDA on the `bureau` and `bureau_balance` datasets, (2) to explore multicollinearity between variables in the aggregated dataset, (3) to facilitate production of the document deliverable, and (4) to help develop the summary video. Mark achieved his goal to complete these tasks by 11/29/2022. Mark accomplished these goals by writing code to aggregate and explore the `bureau` and `bureau_bal` tables, combining work from multiple documents into a singular report document and editing/formatting to produce a final draft document in html, pdf, and ipynb formats, and engaging in zoom meetings to facilitate workflow and develop the video. Accomplishing these goals completes a significant piece of the data preparation and provides insights into the `bureau` data and the \"rolled-up\" denormalized data. Accomplishing these goals also ensures we have high-quality deliverables that are suitable for reading by audiences with varied ML backgrounds. |\n",
    "| 2 |   Sahil    | 42 | Sahil’s goal was to write reusable functions like aggregation, summarization, ETL-transformations for EDA and functions like Num plots, categorical plots, NULL count, Dendrograms, histograms, and unique feature plots for EDA visualization. These goals were required to facilitate creation of new features, pre-processing, remove data below a threshold value(0.7), aggregation of hundreds of attributes for POS (POS-CASH Balance) and PREVAPP (previous applications) tables, and then rolling them to the application train and test tables. Helper functions were used for both EDA and visual EDA. Another goal was to publish details which could be added w.r.t functions. Last goal was to have the results and provide the discussion considering the feature engineering and score recorded in the last phase and doing a comparative analysis. This is all to be done before 11/29/2022. Sahil accomplished these goals by running and understanding the starter notebook, research on helpful EDA, EDA visualization, and feature engineering techniques which could be used across tables. Accomplishing these goals will enable Sahil to have better understanding of data visually and statistically. Accomplishing these goals will also provide insight for the next phase of targeted feature engineering and hyper-parameter tuning. |\n",
    "| 2    |  Pragat     | 34 | Pragat’s goal was to create visualizations including correlation plots and numerical plots such as a histogram, barplot, and kernel density tables. Another goal was to roll up the credit card balance table by summarizing and aggregating that data and then merging into the train data. Lastly, to run, train the pipeline on the final aggregated table and test it on the final aggregated test table to measure accuracy of the model on the test set, and submit to Kaggle. This is all to be done before 11/29/2022. Pragat will accomplish this goal by looking at the existing code, reading python documentation, and by using the web to find existing implementations. This goal will help with EDA to eventually create a more improved model from phase 1 to improve test set accuracy on the dataset provided by kaggle. |\n",
    "| 2    | Kunal      | 20 | Kunals goal was to load , transform and perform EDA on “installments payments” table. He explored the collinearity between the variables in the dataset. He also worked for preparing the documentation for Team and Plan updates, Project Abstract and description. He also helped develop the summary video. |\n",
    "| | | |\n",
    "|1  |   Mark   |   16     | Mark's goals were to accomplish exploration and write about the data description, combine project elements developed by team members into a singular document, organize phase leader table, edit document appearance and content, guide discussion, goals, and organize team and resources as Phase 1 Leader, and to collaborate with team members where needed by November 15, 2022. Mark accomplished these goals by stitching together elements into a jupyter notebook and formatting the markdown into a report style, setting up a project Github resource and downloading/democratizing datasource, exploring and describing the data structure, editing the jupyter notebook submittal, being an active leader on team discussions, consulting other team members on writing in the Project Execution Plan, ML Algorithms, and ML Pipelines sections. Accomplishing these goals is key to project delivery in Phase 1 because these tasks complete the logistical elements of report development. They also contribute to the group understanding of the data structure and context. This work also sets precedents and tools for delivering future reports.     |\n",
    "| 1 |   Sahil    |    14     |  Sahil’s goal for this phase was to do EDA, feature engineering, create pipelines, create Block Diagram, conclude results and publish results for the same in Kaggle by 15-NOV-2022. To achieve this, Sahil went through the initial notebook and worked from there to do EDA, feature selection and create pipeline with different attribute list using the knowledge from prior assignments. Accomplishing this goal will help us get us to baseline test score and AUC score which will give us a good idea on our predictions and what more we can do to make better predictions.   |\n",
    "| 1    |  Pragat     |   12     |   Pragat’s goal was to describes the algorithms to be used and work on the getting it done prior to the November 15th deadline. Pragat will accomplish this goal by looking at the implementations on sklearn and the prioritize the most important parameters for each algorithm by looking at existing implementations seen in the documentation. Also by looking at existing implementations he will determine the loss functions and metrics and analysis to be used to measure the accuracy and overall quality of the algorithm. This goal will help to prioritize the most important arguments to consider for hyper-parameter tuning, optimizing the model, and in measuring the overall quality of results.    |\n",
    "| 1    | Kunal      |   10     |  Kunal's was to develop the Abstract and Project Execution plan. He accomplished this by collaborating with team members on regular team calls. This is important for the project as it helps to outline a plan of expectations and summarizes work done this week.|\n",
    "\n",
    "<img src=\"../images/GanttChart.png\" alt=\"drawing\" width=\"1500\"/>\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract <a name='abstract'></a>\n",
    "\n",
    "The Home Credit Group needs a machine learning-based classification model to make accurate lending decisions for individuals by predicting loan default risk using a scope of data beyond just traditional credit history, including loan applicant data encompassing their demographics, social status, employment status, and previous credit history. The seven data tables comprising these information use custom EDA and ETL functions to numerically and visually explore characteristics such as input variable distributions and skewness, inter-variable and target-variable correlation, and multicollinearity. Insights from the data exploration inform the process and decisions necessary to responsibly and accurately denormalize these tables into an ML-model ready input variable. This input variable - now comprised of the entire 2.7 gigabyte dataset - is run through a baseline logistic regression model in `SKLearn` using 1277 input features. Results from modelling on test data indicate the *Wagle-Dhingra-Singh-Green* baseline model achieves AUC-ROC score of approximately 0.7709, with prediction accuracy of approximately 92%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction <a name='intro'></a>\n",
    "\n",
    "Loans have always been an important part of people’s lives. Each individual has different reasons for borrowing a loan. It could be to buy a dream car or a home, to set up a business, or to buy some products. A large part of the population finds it difficult to get their home loans approved due to insufficient or absent credit history. It is a major challenge for banks and other finance lending agencies to decide for which candidates to approve housing loans.\n",
    "\n",
    "The Home Credit organization's mission is to provide responsible lending solutions to people with little to no credit history. Home Credit wants to determine their customer's eligibility for their financial products by determining their risk of default - the risk they will miss payments. **Group 10** has been tasked to evaluate their data and develop a machine learning (\"ML\") classification tool to predict loan default risk which uses more features than just the traditional credit history. Once developed, the *Wagle-Dhingra-Singh-Green model* will eventaully be used as a tool to evaluate future loan candidates. \n",
    "\n",
    "This report discusses the integration of the full normalized dataset into a machine learning model. The Exploratory Data Analysis (*EDA*) performed throughout this process is also presented as a means to familiarize the data and to make informed decisions throughout the data engineering and model integration process. \n",
    "\n",
    "A note on the styling of this report. After Section 5, many sections will be broken up by inline code blocks as the aim of this report is to highlight the exploratory data analysis and the work we are performing. \n",
    "\n",
    "## Phase 2 - Tasks To be Tackled\n",
    "\n",
    "The focus of Phase 2 is Exploratory Data Analysis and integration of the full dataset into a baseline machine learning model. A generalized machine learning project pipeline is outlined in figure 3 below and each step is described in detail in the Phase 1 Project report. The steps relevant to this phase (2, 3, 4) are outlined in detail below. \n",
    "\n",
    "2. **Data Logging and ETL**\n",
    "   + The data is segregated in multiple csv files related to each other by primary and foreign keys. Extract-Transform-Load operations must be performed on the dataset to consolidate the datasources into single denormalized table for efficient analysis.\n",
    "3. **Exploratory Data Analysis**\n",
    "   + While integrating the data, we explore the data with the goal of answering questions like: \n",
    "      + How many features are present?\n",
    "      + How are they correlated to one another and the target? \n",
    "      + How is the data quality? Are there any problems such as high leverage points or multicollinearity?\n",
    "      + What are the data types, missing values, or non-numeric features? \n",
    "      + Are there any patterns between the predictor and response features?\n",
    "   + The denormalized data is explored to find features which have the most impact on predicting the default risk of a loan applicant. Additional feature characteristics such as correlation between features and skewness are also observed and noted at this stage. \n",
    "4. **Feature Engineering**\n",
    "   + Using unnecessary features to train a machine learning model reduces the overall model accuracy and increases the model complexity and bias. To counter these issues, Feature Selection is implemented to reduce the number of input variables for the model by using only features with a relevant effect on predicting the target variable. Some feature selection methods include finding correlation coefficients, forward selection, backward selections, P-value tests, regularization, gradient boosting, and principal components analysis.\n",
    "   + The features also need to be re-engineered for optimal learning. This is accomplished by restructuring the input data such that the gradient surface of the model's objective function is sufficiently convex and of a form sympathetic to gradient descent optimization. Such feature engineering tasks include transforming categorical features into a numerical space, transforming skewed features into a normalized space, or extracting relevant information \"hidden\" within text strings. \n",
    "\n",
    "<br></br>\n",
    "\n",
    "<img src=\"../images/Pipeline.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "**Figure 3: Visualization of key project steps to successfully build a machine learning model application.**\n",
    "\n",
    "## Challenges\n",
    "\n",
    "Challenges encountered during this phase include the following: \n",
    "\n",
    "1. Developing a method to consistently and appropriately aggregate and summarize normalized data into a denormalized input variable. The challenge here was to do this in such a way as to not artificially dilute the data, to avoid choosing \"winners\" and \"losers\" based on arbitrary decisions, and to be systematic and reasoned throughout the process. \n",
    "2. Running the model was challenging given the full raw dataset is approximately 2.7 gb of data. The process of denormalizing this dataset results in a high-dimensional dataset which can be cumbersome for complex models to work on. Local machines were having difficulty running the pipeline on the unrefined denormalized dataset.\n",
    "\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Description <a name='datadesc'></a>\n",
    "\n",
    "Although the Data Description is also discussed in the Phase 1 Proposal Report, a review of the underlying data structure will aid understanding the process by which the data is denormalized and prepared for the ML pipeline. \n",
    "\n",
    "The data provided in the loan applications give insights into an applicant's current demographic and social status. Data from past credit accounts also give insight into an applicant's past financial behaviors. Notably, the primary key for each sample (loan application) is the Loan ID `SK_ID_CURR`. This relates the current loan application to associated loans from Home Credit's past account records, and to associated loans from other institution's that were reported to the Credit Bureaus. The data are stored in 7 different table schemas shown on Figure 2 below. The specific attributes in each table, along with their descriptions, are stored in an addendum table named `HomeCredit_columns_description.csv`. \n",
    "\n",
    "<br></br>\n",
    "\n",
    "<img src=\"../images/home_credit.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "**Figure 2: Entity Relation Diagram for Home Credit Applicant Datatables**\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Home Credit provides three main sources of data upon which to learn the ML model: applicant information, credit bureau information, and Home Credit's account records. The sections below describe the data from these various sources, and some of the key variables.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "## Background on the dataset\n",
    "> Home Credit is a non-banking financial institution, founded in 1997 in the Czech Republic.\n",
    ">\n",
    "> The company operates in 14 countries (including United States, Russia, Kazahstan, Belarus, China, India) and focuses on lending primarily to people with little or no credit history which will either not obtain loans or became victims of untrustworthly lenders.\n",
    ">\n",
    "> Home Credit group has over 29 million customers, total assests of 21 billions Euro, over 160 millions loans, with the majority in Asia and and almost half of them in China (as of 19-05-2018).\n",
    ">\n",
    "> While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n",
    ">\n",
    "> *-- From original Jupyter Notebook*\n",
    "\n",
    "## Data files overview\n",
    "> There are 7 different sources of data:\n",
    "\n",
    "> * __application_train/application_test:__ the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating __0: the loan was repaid__ or __1: the loan was not repaid__. The target variable defines if the client had payment difficulties meaning he/she had late payment more than X days on at least one of the first Y installments of the loan. Such case is marked as 1 while other all other cases as 0.\n",
    "> * __bureau:__ data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n",
    "> * __bureau_balance:__ monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n",
    "> * __previous_application:__ previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n",
    "> * __POS_CASH_BALANCE:__ monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n",
    "> * __credit_card_balance:__ monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n",
    "> * __installments_payment:__ payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.\n",
    ">\n",
    "> *-- From original Jupyter Notebook*\n",
    "\n",
    "## Loan Application Information\n",
    "\n",
    "This is a denormalized pair of tables `application_test|train.csv` with attribute information about the current loan. These attributes describe both the financial instrument itself (the contract type, rate, size of loan, annuity, etc.) and also the demographic information for the applicant (age, gender, income, education, etc.). In addition to these basic demographic data on the applicant, a suite of other very detailed information about the applicant are provided including details on the building/property in which they live, differences in work and home addresses, car ownership, if the applicant's friends recently defaulted on any loans, which documents were turned in, and many more. This table also contains the target variable `TARGET`. This is a binary variable indicating whether or not the client had payment difficulties on the loan.\n",
    "\n",
    "These data are explored and a baseline model is created in the supplementary Baseline Jupyter Notebook in [Section 10](#ipynb). \n",
    "\n",
    "## Credit Bureau Information\n",
    "\n",
    "Some applicants have had loans through other financial institutions which were reported to the Credit Bureaus. Data from these loan applications are available from `bureau.csv` and are related to the current loan application IDs. A monthly time series describing the payment status through loan term is available for each of these related outside institution loans in `bureau_balance.csv`, related to `bureau.csv` by `SK_ID_BUREAU`. The `STATUS` attribute in this table categorically indicates whether a loan has past-due payments on a given statement. These data can be used to evaluate an applicant's past payment behaviors.\n",
    "\n",
    "## Home Credit Previous Records\n",
    "\n",
    "Some applicants have had loans or other financial products through Home Credit in the past. These related financial product applications are made available under `previous_application.csv`. The `SK_ID_PREV` relates these applications to payment and balance information depending on the type of financial product (credit card balances `credit_card_balance.csv`, installment payments `installments_payments.csv`, and loan balances `POS_CASH_balance.csv`). These data can be used to evaluate an applicant's past payment behaviors.\n",
    "\n",
    "The credit card balances table shows detailed statement accounts of an applicants credit card withdrawals, credit limit, and days past due. Similarly, the installment payments and loan balances tables detail the amount prescribed and paid for various loan installments as well as the days past due for any late payments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Environment Setup\n",
    "\n",
    "This section is composed of code that sets up the data for exploration, denormaliation, and integration with an ML pipeline. It assumes the `*.csv` files from the Kaggle dataset are stored in a folder at the following directory relative to the path of this notebook:\n",
    "\n",
    "```bash\n",
    "DATA_DIR = \"../Data/\"  \n",
    "```\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pandas.plotting import scatter_matrix\n",
    "import missingno as msno\n",
    "#!pip install msno\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Creating dictionary for datasets so that we can keep track of datsets and also make them callable to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "DATA_DIR =  \"/../Data/\"\n",
    "\n",
    "ds_names = (\"application_train\", \"application_test\", \"bureau\",\"bureau_balance\",\n",
    "            \"credit_card_balance\", \"installments_payments\",\"previous_application\",\n",
    "            \"POS_CASH_balance\")\n",
    "\n",
    "datasets = {}\n",
    "datasets_transformed = {}\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    datasets[ds_name] = pd.read_csv(os.getcwd() + DATA_DIR + f'{ds_name}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary, Size, and Overview\n",
    "\n",
    "As part of the data download comes a *Data Dictionary* named `HomeCredit_columns_description.csv` and it is summarized in [Section 11](#datadict) as an attachment to this report. The subsections below give brief table descriptions, variable name overviews, and head views for the attributes in the normalized data tables. Additionally included are information on the size (memory usage) of each table. All together summed the dataset is approximately 2.7 gigabytes large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application train\n",
    "\n",
    "* __application_train/application_test:__ the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating __0: the loan was repaid__ or __1: the loan was not repaid__. The target variable defines if the client had payment difficulties meaning he/she had late payment more than X days on at least one of the first Y installments of the loan. Such case is marked as 1 while other all other cases as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df,name):\n",
    "    #df = pd.read_csv(in_path)\n",
    "    print(f\"{name}: shape is {df.shape}\")\n",
    "    print(df.info())\n",
    "    display(df.head(5))\n",
    "    return df\n",
    "\n",
    "ds_name = 'application_train'\n",
    "datasets[ds_name]= load_data(datasets[ds_name],ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'application_test'\n",
    "datasets[ds_name]= load_data(datasets[ds_name],ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application dataset has the most information about the client: Gender, Income, Family Status, Education, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Other Datasets\n",
    "\n",
    "* __bureau:__ data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau but one loan in the application data can have multiple previous credits.\n",
    "* __bureau_balance:__ monthly data about previous credits in bureau. Each row is one month of a previous credit and a single previous credit can have multiple rows, one for each month of the credit length.\n",
    "* __previous_application:__ previous applications for loans at Home Credit of clients who have loans in the application train data. Each current loan in the application train data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n",
    "* __POS_CASH_BALANCE:__ monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan and a single previous loan can have many rows.\n",
    "* __Credit_card_balance:__ monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance and a single credit card can have many rows.\n",
    "* __installments_payment:__ payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_names = (\"application_train\", \"application_test\", \"bureau\",\"credit_card_balance\",\"installments_payments\",\n",
    "            \"previous_application\",\"POS_CASH_balance\")\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    datasets[ds_name] = load_data(datasets[ds_name], ds_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_name in datasets.keys():\n",
    "    print(f'dataset {ds_name:24}: [ {datasets[ds_name].shape[0]:10,}, {datasets[ds_name].shape[1]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis <a name='EDA'></a>\n",
    "\n",
    "Exploratory Data Analysis (*EDA*) on the input tables is necessary to understand the data and to make decisions on transforming the data into a format that is ready for the machine learning pipeline. Custom functions are designed and used for each normalized table to explore the data distribution and transform the table. The tables are transformed by aggregating the data into summary statistics for the table so the child tables can be joined to their parent tables, and ultimately rolled into the application_test_train data based on their ID keys (`SK_ID_CURR`, `SK_ID_PREV`, `SK_ID_BUREAU`). The subsections below outline the main functions designed to accomplish the EDA and data transformation, and also the individual steps applied to each table to accomplish this process.  \n",
    "\n",
    "## EDA Functions\n",
    "\n",
    "A number of custom functions were developed to explore and transform the data. Names of the functions and brief descriptions of their usefulness are summarized here:\n",
    "\n",
    "1. Summary Statistics\n",
    "   + This provides an overview of summary statistics on each variable of each table\n",
    "2. Feature Type Extraction\n",
    "   + This function describes numerical and categorical feature types and provides reference lists for other functions in the EDA process\n",
    "3. Missing Data Analysis\n",
    "   + Looks at the missing data rate for a table's columns\n",
    "4. Feature Aggregating\n",
    "   + A function to group the tables by their ID key and aggregate them into their summary statistics\n",
    "5. One-Hot-Encoding Extract-Transform-Load\n",
    "   + An ETL function that one-hot encodes categorical variables so they can be aggregated, and applies the transformation to the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics for All tables\n",
    "\n",
    "__Function Feature Summary:__\n",
    "\n",
    "A summary of each table is provided using the described function below. This function will take a dataframe as input and provide summary statistics for the table. This generic function will help us evaluate each table with just one line of code, improving the reusablity of the code. Summary statistics will show statistics on each dataframe and through that meaningful interpretations were made about the data to further plan EDA on each dataset.\n",
    "\n",
    "Function name: feature_summary \n",
    "\n",
    "Input: Dataframe\n",
    "\n",
    "Output: Dataframe Summary\n",
    "\n",
    "Function output:\n",
    "1. Provide table name for which analysis in progress.\n",
    "2. Shape - provides rows and columns in the data set.\n",
    "3. NULLS - Number of nulls in the column.\n",
    "4. %_NULL - NULL percentage. (nulls/attributes count).\n",
    "5. Unique values - No of unique values for the feature.\n",
    "6. Data Type - Id column numerical or categorical.\n",
    "7. Max/Min - Max and min for the column. This provides idea about scale. helps if need log transformation.\n",
    "8. Mean - Mean of the attribute.\n",
    "9. Std - Standard Deviation of the atttibute.\n",
    "10. Skewness - Skewness, data distribution of transformations.\n",
    "11. Sample values - Sample values for the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_summary(df_fa):\n",
    "    print('DataFrame shape')\n",
    "    print('Rows:',df_fa.shape[0])\n",
    "    print('Cols:',df_fa.shape[1])\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    \n",
    "    col_list=['Null','%_Null','Unique_Count','Data_type','Max/Min','Mean','Std','Skewness','Sample_values']\n",
    "    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n",
    "    df['Null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n",
    "    df['%_Null']=round((df_fa.isnull().sum()/df_fa.isnull().count()*100),3).sort_values(ascending = False)\n",
    "    df['Unique_Count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n",
    "    df['Data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n",
    "    for i,col in enumerate(df_fa.columns):\n",
    "        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n",
    "            df.at[col,'Max/Min']=str(round(df_fa[col].max(),2))+'/'+str(round(df_fa[col].min(),2))\n",
    "            df.at[col,'Mean']=df_fa[col].mean()\n",
    "            df.at[col,'Std']=df_fa[col].std()\n",
    "            df.at[col,'Skewness']=df_fa[col].skew()\n",
    "        df.at[col,'Sample_values']=list(df_fa[col].unique())\n",
    "    print(\"Table Statistics\")\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    display(df.fillna('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ds_name in enumerate(datasets.keys()):\n",
    "    print(\"Table under consideration:\",ds_name.upper())\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    ds = feature_summary(datasets[ds_name])\n",
    "    print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction based on Type\n",
    "\n",
    "\n",
    "__Function id_num_cat_feature:__\n",
    "\n",
    "This function will take a dataframe as input and return 4 lists that contain ID columns, numerical features, categorical features, and numerical features without the ID cols. \n",
    "\n",
    "Function name: id_num_cat_feature\n",
    "Input : Dataframe\n",
    "Output : 4 Lists\n",
    "\n",
    "Function output:\n",
    "1. ID columns\n",
    "2. Numerical features\n",
    "3. Categorical features\n",
    "4. Numerical features excluding the ID columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function gives us mutiple lists of the column types which are then used in other functions. The function returns clear separations on what columns are categorical and numerical. By having the distinctive categorical columns and numerical columns lists, different transformations are performed on them based on if they are categorical or numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_num_cat_feature(df,text = True):\n",
    "    numerical = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical = df.select_dtypes(include=['object', 'bool']).columns\n",
    "    feat_num = list(numerical)\n",
    "    feat_cat = list(categorical)\n",
    "    \n",
    "    id_cols = ['SK_ID_CURR','SK_ID_BUREAU']\n",
    "    \n",
    "    id_cols = [cols for cols in  list(df.columns.intersection(id_cols))] \n",
    "    features = list(set(df.columns) - set(id_cols))\n",
    "\n",
    "    if text == True:\n",
    "          # print eda\n",
    "        print('--------')\n",
    "        print(f\"# of ID's: {len(id_cols)}\")\n",
    "        print(f\" ID's:\")\n",
    "        print(id_cols)\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# All features: {len(features)}\")\n",
    "        print(f\"All features:\")\n",
    "        print(features)\n",
    "        print('')\n",
    "        print(f\"Missing data:\")\n",
    "        print(missing_data(df[features]))\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# of Numerical features: {len(feat_num)}\")\n",
    "        print(f\"Numerical features:\")\n",
    "        print(feat_num)\n",
    "        print('')\n",
    "        print(f\"Numerical Statistical Summary:\")\n",
    "        print('')\n",
    "        print(df[feat_num].describe())\n",
    "        print('')\n",
    "        print('--------')\n",
    "        print(f\"# of Categorical features: {len(feat_cat)}\")\n",
    "        print(f\"Categorical features:\")\n",
    "        print(feat_cat)\n",
    "        print('')\n",
    "        print(f\"Categorical Statistical Summary:\")\n",
    "        print('')\n",
    "        #print(df[feat_cat].describe(include='all'))\n",
    "        print('')\n",
    "        print(\"Categories:\")\n",
    "        print('')\n",
    "        print(df[feat_cat].apply(lambda col: col.unique()))\n",
    "        print('')\n",
    "        print('--------')\n",
    "    return id_cols,feat_num,feat_cat,features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Count and percentage\n",
    "\n",
    "\n",
    "__Function missing_data:__\n",
    "\n",
    "This function will take a dataframe as input and provide null count and % of nulls for a dataframe.\n",
    "\n",
    "Function name: missing_data\n",
    "\n",
    "Input: Dataframe\n",
    "\n",
    "Output: NULL count and NULL %\n",
    "\n",
    "Function output:\n",
    "1. NULL Count\n",
    "2. NULL Percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Aggregating\n",
    "\n",
    "__Class:__ FeatureSummarizer\n",
    "\n",
    "\n",
    "Class FeatureSummarizer has following aggregation parameters:\n",
    "1. \"min\"\n",
    "2. \"max\"\n",
    "3. \"count\"\n",
    "4. \"sum\"\n",
    "5. \"median\"\n",
    "6. \"mean\"\n",
    "7. \"var\"\n",
    "\n",
    "Based on the keys of the dataframe, transformation function will enable grouping of feature variables on ID and then aggregate the feature variables into their predefined statistical summaries for each grouping.\n",
    "\n",
    "__Function:__ runFeatureSummarizer\n",
    "\n",
    "This function will take dataframe and features as input and using the class above, get aggregated features. The output of the above function will be the transformed aggregated features for the dataframe passed into the function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSummarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "        self.agg_ops = [\"min\", \"max\", \"count\", \"sum\", \"median\", \"mean\", \"var\"]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        keys = list(set(X.columns) - set(self.features))\n",
    "        \n",
    "        result = X.groupby(keys, as_index=False).agg({ft:self.agg_ops for ft in self.features}).reset_index()\n",
    "        result.columns = result.columns.map(lambda ct: '_'.join([x for x in ct if x != '']))\n",
    "        result.reset_index()\n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFeatureSummarizer(df, features):\n",
    "    print(f\"df.shape: {df.shape}\\n\")\n",
    "    print(f\"Aggregated Features:\\ndf[{features}][0:5]: \\n{df[features][0:5]}\")\n",
    "    pipeline = make_pipeline(FeatureSummarizer(features))\n",
    "    return(pipeline.fit_transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoded Extract-Transform-Load\n",
    "\n",
    "Function eda_transformation:\n",
    "\n",
    "Function input: Dataframe, n. Where n is a parameter for feature selection. \n",
    "\n",
    "- This function calls id_num_cat_feature to put all features types id columns, numerical features, categorical features, and numerical features excluding id columns into 4 respectively lists.\n",
    "- Categorical variables are one hot encoded into some numerical value, to allow the pipeline to make interpretations from categorical varibles more easily.\n",
    "- run FeatureSummarizer function is called to get all aggregated features. \n",
    "- Final features are selected through feature selection from the transformed tables. \n",
    "- Output of this function will be a dataframe with all aggregated features selected, which will eventually be used in feature selection.\n",
    "- The function will also prints aggregated features and aggregated data. \n",
    "\n",
    "\n",
    "__Feature Selection:__\n",
    "\n",
    "    - Once we have completed OHE, we can remove some attributes which are redundant, for example:\n",
    "    - count of previous application is good to have, but mean,max is not required.\n",
    "    - Features with DAYS, count suffice the aggregated feature, we can remove all other min, max, etc. feature forthat column.\n",
    "    - Based on above logic, we removed certain columns from the supporting tables to remove the redundant attributes and save on execution time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_transformation(df,n):\n",
    "\n",
    "    id_cols, feat_num, feat_cat, features =  id_num_cat_feature(df)\n",
    "    \n",
    "    df = pd.get_dummies(data=df, columns=feat_cat)\n",
    "\n",
    "    features = list(set(df.columns) - set(id_cols))\n",
    "    feat_ohe = list(set(features) - set(feat_num))\n",
    "\n",
    "    print(f\"# of OHE categorical features: {len(feat_ohe)}\")\n",
    "    print(f\"OHE Categorical features: {feat_ohe}\")\n",
    "    print('--------')\n",
    "\n",
    "    df = runFeatureSummarizer(df, features)\n",
    "    \n",
    "    if n == 0:\n",
    "        # bureau_balance\n",
    "        feature_selection = [\n",
    "            df[id_cols],\n",
    "            df[[column for column in df.columns if column.startswith('MONTHS') and column.endswith('count')]],\n",
    "            df[[column for column in df.columns if column.startswith('STATUS') and column.endswith(('mean', 'median', 'var'))]]\n",
    "        ]\n",
    "    elif n == 1:\n",
    "        # bureau\n",
    "        feature_selection = [\n",
    "            df[[column for column in df.columns if not column.startswith(tuple(feat_cat)) and not column.endswith('count')]],\n",
    "            df[[column for column in df.columns if column.startswith('DAYS_CREDIT') and column.endswith('count')]],\n",
    "            df[[column for column in df.columns if column.startswith(tuple(feat_cat)) and column.endswith(('mean', 'median', 'var'))]]\n",
    "        ]\n",
    "    elif n ==3:\n",
    "        \n",
    "        feature_selection = [\n",
    "            df[[column for column in df.columns if not column.startswith('SK_ID_PREV') and column.startswith(tuple(feat_num))]],\n",
    "            df[[column for column in df.columns if column.startswith('DAYS') and column.endswith('count')]],\n",
    "            df[[column for column in df.columns if column.startswith('SK_ID_PREV') and column.endswith('count')]],\n",
    "            df[[column for column in df.columns if column.startswith(tuple(feat_cat)) and column.endswith(('mean', 'median', 'var'))]]\n",
    "        ]\n",
    "     \n",
    "    elif n ==4:\n",
    "        \n",
    "        feature_selection = [\n",
    "            df[[column for column in df.columns if not column.startswith('SK_ID_PREV') and column.startswith(tuple(feat_num))]],\n",
    "            df[[column for column in df.columns if column.startswith('SK_ID_PREV') and column.endswith('count')]],\n",
    "            df[[column for column in df.columns if column.startswith(tuple(feat_cat)) and column.endswith(('mean', 'median', 'var'))]]\n",
    "        ]\n",
    "    else: \n",
    "        print('ERROR: Invalid feature method. ')\n",
    "\n",
    "    df = pd.concat(feature_selection, axis=1)\n",
    "\n",
    "    features = list(set(df.columns) - set(id_cols))\n",
    "\n",
    "    print('--------')\n",
    "    print('Aggregated Features:')\n",
    "    print('\\n'.join(map(str, sorted(features))))\n",
    "    print('')\n",
    "    print('Aggregated Data:')\n",
    "    print('')\n",
    "    print(df[features].describe().T)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on Tables\n",
    "\n",
    "Since each table is structured slightly differently and they are all composed of separate variables, It is necessary to perform different steps on each of the tables as necessary. While all the tables can be explored and transformed by the `eda_transformation()` function, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bureau Balance\n",
    "\n",
    "1. This function is specifically for Bureau Balance table. Below are the intial pre-processing steps done before passing this table into the pipeline.\n",
    "    - Since this table has only 2 features, no features are dropped or created for this table. \n",
    "    - Take absolute of the months balance attribute. which was provided as negative values, as it is relative to application date.\n",
    "    - Any column or row with more than 70% of its data as null will be deleted from the dataframe, as the threshold is set to .7.\n",
    "    - Once processed, store the transformed data into a csv file. Benefit of this is that data can be passed ed csv file. directly to model for merging into application train/test table. We do not have to repeatedly perform expensive EDA/ETL/Transformation.\n",
    "    \n",
    "    - Note that this table does not contain SK_ID_CURR. It will be rolled up to Bureau table which contains the SK_ID_CURR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbal_eda(df):\n",
    "    print(\"bureau_bal :: EDA and transformation\")\n",
    "    print('')\n",
    "    bbal = df\n",
    "    #bureau balance table contains all the data so no need to drop any column or row.\n",
    "    #Adding new features, take the abs for the monthly balance attribute.\n",
    "    bbal['MONTHS_BALANCE'] = bbal['MONTHS_BALANCE'].abs()\n",
    "    return (eda_transformation(bbal,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbal = datasets['bureau_balance']\n",
    "bbal = bbal_eda(bbal)\n",
    "datasets_transformed['bureau_balance'] = bbal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bureau\n",
    "\n",
    "1. This function is specifically for Bureau table. Below are the intial pre-processing steps done before passing this table into the pipeline.\n",
    "\n",
    "    - For columns with DAYS in name, there are two with negative values, we will take the absolute for those. \n",
    "    \n",
    "    - Data from Buereau balance table is rolled up in Bureau table befor any EDA. This will enable using all the featues from buereau balance table as well before rolling up all data to main table, i.e. application train. \n",
    "    \n",
    "    - while doing left join, we updated OHE column names to more readale forms by removing any space or spcl charater to \"\\_\" which is widely used in column names. \n",
    "    \n",
    "    - Any column or row with more than 70% of its data as null will be deleted from the dataframe, as the threshold is set to .7.\n",
    "    \n",
    "    - Once processed, store the transformed data into a csv file. Benefit of this is that data can be passed ed csv file. directly to model for merging into application train/test table. We do not have to repeatedly perform expensive EDA/ETL/Transformation.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_eda(df):\n",
    "    bureau = df\n",
    "    drop_list_bureau = []\n",
    "    \n",
    "    #Adding new features\n",
    "    #bureau['MONTHS_BALANCE'] = bureau['MONTHS_BALANCE'].abs()\n",
    "    for c in [co for co in bureau.columns if 'DAYS' in co]:\n",
    "        bureau[c] = bureau[c].replace({365243.0: np.nan})\n",
    "        bureau[c] = bureau[c].abs()\n",
    "    # Drop elements in drop list\n",
    "    threshold = 0.7\n",
    "\n",
    "    #Dropping rows with missing value rate higher than threshold\n",
    "    bureau = bureau.loc[bureau.isnull().mean(axis=1) < threshold]\n",
    "    \n",
    "    return (eda_transformation(bureau,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = datasets['bureau']\n",
    "\n",
    "# rollup bureau_bal\n",
    "# gets rid of the unwanted characters in categorical columns entries - makes for nicer OHE column names later...\n",
    "bureau = bureau.merge(bbal, on='SK_ID_BUREAU', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) \\\n",
    "               .drop('SK_ID_BUREAU', axis=1)\n",
    "bureau = bureau_eda(bureau)\n",
    "datasets_transformed['bureau'] = bureau\n",
    "#bureau.to_csv(os.getcwd() + DATA_DIR + 'bureau_agg_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collinearity Analysis\n",
    "\n",
    "The collinearity of variable pairs are compared via correlation and can be iteratively dropped from combined dataset based on which variables are least correlated to the target variable. Below is an example of the collinearity analysis. This step will be added to the pre-processing pipeline as a method of feature selection in future phases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colinearityReducer(dataframe, threshold=0.5):\n",
    "\n",
    "    '''\n",
    "    This function explores the correlation between each variable pair and the Target.\n",
    "    Of the var1iable pairs with absolute correlations above the threshold value...\n",
    "    ...the variable with the lowest target variable correlation is dropped from the input dataframe.\n",
    "    The process is repeated until there are no more colinear pairs with absolute correlations above the threshold.\n",
    "    \n",
    "    NOTE! The function receives a dataframe structured with the target variable in first column.\n",
    "    '''\n",
    "    \n",
    "    print('------------------------------------')\n",
    "    print('BEGIN COLINEAR FEATURE REDUCTION')\n",
    "    print('------------------------------------')\n",
    "    \n",
    "    i = 1\n",
    "    dropped_variables = list()\n",
    "    while True:\n",
    "    \n",
    "        # read-in and assign columns\n",
    "        # gets correlation matrix between variables and pivots to a longer df\n",
    "        # identify target variable\n",
    "        # drop same-name and target correlations \n",
    "        \n",
    "        print('------------------------------------')\n",
    "        print(f\"Colinearity Reduction Iteration {i}\\n\")\n",
    "        \n",
    "        df = dataframe\n",
    "        features = df.iloc[:,1:].columns\n",
    "        target_name = df.iloc[:,0].name\n",
    "        \n",
    "        print('')\n",
    "        print(f'Dataframe Features ({len(features)}):')\n",
    "        print(features)\n",
    "        \n",
    "        df = pd.melt(abs(df.corr()).reset_index(), id_vars='index', value_vars=features)\n",
    "        targets = df[df['index']==target_name]\n",
    "        df = df[(df['index'] != df['variable']) & (df['index'] != target_name) & (df['variable'] != target_name)]\n",
    "\n",
    "        # combine the correlated variables into ordered string\n",
    "        # aggregate the max correlation and sort pairs\n",
    "        # split out the variables from the original string\n",
    "        # join the target variable correlations for each variable pair, rename columns\n",
    "\n",
    "        df['joined'] = df[['index', 'variable']].apply(lambda row: '::'.join(np.sort(row.values.astype(str))), axis=1)\n",
    "\n",
    "        df = df.groupby('joined', as_index=False) \\\n",
    "               .agg({'value':'max'}) \\\n",
    "               .sort_values(by='value', ascending=False)\n",
    "\n",
    "        df[['var_1','var_2']] = df['joined'].str.split(\"::\",expand=True)\n",
    "\n",
    "        df = df.merge(targets, how='left', left_on='var_1', right_on='variable') \\\n",
    "               .merge(targets, how='left', left_on='var_2', right_on='variable')\n",
    "        df.rename(columns = {'value_x':'var_pair_corr', 'value_y':'var_1_target_corr', 'value':'var_2_target_corr'}, inplace = True)\n",
    "\n",
    "        # This section takes all variable pairs with a correlation greater than threshold\n",
    "        # test to determine which has a higher correlation with the target.\n",
    "        # The higher of the two gets marked as a win\n",
    "        # While the other gets marked as a loss\n",
    "        # the wins and losses for each variable are then grouped and summed\n",
    "\n",
    "        exceeds = df[df['var_pair_corr']>threshold]\n",
    "\n",
    "        # break if none above threshold\n",
    "        if len(exceeds['var_pair_corr'])==0:\n",
    "            print('------------------------------------')\n",
    "            print(f\"NO VARIABLE PAIRS WITH CORRELATION > {threshold}\")\n",
    "            break\n",
    "\n",
    "        exceeds['var_1_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] >= row[\"var_2_target_corr\"] else 0, axis=1)\n",
    "        exceeds['var_1_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] >= row[\"var_1_target_corr\"] else 0, axis=1)\n",
    "        exceeds['var_2_win'] = exceeds.apply(lambda row: 1 if row[\"var_1_target_corr\"] < row[\"var_2_target_corr\"] else 0, axis=1)\n",
    "        exceeds['var_2_loss'] = exceeds.apply(lambda row: 1 if row[\"var_2_target_corr\"] < row[\"var_1_target_corr\"] else 0, axis=1)\n",
    "\n",
    "        var1 = exceeds[['var_1', 'var_1_win', 'var_1_loss']].groupby('var_1', as_index=False) \\\n",
    "                                                            .agg({'var_1_win':'sum', 'var_1_loss':'sum'})\n",
    "        var1.rename(columns = {'var_1':'var', 'var_1_win':'win', 'var_1_loss':'loss'}, inplace=True)\n",
    "\n",
    "        var2 = exceeds[['var_2', 'var_2_win', 'var_2_loss']].groupby('var_2', as_index=False) \\\n",
    "                                                            .agg({'var_2_win':'sum', 'var_2_loss':'sum'})\n",
    "        var2.rename(columns = {'var_2':'var', 'var_2_win':'win', 'var_2_loss':'loss'}, inplace=True)\n",
    "\n",
    "        corrcomps = pd.concat([var1,var2], axis=0).groupby('var', as_index=False) \\\n",
    "                                                  .agg({'win':'sum', 'loss':'sum'})\n",
    "\n",
    "        # drop variables which had 0 wins - IE collinear variables which were always least related to the target\n",
    "        dropvars = corrcomps[corrcomps['win']==0]['var']\n",
    "        \n",
    "        dropped_variables.extend(list(dropvars))\n",
    "        \n",
    "        dropvarsummary = targets[targets['variable'].isin(dropvars)].iloc[:,1:]\n",
    "        dropvarsummary.rename(columns={'variable':'Dropped Variable', 'value':'Target Variable Correlation'}, inplace = True)\n",
    "        \n",
    "        print('')\n",
    "        print('Dropped Variables:')\n",
    "        print(dropvarsummary)\n",
    "        # print('------------------------------------')\n",
    "        # print('Exceedances:')\n",
    "        # print(exceeds)\n",
    "        \n",
    "        dataframe = dataframe.drop(dropvars, axis=1)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    print('------------------------------------')\n",
    "    print('Final Dropped Variable List:')\n",
    "    print(dropped_variables)\n",
    "    print('------------------------------------')\n",
    "    print('END COLINEAR FEATURE REDUCTION')\n",
    "    print('------------------------------------')\n",
    "    \n",
    "    return dataframe\n",
    "    \n",
    "    \n",
    "# testing\n",
    "df = pd.concat([y_train,X_train[X_feat_num]],axis=1)\n",
    "\n",
    "colinearityReducer(df, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS - POS_CASH_balance\n",
    "\n",
    "1. This function is specifically for POS table. Below are the intial pre-processing steps done before passing this table into the pipeline.\n",
    "    - Create a drop list. \n",
    "        - Attributes that will be dropped are added to this list and all columns will be deleted before passing the dataframe into the eda function.\n",
    "    - Create new features based on analysis. 3 new features were created:\n",
    "        - Percentage of installments pending.\n",
    "        - Number of installments pending.\n",
    "        - Days with Tolerance.\n",
    "    - Take absolute of the months balance attribute. which was provided as negative values, as it is relative to application date.\n",
    "    - Replace \" \" with \"_\" for OHE columns.\n",
    "    - Any column or row with more than 70% of its data as null will be deleted from the dataframe, as the threshold is set to .7.\n",
    "    - Once processed, store the transformBenefit of this is that the data can be passed ed csv file. directly to model for merging into application train/test table. We do not have to repeatedly perform expensive EDA/ETL/Transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_eda(df):\n",
    "    pos = df\n",
    "    drop_list_pos = []\n",
    "    \n",
    "    #Adding new features\n",
    "    pos['POS_PERC_INSTL_PNDNG']=pos['CNT_INSTALMENT_FUTURE']/pos['CNT_INSTALMENT']\n",
    "    pos['POS_CNT_INSTAL_PNDNG']=pos['CNT_INSTALMENT']-pos['CNT_INSTALMENT_FUTURE']\n",
    "    pos['POS_DAYS_WTHT_TOLRNC']=pos['SK_DPD']-pos['SK_DPD_DEF']\n",
    "    pos['MONTHS_BALANCE'] = pos['MONTHS_BALANCE'].abs()\n",
    "    \n",
    "    #replacing \" \" with _ for OHE cols.\n",
    "    pos['NAME_CONTRACT_STATUS']=pos['NAME_CONTRACT_STATUS'].apply(lambda x: str(x).replace(\" \",\"_\")) \n",
    "    \n",
    "    # Drop elements in drop list\n",
    "    threshold = 0.7\n",
    "\n",
    "    #Dropping rows with missing value rate higher than threshold\n",
    "    pos = pos.loc[pos.isnull().mean(axis=1) < threshold]\n",
    "    \n",
    "    return (eda_transformation(pos,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = datasets['POS_CASH_balance']\n",
    "pos = pos_eda(pos)\n",
    "datasets_transformed['POS_CASH_balance'] = pos\n",
    "#pos.to_csv(os.getcwd() + DATA_DIR + 'pos_agg_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREVAPP - Previous Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This function is specifically for POS table. Below are the initial pre-processing steps done before passing this table into the pipeline.\n",
    "\n",
    "    - Create a drop list. \n",
    "        - Attributes that will be dropped will be added to this list and all columns will be deleted before passing the dataframe into the eda function.\n",
    "    - Create new features. based on analysis, 6 new features were created:\n",
    "        - Count of approved previous application. \n",
    "        - Count of Rejected previous applications.  \n",
    "        - Difference: Amount requested in application - Actual credit amount.\n",
    "        - Ratio - Ratio of application amount to amount credited. \n",
    "        - Ratio - Ratio of amount credited to amount annuity\n",
    "        - Ratio - Ratio of down payment to amount credited. \n",
    "\n",
    "    - There are number of attributes which are in days and amount. For that, we created list of columns which ends with \n",
    "    'DAYS' and 'AMT'\n",
    "\n",
    "    - Analysis on attributes with date shows that many are capped to 365243, which is 100 years. This looks to be added by the system and not user data. This will be replaced by nan and later imputed.\n",
    "    - Another observation was that days columns are marked as negative and so the absolute values were calcuated and used.\n",
    "\n",
    "    - Added below attributes to droplist:\n",
    "        - WEEKDAY_APPR_PROCESS_START\n",
    "        - HOUR_APPR_PROCESS_START\n",
    "\n",
    "    - Any column or row with more than 70% of its data as null will be deleted from the dataframe as the threshold is set to .7.\n",
    "\n",
    "    - Once processed, store the transformed csv file. Benefit of this is that we can then pass it directly to model for merging into application train/test table. We do not have to perform expensive EDA/ETL/Transformation everytime we want to process the same data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevapp_eda(df):\n",
    "    prevapp = df\n",
    "    drop_list_pa = []\n",
    "    \n",
    "    #Day and Amount columns\n",
    "    day_cols = [col for col in prevapp.columns if 'DAY' in col]\n",
    "\n",
    "    amt_cols = [col for col in prevapp.columns if 'AMT' in col]\n",
    "    \n",
    "    #Adding new features\n",
    "    prevapp['PREV_APRV_CNT'] = prevapp['NAME_CONTRACT_STATUS'].map(lambda x: 1 if (x == 'Approved') else 0)\n",
    "    prevapp['PREV_REJ_CNT'] = prevapp['NAME_CONTRACT_STATUS'].map(lambda x: 1 if (x == 'Rejected') else 0)\n",
    "    prevapp['PREV_APCTN_CRDT_DIFF'] = prevapp['AMT_APPLICATION'] - prevapp['AMT_CREDIT']\n",
    "    prevapp['PREV_APCTN_CRDT_RATIO'] = prevapp['AMT_APPLICATION'] / prevapp['AMT_CREDIT']\n",
    "    prevapp['PREV_CRDT_ANNUTY_RATIO'] = prevapp['AMT_CREDIT']/prevapp['AMT_ANNUITY']\n",
    "    prevapp['PREV_DWN_PYMNT_CRDT_RATIO'] = prevapp['AMT_DOWN_PAYMENT'] / prevapp['AMT_CREDIT']\n",
    "    \n",
    "    for c in [co for co in prevapp.columns if 'DAYS' in co]:\n",
    "        prevapp[c] = prevapp[c].replace({365243.0: np.nan})\n",
    "        prevapp[c] = prevapp[c].abs()\n",
    "    \n",
    "    drop_list_pa.append('WEEKDAY_APPR_PROCESS_START') ## weekday data is normally distributed, so wont make any sense to add that\n",
    "    drop_list_pa.append('HOUR_APPR_PROCESS_START') ## Hour application started.\n",
    "    \n",
    "    # Drop elements in the drop list\n",
    "    drop_list_pa.append('WEEKDAY_APPR_PROCESS_START') ## weekday data is normally distributed, so wont make any sense to add that\n",
    "    drop_list_pa.append('HOUR_APPR_PROCESS_START') ## Hour application started.\n",
    "\n",
    "    threshold = 0.7\n",
    "    drop_list_pa = list(prevapp.columns[prevapp.isnull().mean() > threshold])\n",
    "\n",
    "    prevapp = prevapp.drop(columns=drop_list_pa, axis=1)\n",
    "\n",
    "    #Dropping columns with missing value rate higher than threshold\n",
    "    prevapp = prevapp[prevapp.columns[prevapp.isnull().mean() < threshold]]\n",
    "\n",
    "    #Dropping rows with missing value rate higher than threshold\n",
    "    prevapp = prevapp.loc[prevapp.isnull().mean(axis=1) < threshold]\n",
    "    \n",
    "    prevapp= eda_transformation(prevapp,3)\n",
    "    return prevapp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevapp = datasets['previous_application']\n",
    "prevapp = prevapp_eda(prevapp)\n",
    "datasets_transformed['previous_application'] = prevapp\n",
    "\n",
    "#prevapp.to_csv(os.getcwd() + DATA_DIR + 'prevapp_agg_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### CCB - Credit card Balance EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformation Visualizations on Credit Card Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the visualizations below where ever the skew become more centrally distributed compared to norm was log transformed. Where there were a substantial amount of 0's it was left as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccb = datasets['credit_card_balance']\n",
    "id_cols,num_cols,_,_ = id_num_cat_feature(ccb, text = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_comparision(df):\n",
    "    id_cols,num_cols,_,_ = id_num_cat_feature(ccb, text = False)\n",
    "    cols= list(set(num_cols) - set(id_cols))\n",
    "    ccb.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    for i in cols:\n",
    "        print(i)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.xlabel(\"normal\")\n",
    "        plt.hist(ccb[i], bins=20)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.xlabel(\"log_transformed\")\n",
    "        plt.hist(np.log(ccb[i].abs() + 1), bins=20)\n",
    "        #fig.tight_layout(pad=5.0)\n",
    "        plt.show()\n",
    "log_comparision(ccb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_trans_to_log_ccb = [\"CNT_INSTALMENT_MATURE_CUM\", \"AMT_CREDIT_LIMIT_ACTUAL\", \"AMT_PAYMENT_CURRENT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA and Feature Engineering Function on Credit Card Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This function is specifically for Credit Card Balance table. Below are the pre-processing done before passing this table into the pipeline.\n",
    "\n",
    "    - Absolute value of the Months Balance is used as that allows only use of postive values without effecting the correlation, as it is realtive to the application date.\n",
    "\n",
    "    - Any column or row with more than 70% of its data as null will be deleted from the dataframe as the threshold is set to .7.\n",
    "\n",
    "    - Similar to above, once processed, store the transformed csv file. Benefit of this is that we can then pass it directly to model for merging into application train/test table. We do not have to perform expensive EDA/ETL/Transformation everytime we want to process the same data.\n",
    "    \n",
    "2. Specific Feature Transformations\n",
    "    - The NAME_CONTRACT_STATUS contained the below values with the count and ratio.\n",
    "                         \n",
    "                         COUNT  RATIO\n",
    "                         \n",
    "          Active         3698436   0.96\n",
    "        \n",
    "          Completed       128918   0.03\n",
    "        \n",
    "          Signed           11058   0.00\n",
    "        \n",
    "          Demand            1365   0.00\n",
    "        \n",
    "          Sent proposal      513   0.00\n",
    "        \n",
    "          Refused             17   0.00\n",
    "         \n",
    "          Approved             5   0.00\n",
    "             \n",
    "         - All values with Active and Complete were made into one category. If complete that is a positive finding or if acitve  that is also a postive finding as that means it payments are being made on it or the status is healthy enough to stay active for the credit.                 \n",
    "         <br>     \n",
    "         <br>     \n",
    "     - The other created column is of a count of the number of credit lines the person has open. This was done by aggreated the SK_ID_CURR. This also can give us some glimpse into number of credit lines open vs the target. The higher the credit lines should demostrate a postive finding as the newer ones may have been denied as they would have a bad credit score and possibly not get approved for another line.\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccb_eda(df, col_to_drop, cols_trans_to_log_ccb):\n",
    "    #dropping a highly correlated columns (.95 or .99)\n",
    "    ccb = df.drop(columns_to_drop, axis = 1)\n",
    "    \n",
    "    #log tranformations \n",
    "    for column in cols_trans_to_log_ccb:\n",
    "        name = \"AT_L_O_\" + column \n",
    "        df[name] = np.log(df[column].abs() + 1)\n",
    "        \n",
    "    ccb[\"NAME_CONTRACT_STATUS\"] = np.where((ccb[\"NAME_CONTRACT_STATUS\"].isin([\n",
    "       \"Active\", \"Completed\"\n",
    "    ])), \"GoodStatus\", ccb[\"NAME_CONTRACT_STATUS\"])\n",
    "    \n",
    "    #Adding new features\n",
    "    ccb['MONTHS_BALANCE'] = ccb['MONTHS_BALANCE'].abs()\n",
    "    \n",
    "    #replacing \" \" with _ for OHE cols.\n",
    "    ccb['NAME_CONTRACT_STATUS']=ccb['NAME_CONTRACT_STATUS'].apply(lambda x: str(x).replace(\" \",\"_\")) \n",
    "    \n",
    "    threshold = 0.7\n",
    "\n",
    "    #Dropping columns with missing value rate higher than threshold\n",
    "    ccb = ccb[ccb.columns[ccb.isnull().mean() < threshold]]\n",
    "\n",
    "    #Dropping rows with missing value rate higher than threshold\n",
    "    ccb = ccb.loc[ccb.isnull().mean(axis=1) < threshold]\n",
    "    agg = eda_transformation(ccb,4)\n",
    "    agg['CC_COUNT'] = ccb.groupby('SK_ID_CURR').size()\n",
    "        \n",
    "    return (agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest correlations on Credit Card Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is calcuated with the highest correlated columns being dropped. The threshold used was when columns had a correlation of .95 or higher, only one of the columns was kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_correlation(datasets['credit_card_balance'], remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below are .99 correlations \n",
    "columns_to_drop = [\"AMT_RECIVABLE\", \"AMT_TOTAL_RECEIVABLE\", \"AMT_RECEIVABLE_PRINCIPAL\", \"AMT_PAYMENT_TOTAL_CURRENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below are .95+ correlations \n",
    "columns_to_drop = [\"AMT_RECIVABLE\", \"AMT_TOTAL_RECEIVABLE\", \"AMT_RECEIVABLE_PRINCIPAL\", \"AMT_PAYMENT_TOTAL_CURRENT\", \"CNT_DRAWINGS_POS_CURRENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccb = datasets['credit_card_balance']\n",
    "ccb = ccb_eda(ccb, columns_to_drop, cols_trans_to_log_ccb)\n",
    "datasets_transformed['credit_card_balance'] = ccb\n",
    "\n",
    "ccb.to_csv(os.getcwd() + DATA_DIR + 'ccb_agg_data_tr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installment Payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This function is specifically for Installment Payments table. Below are the pre-processing done before passing this table into the pipeline.\n",
    "\n",
    "    - Absolute value of the Days Installment and Days Entry Payment are used as that allows only use of postive values without effecting the correlation. As more negative means longer time from the intial date. \n",
    "\n",
    "    - A new column of IP_DIFF_PAYMNT_INSTLMNT was also calculated from difference of AMT_INSTALMENT from AMT_PAYMENT as a part of the feature select/transformation process.\n",
    "\n",
    "    - Any column or row with more than 70% of its data as null will be deleted from the dataframe as the threshold is set to .7.\n",
    "\n",
    "    - Similar to above, once processed, store the transfored csv file. Benefit of this is that we can then pass it directly to model for merging into application train/test table. We do not have to perform expensive EDA/ETL/Transformation everytime we want to process the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_eda(df):\n",
    "    ip = df\n",
    "    drop_list_ip = []\n",
    "    \n",
    "    #Adding new features\n",
    "    ip['DAYS_INSTALMENT'] = ip['DAYS_INSTALMENT'].abs()\n",
    "    ip['DAYS_ENTRY_PAYMENT'] = ip['DAYS_ENTRY_PAYMENT'].abs()\n",
    "    ip['IP_DIFF_PAYMNT_INSTLMNT'] = ip['AMT_PAYMENT'] - ip['AMT_INSTALMENT']\n",
    "    \n",
    "    threshold = 0.7\n",
    "\n",
    "    #Dropping columns with missing value rate higher than threshold\n",
    "    ip = ip[ip.columns[ip.isnull().mean() < threshold]]\n",
    "\n",
    "    #Dropping rows with missing value rate higher than threshold\n",
    "    ip = ip.loc[ip.isnull().mean(axis=1) < threshold]\n",
    "    \n",
    "    return (eda_transformation(ip,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = datasets['installments_payments']\n",
    "ip = ip_eda(ip)\n",
    "datasets_transformed['installments_payments'] = ip\n",
    "\n",
    "ip.to_csv(os.getcwd() + DATA_DIR + 'ip_agg_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Engineering on Application Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets[\"application_train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformations Visualizations on Application Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of columns within applicaiton train that were log transformed due to the scale of the data. DAYS_EMPLOYED has no central distribution and is distrubted either to one extreme or another. For the other columns they are contained within a range of values eg. 'AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are visualizations of the columns 'AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', and 'DAYS_ID_PUBLISH' prior to log transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,num_cols,_,_ = id_num_cat_feature(train, text = False)\n",
    "#datasets[ds_name].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "cols_trans_to_log = ['AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH'] \n",
    "\n",
    "num_plot(train, cols_trans_to_log, remove=['SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV'], figsize = (15,3))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are columns 'AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH' log transformed and you can see for that the scale is smaller and AMT_ANNUITY and DAYS_EMPLOYED have become more centralized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transformations\n",
    "train_log_visual = train[['AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH']].copy()\n",
    "for column in cols_trans_to_log:\n",
    "    name = \"AT_L_O_\" + column \n",
    "    train_visual[name] = np.log(train_visual[column].abs() + 1)\n",
    "    \n",
    "col_names_log = []\n",
    "for column in cols_trans_to_log:\n",
    "        name = \"AT_L_O_\" + column\n",
    "        col_names_log.append(name)\n",
    "        \n",
    "num_plot(train_visual, col_names_log, remove=['SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV'], figsize = (15,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Correlations on application train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_correlation(train, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are correlations in application train. The first is .99 and above while the second is .95 and above. The 99% correlations will be dropped as their correlation to each other is at 99%. With more time a better threshold could have been determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop .99\n",
    "train_cols_99 = [\"FLAG_EMP_PHONE\", \"APARTMENTS_MEDI\", \"BASEMENTAREA_MEDI\",\"FLOORSMIN_MEDI\", \"YEARS_BEGINEXPLUATATION_MEDI\", \"YEARS_BUILD_MEDI\", \"COMMONAREA_MEDI\", \"ELEVATORS_MEDI\", \"ENTRANCES_MEDI\", \"FLOORSMAX_MEDI\", \"LANDAREA_MEDI\", \"LIVINGAPARTMENTS_MEDI\", \"LIVINGAREA_MEDI\", \"NONLIVINGAPARTMENTS_MEDI\", \"NONLIVINGAREA_MEDI\",\"OBS_60_CNT_SOCIAL_CIRCLE\"]\n",
    "\n",
    "# columns to drop .95\n",
    "train_cols_95 = [\"FLAG_EMP_PHONE\", \"APARTMENTS_MEDI\", \"BASEMENTAREA_MEDI\",\"FLOORSMIN_MEDI\", \"YEARS_BEGINEXPLUATATION_MEDI\", \"YEARS_BUILD_MEDI\", \"COMMONAREA_MEDI\", \"ELEVATORS_MEDI\", \"ENTRANCES_MEDI\", \"FLOORSMAX_MEDI\", \"LANDAREA_MEDI\", \"LIVINGAPARTMENTS_MEDI\", \"LIVINGAREA_MEDI\", \"NONLIVINGAPARTMENTS_MEDI\", \"NONLIVINGAREA_MEDI\",\"OBS_60_CNT_SOCIAL_CIRCLE\",\"FLAG_EMP_PHONE\", \"REGION_RATING_CLIENT_W_CITY\", \"APARTMENTS_MODE\", \"BASEMENTAREA_MODE\", \"YEARS_BEGINEXPLUATATION_MODE\", \"YEARS_BUILD_MODE\", \"COMMONAREA_MODE\", \"ELEVATORS_MODE\", \"ENTRANCES_MODE\", \"FLOORSMAX_MODE\", \"FLOORSMIN_MODE\", \"LANDAREA_MODE\", \"LIVINGAPARTMENTS_MODE\", \"LIVINGAREA_MODE\", \"NONLIVINGAPARTMENTS_MODE\", \"NONLIVINGAREA_MODE\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Analysis on Application Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Variables & Target\n",
    "def cat_analyzer(dataframe, variable, target = None):\n",
    "    print(variable)\n",
    "    if target == None:\n",
    "        print(pd.DataFrame({\n",
    "            \"COUNT\": dataframe[variable].value_counts(),\n",
    "            \"RATIO\": dataframe[variable].value_counts() / len(dataframe)}), end=\"\\n\\n\\n\")\n",
    "    else:\n",
    "        temp = dataframe[dataframe[target].isnull() == False]\n",
    "        print(pd.DataFrame({\n",
    "            \"COUNT\":dataframe[variable].value_counts(),\n",
    "            \"RATIO\":dataframe[variable].value_counts() / len(dataframe),\n",
    "            \"TARGET_COUNT\":dataframe.groupby(variable)[target].count(),\n",
    "            \"TARGET_MEAN\":temp.groupby(variable)[target].mean(),\n",
    "            \"TARGET_MEDIAN\":temp.groupby(variable)[target].median(),\n",
    "            \"TARGET_STD\":temp.groupby(variable)[target].std()}), end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column analysis on WEEKDAY_APPR_PROCESS_START. All weekdays, Monday, Tuesday, Wednesday, Thrusday, and Friday will be converted to weekday and all weekand days Saturday and Sunday to weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_analyzer(train, \"WEEKDAY_APPR_PROCESS_START\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column analysis on NAME_FAMILY_STATUS. If the person is single or not married then will be single, if the person is married or in a civil marriage then multiple. If the person is a widow or separated then Broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_analyzer(train, \"NAME_FAMILY_STATUS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column analysis for NAME_EDUCATION_TYPE. If the person has had higher education or academic degree then complete, if secondary or incomplete higher then partial, otherwise lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_analyzer(train, \"NAME_EDUCATION_TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column analysis for OCCUPATION_TYPE. Based on the skill level of the occupation it was consider to be a part of one of four groups Lowest Skill, Low Skill, Medium Skill, and Highest Skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_analyzer(train, \"OCCUPATION_TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Function for Application Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other feature engineering was also done using NAME_EDUCATION_TYPE, EXT_SOURCE, DAYS_EMPLOYED, DAYS_BIRTH,AMT_INCOME_TOTAL,AMT_CREDIT, INCOME_PER_PERSON, AMT_INCOME_TOTAL, CNT_FAM_MEMBERS, ANNUITY_INCOME_PERC, AMT_ANNUITY, AMT_INCOME_TOTAL, AMT_ANNUITY, AMT_CREDIT, AMT_GOODS_PRICE, DAYS_BIRTH, and AMT_GOODS_PRICE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer_train(df, cols, cols_trans_to_log):\n",
    "    df = df.drop(cols, axis = 1)\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    # WEEKDAY_APPR_PROCESS_START column feature engineering \n",
    "    df[\"WEEKDAY_APPR_PROCESS_START\"] = np.where(df.WEEKDAY_APPR_PROCESS_START.isin(\n",
    "        [\"MONDAY\", \"TUESDAY\", \"WEDNESDAY\", \"THRUSDAY\", \"FRIDAY\"]), \"weekday\", df.ORGANIZATION_TYPE)\n",
    "    \n",
    "    df[\"WEEKDAY_APPR_PROCESS_START\"] = np.where(df.WEEKDAY_APPR_PROCESS_START.isin(\n",
    "        [\"SATURDAY\", \"SUNDAY\"]), \"weekend\", df.ORGANIZATION_TYPE)\n",
    "    \n",
    "    # NAME_FAMILY_STATUS column feature engineering \n",
    "    df[\"NAME_FAMILY_STATUS\"] = np.where(df.NAME_FAMILY_STATUS.isin(\n",
    "        [\"Married\", \"Civil marriage\"]), \"Mutiple\", df.NAME_FAMILY_STATUS)\n",
    "        \n",
    "    df[\"NAME_FAMILY_STATUS\"] = np.where(df.NAME_FAMILY_STATUS.isin(\n",
    "        [\"Single / not married\"]), \"Single\", df.NAME_FAMILY_STATUS)\n",
    "    \n",
    "    df[\"NAME_FAMILY_STATUS\"] = np.where(df.NAME_FAMILY_STATUS.isin(\n",
    "    [\"Separated\" , \"Widow\"]), \"Broken\", df.NAME_FAMILY_STATUS)\n",
    "    \n",
    "    # NAME_EDUCATION_TYPE column feature engineering \n",
    "    df[\"NAME_EDUCATION_TYPE\"] = np.where(df.NAME_EDUCATION_TYPE.isin(\n",
    "    [\"Higher education\" , \"Academic degree\"]), \"Complete\", df.NAME_EDUCATION_TYPE)\n",
    "    \n",
    "    df[\"NAME_EDUCATION_TYPE\"] = np.where(df.NAME_EDUCATION_TYPE.isin(\n",
    "    [\"Secondary / secondary special \" , \"Incomplete higher\"]), \"Partial\", df.NAME_EDUCATION_TYPE)\n",
    "    \n",
    "    df[\"NAME_EDUCATION_TYPE\"] = np.where(df.NAME_EDUCATION_TYPE.isin(\n",
    "    [\"Lower secondary\"]), \"Lower\", df.NAME_EDUCATION_TYPE)\n",
    "    \n",
    "    # ORGANIZATION_TYPE column feature engineering \n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Business Entity\"), \"Business Entity\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Industry\"), \"Industry\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Trade\"), \"Trade\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Transport\"), \"Transport\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"School\", \"Kindergarten\", \"University\"]), \"Education\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Emergency\",\"Police\", \"Medicine\",\"Goverment\", \"Postal\", \"Military\", \"Security Ministries\", \"Legal Services\"]), \"Public\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Bank\", \"Insurance\"]), \"Finance\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Realtor\", \"Housing\"]), \"House\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Hotel\", \"Restaurant\"]), \"HotelRestaurant\", df.ORGANIZATION_TYPE)\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Advertising\", \"Religion\", \"Culture\"]), \"Other\", df.ORGANIZATION_TYPE)\n",
    "\n",
    "    # OCCUPATION_TYPE column feature engineering\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df.OCCUPATION_TYPE.isin([\"Low-skill Laborers\", \"Cooking staff\", \"Security staff\", \"Private service staff\", \"Cleaning staff\", \"Waiters/barmen staff\"]), \"Lowest skill\", df.OCCUPATION_TYPE)\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df.OCCUPATION_TYPE.isin([\"Laborers\",\"Secretaries\", \"Private service staff\", \"Security staff \", \"Sales staff\", \"Core staff\", \"Drivers\", ]), \"Low skill\", df.OCCUPATION_TYPE)\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df.OCCUPATION_TYPE.isin([\"HR staff\", \"Realty agents\", \"Managers\", \"IT staff\", \"High skill tech staff\"]), \"Middle skill\", df.OCCUPATION_TYPE)\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df.OCCUPATION_TYPE.isin([\"Managers\", \"IT staff\", \"High skill tech staff\", \"Accountants\", \"Medicine staff\"]), \"High skill\", df.OCCUPATION_TYPE)\n",
    "    \n",
    "    ## EXT SOURCE MEAN FROM OTHER ASSOCIATIONS \n",
    "    df[\"EXT_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df['EXT_SOURCES_SUM'] = df['EXT_SOURCE_1'] + df['EXT_SOURCE_2'] + df['EXT_SOURCE_3']\n",
    "\n",
    "    # NaN values for 365243 days of employment which is the max number \n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # Some simple new features (percentages)\n",
    "    df['AT_N_R_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['AT_N_R_INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['AT_N_R_INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['AT_N_R_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['AT_N_R_PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    \n",
    "    #Others\n",
    "    df[\"AT_N_R_GOODS_TO_CREDIT\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n",
    "    df['AT_N_R_ANNUITY_RATIO_INCOME'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df[\"AT_N_R_NEW_C_GP\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"AT_N_R_APP_AGE_YEARS\"] = round(df[\"AMT_GOODS_PRICE\"] * -1 / 365)\n",
    "    df['AT_N_R_INCOME_PER_PERSON_IN_FAMILY'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['AT_N_R_PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    df['AT_N_R_LOAN_GOOD_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    #Log Transformations\n",
    "    for column in cols_trans_to_log:\n",
    "        name = \"AT_L_O_\" + column \n",
    "        df[name] = np.log(df[column].abs() + 1)\n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Application Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All but one column with correlation of 95% with other columns were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng = feature_engineer_train(train, train_cols_95, cols_trans_to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng.to_csv(os.getcwd() + DATA_DIR + 'application_train_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering applied to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = datasets[\"application_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eng = feature_engineer_train(test, train_cols_95, cols_trans_to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eng.to_csv(os.getcwd() + DATA_DIR + 'application_test_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visual EDA <a name='EDAV'></a>\n",
    "\n",
    "## EDA Visualization functions\n",
    "\n",
    "Following are the functions created to do visualization. These are rudimentary level plots which provide insights into a dataframe. The input to these functions will either be a dataframe or a combination of dataframe and type of attribute (numerical, categorical) depending upon the type of function.\n",
    "\n",
    "+ **Attr_Type**: This function plots a bar graph which shows the count of type of attribute, i.e. Number, Categorical, Date etc. We are using similar logic in summary statistics where depending upon the type of variable, we derive the numerical and categorical features to further do the processing.\n",
    "+ **Unique Values**: This function plots the bar chart for the unique value for each attribute. This process will give us some insights about the number of binary , ordinal and continuous (more than 10 unique values) features in the dataset. This info can also be read from info and summary python functions. Another thing to notice here is that y-axis is on log scale. This is because on such a large data set, few elements have more unique values(like CURR ID, SK ID PREV) which makes distribution highly skewed. By taking log, we will have fair idea about other features with lesser unique values.\n",
    "+ **Percent Missing**: This bar plot will show to percentage of nulls in the data. This plot will be helpful for us to see how much nulls a attribute has. In phase I, we created pipeline with attributes with more than 50% nulls. This information will also help while finding the correlation and cross-verify if some values are less or highly correlated. 1 thing to note here is that this plot also has y-axis log scaled for the same reason as provided above. \n",
    "+ **Categorical_count**: This function will take a dataframe and categorical features as input and will produce a bar plot for each category. This will enable us to understand if certain set of values are more correlated to the target. For example, we saw that less educated people are more likely to default on their loan, as per the data provided. Now that we have seen this info as a plot, we can analyze  a little more. This will also help us group certain values which are negligible, For example, if 2 categories are related to 90 % of the data, we can group certain low occurring values to make them as 1 category. This can also be seen by using the F score to see how statistically important it can be.\n",
    "+ **Dendo**: This unique plot will groups together columns that have strong correlations in nullity. If a number of columns are grouped together at level zero, then the presence of nulls in one of those columns is directly related to the presence or absence of nulls in the others columns. The more separated the columns in the tree, the less likely the null values can be correlated between the columns.\n",
    "+ **Numerical_features**:  This function will take only the numerical attributes from a dataframe and produce a dot plot. Each dot in this plot is a sample in our dataset and each subplot represents a different feature. The y-axis shows the feature value, while the x-axis is the sample index. These kind of plots provide a lot of ideas for data cleaning and EDA.   \n",
    "+ **Num Hist**: This functions takes dataframe and numerical attributes. This will plot histogram for all the features supplied. A very useful visualization to see the data distribution in 1 view. We can take note of attributes which needs transformation, as well as outliers. For a better input data, we should remove the outliers so that it does not influence the target outcome. \n",
    "+ **all_missing_values_plot**: This function will plot the missing in all the features of the dataframe supplied. For attributes which have no nulls will show as 1, if an attribute has half of the records null, then it will show 0.5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attr_type(df):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    pd.value_counts(df.dtypes).sort_values().plot(kind=\"bar\", figsize=(15, 8),logy=False,\n",
    "                                              title=\"Type of features- Numerical/Categorical\",\n",
    "                                              ylabel=\"Number of features\");\n",
    "    plt.show()\n",
    "\n",
    "def unique_values(df):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    unique_values = df.select_dtypes(include=\"number\").nunique().sort_values(ascending=False)\n",
    "    unique_values.plot.bar(logy=True, figsize=(15, 4), title=\"Unique values per feature\");\n",
    "    plt.show()\n",
    "\n",
    "def percent_missing(df):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    df.isna().mean().sort_values().plot(kind=\"bar\", figsize=(15, 8),logy=True,\n",
    "                                          title=\"Percentage of missing values per feature\",\n",
    "                                          ylabel=\"Ratio of missing values per feature\");\n",
    "    plt.show()\n",
    "    \n",
    "def categorical_count(df,categorical):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i, col in enumerate(feat_cat):\n",
    "        ax = plt.subplot(5, 4, i+1)\n",
    "        sns.countplot(data=df[categorical], y=col, ax=ax,) \n",
    "    plt.suptitle('Category counts for all categorical variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def dendo(df):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    msno.dendrogram(df)\n",
    "    plt.show()\n",
    "    \n",
    "def numerical_features(df,num_cat):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    df[num_cat].plot(lw=0, marker=\".\", subplots=True, layout=(-1, 4),\n",
    "              figsize=(15, 12), markersize=1);\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def num_hist(df):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    df[np.isfinite(df)].hist(bins=25, figsize=(15, 25), layout=(-1, 5), edgecolor=\"black\")\n",
    "    plt.tight_layout();\n",
    "    plt.show();\n",
    "\n",
    "    \n",
    "def continous_features(df,feat_num):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    cols_continuous = df.select_dtypes(include=\"number\").nunique() >= 25\n",
    "    df_continuous = df[cols_continuous[cols_continuous].index]\n",
    "    #df_continuous.shape\n",
    "    sns.pairplot(df_continuous[feat_num], height=1.5, plot_kws={\"s\": 2, \"alpha\": 0.2});\n",
    "    plt.show()\n",
    "def all_missing_values_plot(df):\n",
    "    \n",
    "    div = df.columns[df.isin([np.nan]).any()]\n",
    "    plt.figure(figsize=(20,20))\n",
    "    (msno.bar(df[div]).set_title(\"All features with missing data\",fontsize=24))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDAV Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ds_name in enumerate(datasets.keys()):\n",
    "    \n",
    "    #n = True\n",
    "    #if n:\n",
    "        \n",
    "    if ds_name.lower() not in (#\"application_train\",\n",
    "                               \"application_test\", \"bureau_balance\"):\n",
    "        print(\"Table under consideration:\",ds_name.upper())\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        id_cols, feat_num, feat_cat, features =  id_num_cat_feature(datasets[ds_name],text = False)\n",
    "        only_num_cat = list(set(feat_num)-set(['SK_ID_CURR','SK_ID_PREV','SK_ID_BUREAU']))\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------Type of Features-------------------------------\")\n",
    "        attr_type(datasets[ds_name])\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------UNIQUE VALUES----------------------------------\")\n",
    "        unique_values(datasets[ds_name]) \n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------MISSING PERCENTAGE-----------------------------\")\n",
    "        percent_missing(datasets[ds_name])\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------CATEGORICAL COUNT------------------------------\")\n",
    "        categorical_count(datasets[ds_name],feat_cat)\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------NUM FEATURES-DOT PLOT----------------------\")\n",
    "        numerical_features(datasets[ds_name],only_num_cat)\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------NUM FEATURES - HISTOGRAM ----------------------\")\n",
    "        num_hist(datasets[ds_name][only_num_cat])\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------Continous Features ----------------------------\")\n",
    "        #continous_features(df_x,only_num_cat)\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------All Missing Features---------------------------\")\n",
    "        all_missing_values_plot(datasets[ds_name])\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(ds_name.upper(),\":-------------------------DendoGram for Nulls----------------------------\")\n",
    "        dendo(datasets[ds_name])\n",
    "        print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Plots Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. This correlation is a value put on the relationship between two attributes. A correlation matrix is used to summarize data, then with that information do more advanced analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_plot(data, remove=[\"Id\"], corr_coef = \"pearson\", figsize=(20, 20)):\n",
    "    if len(remove) > 0:\n",
    "        num_cols2 = [x for x in data.columns if (x not in remove)]\n",
    "\n",
    "    sns.set(font_scale=1.1)\n",
    "    c = data[num_cols2].corr(method = corr_coef)\n",
    "    mask = np.triu(c.corr(method = corr_coef))\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(c,\n",
    "                annot=True,\n",
    "                fmt='.1f',\n",
    "                cmap='coolwarm',\n",
    "                square=True,\n",
    "                mask=mask,\n",
    "                linewidths=1,\n",
    "                cbar=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ds_name in enumerate(datasets.keys()):\n",
    "    if(ds_name != \"bureau_balance\"):\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(\"Table under consideration FOR CORRELATION PLOT:\",ds_name.upper())\n",
    "        corr_plot(datasets[ds_name], remove=['SK_ID_CURR','SK_ID_BUREAU'], corr_coef = \"spearman\")\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in Application train and application test dataframes, most of the __FLAG_DOCUMENT_XX__ are null, we will remove these columns at later point of time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Plots and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visualizations consist of the paired plots for each column in the each table. \n",
    "\n",
    "The first is a histogram, a diagram consisting of rectangles whose area is proportional to the frequency of a variable and whose width is equal to the class interval, which helps to visualize distribution. \n",
    "\n",
    "The second is a boxplot, which is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile Q1, median, third quartile Q3 and “maximum”). It can tell you about your outliers and what their values are.\n",
    "\n",
    "https://builtin.com/data-science/boxplot#:~:text=What%20Is%20a%20Boxplot%3F,and%20what%20their%20values%20are.\n",
    "\n",
    "The third is a Kernel Density Plot which also helps to visualize distriubtion over a time peroid or some continuous value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_col_names(dataframe, cat_th=10, car_th=20, show_date=False):\n",
    "    date_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"datetime64[ns]\"]\n",
    "    cat_cols = dataframe.select_dtypes([\"object\", \"category\"]).columns.tolist()\n",
    "    \n",
    "    num_but_cat = [col for col in dataframe.select_dtypes([\"float\", \"integer\"]).columns if dataframe[col].nunique() < cat_th]\n",
    "    cat_but_car = [col for col in dataframe.select_dtypes([\"object\", \"category\"]).columns if dataframe[col].nunique() > car_th]\n",
    "    \n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    num_cols = dataframe.select_dtypes([\"float\", \"integer\"]).columns\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "    \n",
    "    if show_date == True:\n",
    "        return date_cols, cat_cols, cat_but_car, num_cols, num_but_cat\n",
    "    else:\n",
    "        return cat_cols, cat_but_car, num_cols, num_but_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_plot(data, num_cols, remove=[\"Id\"], hist_bins=10, figsize=(20, 4)):\n",
    "\n",
    "    if len(remove) > 0:\n",
    "        num_cols2 = [x for x in num_cols if (x not in remove)]\n",
    "\n",
    "    for i in num_cols2:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        data.hist(str(i), bins=hist_bins, ax=axes[0])\n",
    "        data.boxplot(str(i), ax=axes[1], vert=False);\n",
    "        try:\n",
    "            sns.kdeplot(np.array(data[str(i)]))\n",
    "        except:\n",
    "            ValueError\n",
    "\n",
    "        axes[1].set_yticklabels([])\n",
    "        axes[1].set_yticks([])\n",
    "        axes[0].set_title(i + \" | Histogram\")\n",
    "        axes[1].set_title(i + \" | Boxplot\")\n",
    "        axes[2].set_title(i + \" | Density\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ds_name in enumerate(datasets.keys()):\n",
    "    \n",
    "    if ds_name.lower() not in (\"application_train\",\n",
    "                               \"application_test\", \"bureau_balance\"):\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        #_,_, num_cols, _ = grab_col_names(datasets[ds_name], car_th=10)\n",
    "        _,num_cols,_,_ = id_num_cat_feature(datasets[ds_name], text = False)\n",
    "        datasets[ds_name].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        print(\"Table under consideration FOR NUMERICAL PLOTS:\",ds_name.upper())\n",
    "        num_plot(datasets[ds_name], num_cols, remove=['SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV'], figsize = (15,3))\n",
    "        print(\"------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Skewness/Distribution : Numerical data\n",
    "\n",
    "**Look for skewed column in numerical data but ignore dates, Days,Flags, status, ID's.**\n",
    "\n",
    "**Skewness in : AMT_INCOME_TOTAL,AMT_CREDIT,AMT_ANNUITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = datasets[\"application_train\"]\n",
    "numerical_ix = application_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_ix = application_train.select_dtypes(include=['object', 'bool']).columns\n",
    "num_features = list(numerical_ix)\n",
    "cat_features = list(categorical_ix)\n",
    "print(f\"# of numerical   features: {len(numerical_ix)}\")\n",
    "print(f\"Numerical   features: {numerical_ix}\")\n",
    "print('--------')\n",
    "print(f\"# of categorical features: {len(categorical_ix)}\")\n",
    "print(f\"Categorical features: {categorical_ix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"application_train\"][numerical_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AMT_CREDIT and AMT_ANNUITY looks skewed. we wil do log transformation on these attributes and make them more mormalized.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMT_CREDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datasets[\"application_train\"]['AMT_CREDIT'], bins=25);\n",
    "plt.xlabel('AMT_CREDIT')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Distribution - AMT_CREDIT \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(datasets[\"application_train\"]['AMT_CREDIT']), bins=30);\n",
    "plt.xlabel('Log(AMT_CREDIT)')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Distribution - Log(AMT_CREDIT) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMT_ANNUITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datasets[\"application_train\"]['AMT_ANNUITY'], bins=25);\n",
    "plt.xlabel('AMT_ANNUITY')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Distribution - AMT_ANNUITY \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(datasets[\"application_train\"]['AMT_ANNUITY']), bins=30);\n",
    "plt.xlabel('Log(AMT_ANNUITY)')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Distribution - Log(AMT_ANNUITY) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data for application train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = (datasets[\"application_train\"].isnull().sum()/datasets[\"application_train\"].isnull().count()*100).sort_values(ascending = False).round(2)\n",
    "sum_missing = datasets[\"application_train\"].isna().sum().sort_values(ascending = False)\n",
    "missing_application_train_data  = pd.concat([percent, sum_missing], axis=1, keys=['Percent', \"Train Missing Count\"])\n",
    "missing_application_train_data.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine more than 50% null\n",
    "\n",
    "Determine attributes which have more than 50% NULLS. Once done, these will be used as part of feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_50 = missing_application_train_data[round(missing_application_train_data['Percent']>50.0)==True]\n",
    "#nulls_50.index\n",
    "\n",
    "remove_num_nulls = list(set(nulls_50.index).intersection(set(numerical_ix)))\n",
    "remove_cat_nulls = list(set(nulls_50.index).intersection(set(categorical_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = (datasets[\"application_test\"].isnull().sum()/datasets[\"application_test\"].isnull().count()*100).sort_values(ascending = False).round(2)\n",
    "sum_missing = datasets[\"application_test\"].isna().sum().sort_values(ascending = False)\n",
    "missing_application_train_data  = pd.concat([percent, sum_missing], axis=1, keys=['Percent', \"Test Missing Count\"])\n",
    "missing_application_train_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of the target column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"application_train\"]['TARGET'].astype(int).plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This shows that around 8% of people are not able to repay the loans back.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"application_train\"]['TARGET'].value_counts()/datasets[\"application_train\"]['TARGET'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with  the target column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_against_target(df):\n",
    "    df_joined = {} \n",
    "    df_joined =  datasets['application_train'][[\"SK_ID_CURR\", \"TARGET\"]].merge(df, on='SK_ID_CURR', how='right')\n",
    "    cols = df.columns\n",
    "    keys = list(df.columns)\n",
    "    keys.append(\"TARGET\")\n",
    "    return df_joined[keys].corr()['TARGET'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations against the target on the Original Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corr = {}\n",
    "#barplot\n",
    "for i,ds_name in enumerate(datasets.keys()):\n",
    "    if(ds_name.upper() != \"APPLICATION_TRAIN\"):\n",
    "        if (ds_name.upper() != \"APPLICATION_TEST\"):\n",
    "            if (ds_name.upper() != \"BUREAU_BALANCE\"):\n",
    "                if (ds_name.upper() == \"BUREAU\"):\n",
    "                    if 'AMT_ANNUITY' in datasets[\"bureau\"].columns:\n",
    "                        datasets[\"bureau\"] = datasets[\"bureau\"].drop([\"AMT_ANNUITY\"], axis = 1)\n",
    "                print(\"------------------------------------------------------------------------\")\n",
    "                print(\"Correlation for Orignal Table:\", ds_name.upper())\n",
    "                print(\"------------------------------------------------------------------------\")\n",
    "                ds = correlation_against_target(datasets[ds_name])\n",
    "                print(\"------------------------------------------------------------------------\")\n",
    "                original_corr[ds_name] = ds\n",
    "                print(ds)\n",
    "                print(\"------------------------------------------------------------------------\")\n",
    "                print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations on the Transformed Tables done with a right merge against the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_corr = {}\n",
    "for i,ds_name in enumerate(datasets_transformed.keys()):\n",
    "    datasets_transformed[ds_name].reset_index()\n",
    "    #if(ds_name.upper() not in ( \"APPLICATION_TRAIN\",\"APPLICATION_TEST\",\"BUREAU_BALANCE\"):\n",
    "    if(ds_name.upper() != \"APPLICATION_TRAIN\"):\n",
    "        if (ds_name.upper() != \"APPLICATION_TEST\"):\n",
    "            #if (ds_name.upper() != \"POS_CASH_BALANCE\"):\n",
    "                if (ds_name.upper() != \"BUREAU_BALANCE\"):        \n",
    "                    print(\"------------------------------------------------------------------------\")\n",
    "                    print(\"Correlation for Transformed Table:\", ds_name.upper())\n",
    "                    print(\"------------------------------------------------------------------------\")\n",
    "                    ds = correlation_against_target(datasets_transformed[ds_name])\n",
    "                    print(\"------------------------------------------------------------------------\")\n",
    "                    transformed_corr[ds_name] = ds\n",
    "                    print(ds)\n",
    "                    print(\"------------------------------------------------------------------------\")\n",
    "                    print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highest Correlations on Original Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below gives you the highest correlated attributes from each table with a threshold of .7. This is correlation to other attributes in the same table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get high correlated variables\n",
    "def high_correlation(data, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7):\n",
    "    if len(remove) > 0:\n",
    "        cols = [x for x in data.columns if (x not in remove)]\n",
    "        c = data[cols].corr(method=corr_coef)\n",
    "    else:\n",
    "        c = data.corr(method=corr_coef)\n",
    "\n",
    "    for i in c.columns:\n",
    "        cr = c.loc[i].loc[(c.loc[i] >= corr_value) | (c.loc[i] <= -corr_value)].drop(i)\n",
    "        if len(cr) > 0:\n",
    "            print(i)\n",
    "            print(\"-------------------------------\")\n",
    "            print(cr.sort_values(ascending=False))\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"application_train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are determining the highest correlations for each table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ds_name in enumerate(datasets.keys()):\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Table under consideration FOR HIGHEST CORRELATIONS:\",ds_name.upper())\n",
    "    high_correlation(datasets[ds_name], remove=['SK_ID_CURR','SK_ID_BUREAU', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', \n",
    "                                                'FLAG_DOCUMENT_12','FLAG_DOCUMENT_13','FLAG_DOCUMENT_14',\n",
    "                                               'FLAG_DOCUMENT_15','FLAG_DOCUMENT_16','FLAG_DOCUMENT_17',\n",
    "                                               'FLAG_DOCUMENT_18','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20',\n",
    "                                               'FLAG_DOCUMENT_21','PREV_REJ_CNT'], corr_coef = \"spearman\", corr_value = 0.7)\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we find the attributes which are highly correlated, either positive or negative. Looks like EXT_SOURCE columns are highly correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = datasets[\"application_train\"].corr()['TARGET'].sort_values()\n",
    "print('Most Positive Correlations:\\n', correlations.tail(10))\n",
    "print('\\nMost Negative Correlations:\\n', correlations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top 10__ highly correlated columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = correlations.abs().sort_values().tail(11)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applicants Age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datasets[\"application_train\"]['DAYS_BIRTH'] / -365, edgecolor = 'k', bins = 25)\n",
    "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applicants occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='OCCUPATION_TYPE', data=datasets[\"application_train\"]);\n",
    "plt.title('Applicants Occupation');\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17% of our applicants are labourers and around 10% are from the Sales. This seems like folks which are from the lower income range which apply for the loan.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dataset questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### previous applications - Phase I Analysis\n",
    "The persons in the kaggle submission file have had previous applications in the `previous_application.csv`. 47,800 out 48,744 people have had previous appications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appsDF = datasets[\"previous_application\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.intersect1d(datasets[\"previous_application\"][\"SK_ID_CURR\"], datasets[\"application_test\"][\"SK_ID_CURR\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are  {appsDF.shape[0]:,} previous applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many entries are there for each month?\n",
    "prevAppCounts = appsDF['SK_ID_CURR'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prevAppCounts[prevAppCounts >40])  #more that 40 previous applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevAppCounts[prevAppCounts >50].plot(kind='bar')\n",
    "plt.xticks(rotation=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Number of previous applications for an ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(appsDF['SK_ID_CURR'].value_counts()==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(appsDF['SK_ID_CURR'].value_counts(), cumulative =True, bins = 100);\n",
    "plt.grid()\n",
    "plt.ylabel('cumulative number of IDs')\n",
    "plt.xlabel('Number of previous applications per ID')\n",
    "plt.title('Histogram of Number of previous applications for an ID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Can we differentiate applications by low, medium and high previous apps?**\n",
    "    * Low = <5 claims (22%)\n",
    "    * Medium = 10 to 39 claims (58%)\n",
    "    * High = 40 or more claims (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_all = appsDF['SK_ID_CURR'].nunique()\n",
    "apps_5plus = appsDF['SK_ID_CURR'].value_counts()>=5\n",
    "#print(apps_5plus)\n",
    "apps_40plus = appsDF['SK_ID_CURR'].value_counts()>=40\n",
    "apps_med_plus = 100 - apps_5plus- apps_40plus\n",
    "print('Percentage with 10 or more previous apps:', np.round(100.*(sum(apps_5plus)/apps_all),5))\n",
    "print('Percentage with 11 to 39 no of apps:', np.round(100-(100.*(sum(apps_5plus)/apps_all))-(100.*(sum(apps_40plus)/apps_all)),5))\n",
    "print('Percentage with 40 or more previous apps:', np.round(100.*(sum(apps_40plus)/apps_all),5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(prevAppCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Machine Learning Pipelines <a name='pipelines'></a>\n",
    "\n",
    "## Pre-Processing\n",
    "\n",
    "###  OHE when previously unseen unique values in the test/validation set\n",
    "\n",
    "Train, validation and Test sets (and the leakage problem we have mentioned previously):\n",
    "\n",
    " \n",
    "\n",
    "Let's look at a small usecase to tell us how to deal with this:\n",
    "\n",
    "* The OneHotEncoder is fitted to the training set, which means that for each unique value present in the training set, for each feature, a new column is created. Let's say we have 39 columns after the encoding up from 30 (before preprocessing).\n",
    "* The output is a numpy array (when the option sparse=False is used), which has the disadvantage of losing all the information about the original column names and values.\n",
    "* When we try to transform the test set, after having fitted the encoder to the training set, we obtain a `ValueError`. This is because the there are new, previously unseen unique values in the test set and the encoder doesn’t know how to handle these values. In order to use both the transformed training and test sets in machine learning algorithms, we need them to have the same number of columns.\n",
    "\n",
    "This last problem can be solved by using the option handle_unknown='ignore'of the OneHotEncoder, which, as the name suggests, will ignore previously unseen values when transforming the test set.\n",
    "\n",
    " \n",
    "\n",
    "Here is a example that in action:\n",
    "\n",
    "```python\n",
    "# Identify the categorical features we wish to consider.\n",
    "cat_attribs = ['CODE_GENDER', 'FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE', \n",
    "               'NAME_EDUCATION_TYPE','OCCUPATION_TYPE','NAME_INCOME_TYPE']\n",
    "\n",
    "# Notice handle_unknown=\"ignore\" in OHE which ignore values from the validation/test that\n",
    "# do NOT occur in the training set\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please [this  blog](https://medium.com/hugo-ferreiras-blog/dealing-with-categorical-features-in-machine-learning-1bb70f07262d) for more details of OHE when the validation/test have previously unseen unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameSelector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Algorithms\n",
    "\n",
    "The `sklearn.linear_model.LogisticRegression` implementation will be used for the baseline model with parameters penalty that can be any one the these `l1`, `l2`, `elasticnet`, a mutli class of `ovr` to fit each label using a binary problem, and `C` which is the inverse of regularization strength. The Logistic Regression loss function will be calculated using with cross entropy loss. \n",
    "\n",
    "The objective function for the learning a multinomial logistic regression model (log loss) can be stated as follows:\n",
    "\n",
    "$$\\textrm{CXE}(\\theta) = \\left[-\\frac{1}{m}\\sum_{i=1}^m\\left(y_i\\cdot\\log\\left(p_i\\right)+\\left(1-y_i\\right)\\cdot\\log\\left(1-p_i\\right)\\right)\\right]$$\n",
    "\n",
    "Regularization helps reduce the risk of overfitting.\n",
    "\n",
    "* Ridge Regularization (L2):\n",
    "  * $\\textrm{RidgeCXE}(\\theta) = \\textrm{CXE}(\\theta) + \\lambda \\sum_{j=1}^{n}\\theta_j^2$\n",
    "* Lasso Regularization (L1):\n",
    "  * $\\textrm{LassoCXE}(\\theta) = \\textrm{CXE}(\\theta) + \\lambda \\sum_{j=1}^n|\\theta_j|$\n",
    "* Elastic Net Regularization (Hybrid L1 + L2):\n",
    "  * $\\textrm{ElasticCXE}(\\theta) = \\textrm{CXE}(\\theta) + r\\lambda\\sum_{j=1}^n|\\theta_j| + \\frac{1-r}{2}\\lambda\\sum_{j=1}^n\\theta_j^2$\n",
    "  \n",
    "However, for the scope of Phase 2 regularization *will not* be incorporated into the baseline model due to time and computing power constraints. Regularization is planned to be incorporated in the Phase 3 model pipelines. \n",
    "\n",
    "## Baseline Models - Before EDA\n",
    "\n",
    "Here are some model runs using only the `application_train` data, with various feature selections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Baseline with 14 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the numeric features we wish to consider. \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split  # sklearn.cross_validation in old versions\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = datasets[\"application_train\"]\n",
    "y = data['TARGET']\n",
    "X = data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
    "\n",
    "# Split the provided training data into training and validationa and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")\n",
    "\n",
    "num_attribs = [\n",
    "    'AMT_INCOME_TOTAL',  'AMT_CREDIT','DAYS_EMPLOYED','DAYS_BIRTH','EXT_SOURCE_1',\n",
    "    'EXT_SOURCE_2','EXT_SOURCE_3']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "# Identify the categorical features we wish to consider.\n",
    "cat_attribs = ['CODE_GENDER', 'FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE', \n",
    "               'NAME_EDUCATION_TYPE','OCCUPATION_TYPE','NAME_INCOME_TYPE']\n",
    "\n",
    "selected_features = num_attribs + cat_attribs\n",
    "# Notice handle_unknown=\"ignore\" in OHE which ignore values from the validation/test that\n",
    "# do NOT occur in the training set\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        #('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "data_pipeline = ColumnTransformer(transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, num_attribs),\n",
    "        (\"cat_pipeline\", cat_pipeline, cat_attribs)],\n",
    "        remainder='drop',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_pipeline),\n",
    "        (\"linear\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "param_grid = {'linear__penalty':[#'l1', 'l2', 'elasticnet',\n",
    "                                 'none']\n",
    "              #,'linear__C':[1.0#, 10.0, 100.0 ]\n",
    "             }\n",
    "\n",
    "gd2 = GridSearchCV(full_pipeline_with_predictor, param_grid= param_grid, cv = 5, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "model = gd2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "    expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train AUC\", \n",
    "                                   \"Valid AUC\",\n",
    "                                   \"Test  AUC\"\n",
    "                                  ])\n",
    "\n",
    "exp_name = f\"Baseline_{len(selected_features)}_features\"\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [accuracy_score(y_train, model.predict(X_train)), \n",
    "                accuracy_score(y_valid, model.predict(X_valid)),\n",
    "                accuracy_score(y_test, model.predict(X_test)),\n",
    "                roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),\n",
    "                roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1]),\n",
    "                roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])],\n",
    "    4)) \n",
    "expLog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = datasets[\"application_train\"]\n",
    "y = data['TARGET']\n",
    "X = data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
    "\n",
    "# Split the provided training data into training and validationa and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2:  All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split  # sklearn.cross_validation in old versions\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "data = datasets[\"application_train\"]\n",
    "y = data['TARGET']\n",
    "X = data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
    "\n",
    "# Split the provided training data into training and validationa and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")\n",
    "\n",
    "numerical_features = list(numerical_ix[2:])\n",
    "\n",
    "num_pipeline =Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "categorical_features = list(categorical_ix)\n",
    "\n",
    "selected_features = (numerical_features) + (categorical_features)\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "\n",
    "data_pipeline = ColumnTransformer(transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, numerical_features),\n",
    "        (\"cat_pipeline\", cat_pipeline, categorical_features)],\n",
    "        remainder='drop',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_pipeline),\n",
    "        (\"linear\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "param_grid = {'linear__penalty':[#'l1', 'l2', 'elasticnet',\n",
    "                                 'none']\n",
    "              #,'linear__C':[1.0#, 10.0, 100.0]\n",
    "             }\n",
    "\n",
    "gd1 = GridSearchCV(full_pipeline_with_predictor, param_grid= param_grid, cv = 5, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "model = gd1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "    expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train AUC\", \n",
    "                                   \"Valid AUC\",\n",
    "                                   \"Test  AUC\"\n",
    "                                  ])\n",
    "\n",
    "exp_name = f\"Baseline_{len(selected_features)}_features\"\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [accuracy_score(y_train, model.predict(X_train)), \n",
    "                accuracy_score(y_valid, model.predict(X_valid)),\n",
    "                accuracy_score(y_test, model.predict(X_test)),\n",
    "                roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),\n",
    "                roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1]),\n",
    "                roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])],\n",
    "    4)) \n",
    "expLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 3: 79 Features\n",
    "\n",
    "**Selected Features**\n",
    "\n",
    "**Remove elements with more than 50% nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split  # sklearn.cross_validation in old versions\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "data = datasets[\"application_train\"]\n",
    "y = data['TARGET']\n",
    "X = data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
    "\n",
    "# Split the provided training data into training and validationa and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")\n",
    "\n",
    "numerical_features = list(numerical_ix[2:].drop(remove_num_nulls))\n",
    "\n",
    "num_pipeline =Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "categorical_features = list(categorical_ix.drop(remove_cat_nulls))\n",
    "\n",
    "selected_features = (numerical_features) + (categorical_features)\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "\n",
    "data_pipeline = ColumnTransformer(transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, numerical_features),\n",
    "        (\"cat_pipeline\", cat_pipeline, categorical_features)],\n",
    "        remainder='drop',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_pipeline),\n",
    "        (\"linear\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "param_grid = {'linear__penalty':[#'l1', 'l2', 'elasticnet',\n",
    "                                 'none']\n",
    "              #,'linear__C':[1.0#, 10.0, 100.0]\n",
    "                            }\n",
    "\n",
    "gd3 = GridSearchCV(full_pipeline_with_predictor, param_grid= param_grid, cv = 5, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "model = gd3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "    expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train AUC\", \n",
    "                                   \"Valid AUC\",\n",
    "                                   \"Test  AUC\"\n",
    "                                  ])\n",
    "\n",
    "exp_name = f\"Baseline_{len(selected_features)}_features\"\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [accuracy_score(y_train, model.predict(X_train)), \n",
    "                accuracy_score(y_valid, model.predict(X_valid)),\n",
    "                accuracy_score(y_test, model.predict(X_test)),\n",
    "                roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),\n",
    "                roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1]),\n",
    "                roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])],\n",
    "    4)) \n",
    "expLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 4 : 79 features; 2 log features\n",
    "\n",
    "**Remove elements with more than 50% nulls with log AMT_ANNUITY and AMT_CREDIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = datasets[\"application_train\"]\n",
    "y = data['TARGET']\n",
    "X = data.drop(['SK_ID_CURR', 'TARGET'], axis = 1) #drop some features with questionable value\n",
    "X['LOG_AMT_ANNUITY'] = np.log(X['AMT_ANNUITY']) #add LOG_AMT_ANNUITY colunm\n",
    "X = X.drop(['AMT_ANNUITY'], axis = 1) # drop AMT_ANNUITY colunm\n",
    "X['LOG_AMT_CREDIT'] = np.log(X['AMT_CREDIT']) #add LOG_AMT_ANNUITY colunm\n",
    "X = X.drop(['AMT_CREDIT'], axis = 1) # drop AMT_ANNUITY colunm\n",
    "\n",
    "# Split the provided training data into training and validationa and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")\n",
    "\n",
    "numerical_features = list(numerical_ix[2:].drop(remove_num_nulls))\n",
    "numerical_features.append('LOG_AMT_ANNUITY')\n",
    "numerical_features.append('LOG_AMT_CREDIT')\n",
    "numerical_features.remove('AMT_CREDIT')\n",
    "numerical_features.remove('AMT_ANNUITY')\n",
    "\n",
    "\n",
    "num_pipeline =Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "     \n",
    "categorical_features =  list(categorical_ix.drop(remove_cat_nulls))\n",
    "\n",
    "selected_features = (numerical_features) + (categorical_features)\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "\n",
    "data_pipeline = ColumnTransformer(transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, numerical_features),\n",
    "        (\"cat_pipeline\", cat_pipeline, categorical_features)],\n",
    "        remainder='drop',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_pipeline),\n",
    "        (\"linear\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "param_grid = {'linear__penalty':[#'l1', 'l2', 'elasticnet',\n",
    "                                 'none']\n",
    "              #,'linear__C':[1.0#, 10.0, 100.0]\n",
    "             }\n",
    "\n",
    "gd4 = GridSearchCV(full_pipeline_with_predictor, param_grid= param_grid, cv = 5, n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "model = gd4.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "    expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train AUC\", \n",
    "                                   \"Valid AUC\",\n",
    "                                   \"Test  AUC\"\n",
    "                                  ])\n",
    "\n",
    "exp_name = f\"Baseline_{len(selected_features)}_features with log attributes\"\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [accuracy_score(y_train, model.predict(X_train)), \n",
    "                accuracy_score(y_valid, model.predict(X_valid)),\n",
    "                accuracy_score(y_test, model.predict(X_test)),\n",
    "                roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),\n",
    "                roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 1]),\n",
    "                roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])],\n",
    "    4)) \n",
    "expLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model on Aggregated Features - Post EDA and Feature Engineering\n",
    "\n",
    "The model below was constructed after exploratory data analysis and feature engineering was performed on all the tables.  The sections below outline the data denormalization process (aggregating the tables into a singular input variable 'X') and then incorporate them into a baseline logistic regression model without regularization. \n",
    "\n",
    "### Data Denormalization\n",
    "\n",
    "This first section takes all of the aggregated tables and merges them into the single denormalized table, called `bureau_ip_ccb_prev_pos_merged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR =  \"/../Data/\"\n",
    "\n",
    "ds_names = (\"application_train_eng\", \"application_test_eng\", \"ccb_agg_data_tr\", \"ip_agg_data_tr\",\"prevapp_agg_data_tr\",\n",
    "            \"pos_agg_data_tr\",\"bureau_agg_data_transformed\")\n",
    "\n",
    "datasets_agg = {}\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    datasets_agg[ds_name] = pd.read_csv(os.getcwd() + DATA_DIR + f'{ds_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged = datasets_agg[\"application_train_eng\"].merge(datasets_agg[\"pos_agg_data_tr\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_merged_test = datasets_agg[\"application_test_eng\"].merge(datasets_agg[\"pos_agg_data_tr\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_pos_merged = pos_merged.merge(datasets_agg[\"prevapp_agg_data_tr\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_pos_merged_test = pos_merged_test.merge(datasets_agg[\"prevapp_agg_data_tr\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccb_prev_pos_merged = prev_pos_merged.merge(datasets_agg[\"ccb_agg_data_tr\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccb_prev_pos_merged_test = prev_pos_merged_test.merge(datasets_agg[\"ccb_agg_data_tr\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_ccb_prev_pos_merged = ccb_prev_pos_merged.merge(datasets_agg[\"ip_agg_data_tr\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_ccb_prev_pos_merged_test = ccb_prev_pos_merged_test.merge(datasets_agg[\"ip_agg_data_tr\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_ip_ccb_prev_pos_merged = ip_ccb_prev_pos_merged.merge(datasets_agg[\"bureau_agg_data_transformed\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_ip_ccb_prev_pos_merged_test = ip_ccb_prev_pos_merged_test.merge(datasets_agg[\"bureau_agg_data_transformed\"], on='SK_ID_CURR', how='left') \\\n",
    "               .replace(to_replace='\\s+', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\-', value='_', regex=True) \\\n",
    "               .replace(to_replace='\\(', value='', regex=True) \\\n",
    "               .replace(to_replace='\\)', value='', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_ip_ccb_prev_pos_merged.to_csv(os.getcwd() + DATA_DIR + 'bureau_ip_ccb_prev_pos_merged_train_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_ip_ccb_prev_pos_merged_test.to_csv(os.getcwd() + DATA_DIR + 'bureau_ip_ccb_prev_pos_merged_test_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_ip_ccb_prev_pos_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_ip_ccb_prev_pos_merged_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline with All Features\n",
    "\n",
    "This section runs the baseline ML pipeline using all the integrated data features as outlined by the block diagram below:\n",
    "\n",
    "<img src=\"../images/pipelineblockdiagram.png\" alt=\"drawing\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR =  \"/../Data/\"\n",
    "\n",
    "datasets_agg = {}\n",
    "datasets_agg[\"bureau_ip_ccb_prev_pos_merged_train_tr\"] = pd.read_csv(os.getcwd() + DATA_DIR + f'bureau_ip_ccb_prev_pos_merged_train_tr.csv')\n",
    "bureau_ip_ccb_prev_pos_merged = datasets_agg[\"bureau_ip_ccb_prev_pos_merged_train_tr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bureau_ip_ccb_prev_pos_merged['TARGET']\n",
    "X = bureau_ip_ccb_prev_pos_merged.drop(['SK_ID_CURR', 'TARGET', 'Unnamed: 0'], axis = 1) #drop some features with questionable value\n",
    "\n",
    "#X['LOG_AMT_ANNUITY'] = np.log(X['AMT_ANNUITY']) #add LOG_AMT_ANNUITY colunm\n",
    "#X = X.drop(['AMT_ANNUITY'], axis = 1) # drop AMT_ANNUITY colunm\n",
    "#X['LOG_AMT_CREDIT'] = np.log(X['AMT_CREDIT']) #add LOG_AMT_ANNUITY colunm\n",
    "#X = X.drop(['AMT_CREDIT'], axis = 1) # drop AMT_ANNUITY colunm\n",
    "\n",
    "# Split the provided training data into training and validationa and test\n",
    "_, X, _, y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train           shape: (39361, 1434)\n",
      "X validation      shape: (9841, 1434)\n",
      "X test            shape: (12301, 1434)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X train           shape: {X_train.shape}\")\n",
    "print(f\"X validation      shape: {X_valid.shape}\")\n",
    "print(f\"X test            shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols, feat_num, feat_cat, features =  id_num_cat_feature(X, text = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "     \n",
    "#categorical_features =  list(categorical_ix.drop(remove_cat_nulls))\n",
    "categorical_features =  feat_cat\n",
    "numerical_features = feat_num\n",
    "\n",
    "selected_features = (numerical_features) + (categorical_features)\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "\n",
    "data_pipeline = ColumnTransformer(transformers=[\n",
    "        (\"num_pipeline\", num_pipeline, numerical_features),\n",
    "        (\"cat_pipeline\", cat_pipeline, categorical_features)],\n",
    "        remainder='drop',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", data_pipeline),\n",
    "       # ('pca', decomposition.PCA()),\n",
    "        (\"logistic_Reg\", LogisticRegression(solver=\"liblinear\", max_iter=1000))\n",
    "    ])\n",
    "\n",
    "#n_components = list(range(1,X_train.shape[1]+1,1))\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "solvers = ['saga', 'liblinear']\n",
    "penalty = [\n",
    "           'l1', 'l2'\n",
    "          ]\n",
    "\n",
    "parameters = dict(#pca__n_components=n_components,\n",
    "                      logistic_Reg__C=C,\n",
    "                      logistic_Reg__solver=solvers,\n",
    "                    logistic_Reg__penalty=penalty\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd4 = GridSearchCV(full_pipeline_with_predictor, param_grid=parameters, cv=3, n_jobs=4, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "gd4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression(C=0.01, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# print('Best Penalty:', gd4.best_estimator_.get_params()['logistic_Reg__penalty'])\n",
    "# print('Best C:', gd4.best_estimator_.get_params()['logistic_Reg__C'])\n",
    "# #print('Best Number Of Components:', gd4.best_estimator_.get_params()['pca__n_components'])\n",
    "print(); print(gd4.best_estimator_.get_params()['logistic_Reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "    expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train AUC\", \n",
    "                                   \"Valid AUC\",\n",
    "                                   \"Test  AUC\"\n",
    "                                  ])\n",
    "\n",
    "exp_name = f\"Baseline_{len(selected_features)}_features with log attributes\"\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [accuracy_score(y_train, gd4.predict(X_train)), \n",
    "                accuracy_score(y_valid, gd4.predict(X_valid)),\n",
    "                accuracy_score(y_test, gd4.predict(X_test)),\n",
    "                roc_auc_score(y_train, gd4.predict_proba(X_train)[:, 1]),\n",
    "                roc_auc_score(y_valid, gd4.predict_proba(X_valid)[:, 1]),\n",
    "                roc_auc_score(y_test, gd4.predict_proba(X_test)[:, 1])],\n",
    "    4)) \n",
    "expLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission File Prep\n",
    "\n",
    "For each SK_ID_CURR in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:\n",
    "\n",
    "```python \n",
    "SK_ID_CURR,TARGET\n",
    "100001,0.1\n",
    "100005,0.9\n",
    "100013,0.2\n",
    "etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR =  \"/../Data/\"\n",
    "\n",
    "datasets_agg = {}\n",
    "datasets_agg[\"bureau_ip_ccb_prev_pos_merged_test\"] = pd.read_csv(os.getcwd() + DATA_DIR + f'bureau_ip_ccb_prev_pos_merged_test.csv')\n",
    "bureau_ip_ccb_prev_pos_merged_test = datasets_agg[\"bureau_ip_ccb_prev_pos_merged_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = bureau_ip_ccb_prev_pos_merged_test\n",
    "X_Kaggle_test = data_test.drop(['SK_ID_CURR','Unnamed: 0'], axis = 1)\n",
    "X_Kaggle_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(X_Kaggle_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Kaggle_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#X_Kaggle_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_scores = gd4.predict_proba(X_Kaggle_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = datasets[\"application_test\"][['SK_ID_CURR']]\n",
    "submit_df['TARGET'] = test_class_scores\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kaggle Submission\n",
    "\n",
    "Click on this [link](https://www.kaggle.com/c/home-credit-default-risk/submissions?sortBy=date&group=all&page=1)\n",
    "\n",
    "<img src=\"../images/p2Kaggle.png\" alt=\"drawing\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Discussion <a name='results'></a>\n",
    "\n",
    "## Models before EDA\n",
    "\n",
    "Below are the results from our Pre-EDA Experiment Log.\n",
    "\n",
    "<img src=\"../images/baseline_apptrain.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "The results of our baseline models constructed before EDA and feature aggregation were a good starting point. With only a modicum of complexity, we were able to establish an ROC-AUC score of 74.34%. Below are the details on phase I results:\n",
    "1. Initial run was considered with 14 features, 7 numerical and 7 categorical. We got training accuracy of 91.98%, test accuracy of 91.58% and AUC score as 73.57% for this very first model.\n",
    "   + Num cols : ‘AMT_INCOME_TOTAL',  'AMT_CREDIT','DAYS_EMPLOYED','DAYS_BIRTH','EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3’, \n",
    "   + Categorical Cols : 'CODE_GENDER','FLAG_OWN_REALTY','FLAG_OWN_CAR','NAME_CONTRACT_TYPE','NAME_EDUCATION_TYPE',             'OCCUPATION_TYPE', 'NAME_INCOME_TYPE'\n",
    "2. Next we considered all 120 elements form Application train table. Here we got a marginal increase to 92% train accuracy,  and 91.93% test accuracy. But there was improvement in AUC score, which was 74.34 % in this iteration.\n",
    "3. Next, we used a function to remove features which were more than 50% NULL. This led to a list of 79 features. There was very minute change in the test and train score. AUC score reduced from 74.34% to 74.06%. Although reducing the number of features was definitely a boost but we expect same or higher AUC score.\n",
    "4. Next, we log transformed 2 attributes which were highly skewed and used the features from run 3 from above step. This did not help either, we got same score as in last run up to first decimal point. 1 reason for this is because the attribute we log transformed had a big null percentage which didn’t really helped the score. \n",
    "\n",
    "Our Kaggle score for submission was 73.3 for this phase.\n",
    "\n",
    "## Model after EDA\n",
    "\n",
    "Below is the result from the Post-EDA Experiment Log.\n",
    "\n",
    "<img src=\"../images/baseline_allfeatures.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "After EDA with the new features added, our model performed significantly better with ROC-AUC scores averaging around 77%, with the highest being 77.09%. The inclusion of newly engineered features did help increase our scores, which was anyway our main goal for this phase of the project. \n",
    "\n",
    "Here we have taken all 1277 aggregated features for the pipeline run. As explained in the EDA part, we aggregated all features with \"min\", \"max\", \"count\", \"sum\", \"median\", \"mean\", and  \"var\" and subsequently rolled up all those to the main train and test tables. \n",
    "\n",
    "---\n",
    "\n",
    "We also created more features as part of our analysis on features:\n",
    "\n",
    "From POS: \n",
    "+ Percentage of installments pending.\n",
    "+ Number of installments pending.\n",
    "+ Days with Tolerance.\n",
    "\n",
    "From Previous Application:\n",
    "+ Count of approved previous application.\n",
    "+ Count of Rejected previous applications.\n",
    "+ Difference: Amount requested in application - Actual credit amount.\n",
    "+ Ratio - Ratio of application amount to amount credited.\n",
    "+ Ratio - Ratio of amount credited to amount annuity\n",
    "+ Ratio - Ratio of down payment to amount credited.\n",
    "\n",
    "From Installments payments: \n",
    "+ Difference of payment amount from installment amount.\n",
    "\n",
    "---\n",
    "\n",
    "Although we did not see much difference in test score but there was an increase in the test score by almost 1%. Scores were .9198 and .9209 respectively. But to expectations, there was a substantial increment in the test AUC score which now has the score of 77.09 %. This 3.6% increase in AUC score is deemed a good outcome. This was also evident in our Kaggle submission score of 76.17 %.\n",
    "\n",
    "In both the phases, by using pipelines and parameters, we kept data leakage at bay. See Kaggle Submission below:\n",
    "\n",
    "<img src=\"../images/p2Kaggle.png\" alt=\"drawing\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a name='conclusion'></a>\n",
    "\n",
    "The goal of this project is to predict an aplicant's default risk using data from the loan application, as well as past credit accounts. This is important as it will provide a way to more equitably provide loans to people with poor credit history. With the conclusion of Phase 2, Group 10 has accomplished the following tasks:\n",
    "\n",
    "1. Analyzed the data structures and relationships through visual and numerical exploratory data analysis.\n",
    "2. Transformed the normalized data tables into a denormalized input variable and integrated with the machine learning pipeline. \n",
    "3. Laid the foundation for future hyperparameter tuning and model engineering. \n",
    "\n",
    "One challenge was building a universal transformation method to apply to the tables. Group 10 solved this issue by initially analyzing the tables separately from one another, and then collaborating intensely once their table domain was understood to find the common transformation methods - also to identify where the table transformations must be different. \n",
    "\n",
    "Another challenge overcome was working with the large (2.7 gb) high-dimensional dataset. This was overcome by simplifying code, running code chunks in batches, and using GPU-enhanced computing methods like Google Collab. Also special thanks to Pragat for training the model on his machine for *hours* overnight. \n",
    "\n",
    "With a AUC-ROC score of 0.76 on the test data using the fully integrated dataset, this baseline pipeline shows logistic regression is a decent tool that can successfully use the massive amount of data to make predictions on applicant default risk. Furthermore, it shows an improvement when using *all* of the data as opposed to just data from the applications. \n",
    "\n",
    "In the next phase, we will focus on hyperparameter tuning, additional feature engineering and feature selection, and refining the overall machine-learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography <a name='bibliography'></a>\n",
    "\n",
    "1. https://towardsdatascience.com/using-the-missingno-python-library-to-identify-and-visualise- missing-data-prior-to-machine-learning-34c8c5b5f009\n",
    "2. https://www.analyticsvidhya.com/blog/2021/04/rapid-fire-eda-process-using-python-for-ml-implementation\n",
    "3. https://www.kaggle.com/code/ekrembayar/homecredit-default-risk-step-by-step-1st-notebook/notebook#8.-Installments-Payments ",
    " -some code was directly used from this notebook, as the code was so well written, and was used as a reference for EDA, as explanations were through.\n",
    "4. https://miykael.github.io/blog/2022/advanced_eda/\n",
    "5. Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning : with Applications in R. Chapter 3. New York :Springer, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dictionary <a name='datadict'></a>\n",
    "\n",
    "\n",
    "|     | Table                         | Row                          | Description                                                                                                                                                                                                                                                                            | Special                               | \n",
    "|-----|-------------------------------|------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------| \n",
    "| 1   | application_{train\\|test}.csv | SK_ID_CURR                   | ID of loan in our sample                                                                                                                                                                                                                                                               |                                       | \n",
    "| 2   | application_{train\\|test}.csv | TARGET                       | \"Target variable (1 - client with payment difficulties: he/she had late payment more than X days on at least one of the first Y installments of the loan in our sample, 0 - all other cases)\"                                                                                          |                                       | \n",
    "| 5   | application_{train\\|test}.csv | NAME_CONTRACT_TYPE           | Identification if loan is cash or revolving                                                                                                                                                                                                                                            |                                       | \n",
    "| 6   | application_{train\\|test}.csv | CODE_GENDER                  | Gender of the client                                                                                                                                                                                                                                                                   |                                       | \n",
    "| 7   | application_{train\\|test}.csv | FLAG_OWN_CAR                 | Flag if the client owns a car                                                                                                                                                                                                                                                          |                                       | \n",
    "| 8   | application_{train\\|test}.csv | FLAG_OWN_REALTY              | Flag if client owns a house or flat                                                                                                                                                                                                                                                    |                                       | \n",
    "| 9   | application_{train\\|test}.csv | CNT_CHILDREN                 | Number of children the client has                                                                                                                                                                                                                                                      |                                       | \n",
    "| 10  | application_{train\\|test}.csv | AMT_INCOME_TOTAL             | Income of the client                                                                                                                                                                                                                                                                   |                                       | \n",
    "| 11  | application_{train\\|test}.csv | AMT_CREDIT                   | Credit amount of the loan                                                                                                                                                                                                                                                              |                                       | \n",
    "| 12  | application_{train\\|test}.csv | AMT_ANNUITY                  | Loan annuity                                                                                                                                                                                                                                                                           |                                       | \n",
    "| 13  | application_{train\\|test}.csv | AMT_GOODS_PRICE              | For consumer loans it is the price of the goods for which the loan is given                                                                                                                                                                                                            |                                       | \n",
    "| 14  | application_{train\\|test}.csv | NAME_TYPE_SUITE              | Who was accompanying client when he was applying for the loan                                                                                                                                                                                                                          |                                       | \n",
    "| 15  | application_{train\\|test}.csv | NAME_INCOME_TYPE             | \"Clients income type (businessman, working, maternity leave,",
    ")\"                                                                                                                                                                                                                        |                                       | \n",
    "| 16  | application_{train\\|test}.csv | NAME_EDUCATION_TYPE          | Level of highest education the client achieved                                                                                                                                                                                                                                         |                                       | \n",
    "| 17  | application_{train\\|test}.csv | NAME_FAMILY_STATUS           | Family status of the client                                                                                                                                                                                                                                                            |                                       | \n",
    "| 18  | application_{train\\|test}.csv | NAME_HOUSING_TYPE            | \"What is the housing situation of the client (renting, living with parents, ...)\"                                                                                                                                                                                                      |                                       | \n",
    "| 19  | application_{train\\|test}.csv | REGION_POPULATION_RELATIVE   | Normalized population of region where client lives (higher number means the client lives in more populated region)                                                                                                                                                                     | normalized                            | \n",
    "| 20  | application_{train\\|test}.csv | DAYS_BIRTH                   | Client's age in days at the time of application                                                                                                                                                                                                                                        | time only relative to the application | \n",
    "| 21  | application_{train\\|test}.csv | DAYS_EMPLOYED                | How many days before the application the person started current employment                                                                                                                                                                                                             | time only relative to the application | \n",
    "| 22  | application_{train\\|test}.csv | DAYS_REGISTRATION            | How many days before the application did client change his registration                                                                                                                                                                                                                | time only relative to the application | \n",
    "| 23  | application_{train\\|test}.csv | DAYS_ID_PUBLISH              | How many days before the application did client change the identity document with which he applied for the loan                                                                                                                                                                        | time only relative to the application | \n",
    "| 24  | application_{train\\|test}.csv | OWN_CAR_AGE                  | Age of client's car                                                                                                                                                                                                                                                                    |                                       | \n",
    "| 25  | application_{train\\|test}.csv | FLAG_MOBIL                   | \"Did client provide mobile phone (1=YES, 0=NO)\"                                                                                                                                                                                                                                        |                                       | \n",
    "| 26  | application_{train\\|test}.csv | FLAG_EMP_PHONE               | \"Did client provide work phone (1=YES, 0=NO)\"                                                                                                                                                                                                                                          |                                       | \n",
    "| 27  | application_{train\\|test}.csv | FLAG_WORK_PHONE              | \"Did client provide home phone (1=YES, 0=NO)\"                                                                                                                                                                                                                                          |                                       | \n",
    "| 28  | application_{train\\|test}.csv | FLAG_CONT_MOBILE             | \"Was mobile phone reachable (1=YES, 0=NO)\"                                                                                                                                                                                                                                             |                                       | \n",
    "| 29  | application_{train\\|test}.csv | FLAG_PHONE                   | \"Did client provide home phone (1=YES, 0=NO)\"                                                                                                                                                                                                                                          |                                       | \n",
    "| 30  | application_{train\\|test}.csv | FLAG_EMAIL                   | \"Did client provide email (1=YES, 0=NO)\"                                                                                                                                                                                                                                               |                                       | \n",
    "| 31  | application_{train\\|test}.csv | OCCUPATION_TYPE              | What kind of occupation does the client have                                                                                                                                                                                                                                           |                                       | \n",
    "| 32  | application_{train\\|test}.csv | CNT_FAM_MEMBERS              | How many family members does client have                                                                                                                                                                                                                                               |                                       | \n",
    "| 33  | application_{train\\|test}.csv | REGION_RATING_CLIENT         | \"Our rating of the region where client lives (1,2,3)\"                                                                                                                                                                                                                                  |                                       | \n",
    "| 34  | application_{train\\|test}.csv | REGION_RATING_CLIENT_W_CITY  | \"Our rating of the region where client lives with taking city into account (1,2,3)\"                                                                                                                                                                                                    |                                       | \n",
    "| 35  | application_{train\\|test}.csv | WEEKDAY_APPR_PROCESS_START   | On which day of the week did the client apply for the loan                                                                                                                                                                                                                             |                                       | \n",
    "| 36  | application_{train\\|test}.csv | HOUR_APPR_PROCESS_START      | Approximately at what hour did the client apply for the loan                                                                                                                                                                                                                           | rounded                               | \n",
    "| 37  | application_{train\\|test}.csv | REG_REGION_NOT_LIVE_REGION   | \"Flag if client's permanent address does not match contact address (1=different, 0=same, at region level)\"                                                                                                                                                                             |                                       | \n",
    "| 38  | application_{train\\|test}.csv | REG_REGION_NOT_WORK_REGION   | \"Flag if client's permanent address does not match work address (1=different, 0=same, at region level)\"                                                                                                                                                                                |                                       | \n",
    "| 39  | application_{train\\|test}.csv | LIVE_REGION_NOT_WORK_REGION  | \"Flag if client's contact address does not match work address (1=different, 0=same, at region level)\"                                                                                                                                                                                  |                                       | \n",
    "| 40  | application_{train\\|test}.csv | REG_CITY_NOT_LIVE_CITY       | \"Flag if client's permanent address does not match contact address (1=different, 0=same, at city level)\"                                                                                                                                                                               |                                       | \n",
    "| 41  | application_{train\\|test}.csv | REG_CITY_NOT_WORK_CITY       | \"Flag if client's permanent address does not match work address (1=different, 0=same, at city level)\"                                                                                                                                                                                  |                                       | \n",
    "| 42  | application_{train\\|test}.csv | LIVE_CITY_NOT_WORK_CITY      | \"Flag if client's contact address does not match work address (1=different, 0=same, at city level)\"                                                                                                                                                                                    |                                       | \n",
    "| 43  | application_{train\\|test}.csv | ORGANIZATION_TYPE            | Type of organization where client works                                                                                                                                                                                                                                                |                                       | \n",
    "| 44  | application_{train\\|test}.csv | EXT_SOURCE_1                 | Normalized score from external data source                                                                                                                                                                                                                                             | normalized                            | \n",
    "| 45  | application_{train\\|test}.csv | EXT_SOURCE_2                 | Normalized score from external data source                                                                                                                                                                                                                                             | normalized                            | \n",
    "| 46  | application_{train\\|test}.csv | EXT_SOURCE_3                 | Normalized score from external data source                                                                                                                                                                                                                                             | normalized                            | \n",
    "| 47  | application_{train\\|test}.csv | APARTMENTS_AVG               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 48  | application_{train\\|test}.csv | BASEMENTAREA_AVG             | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 49  | application_{train\\|test}.csv | YEARS_BEGINEXPLUATATION_AVG  | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 50  | application_{train\\|test}.csv | YEARS_BUILD_AVG              | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 51  | application_{train\\|test}.csv | COMMONAREA_AVG               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 52  | application_{train\\|test}.csv | ELEVATORS_AVG                | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 53  | application_{train\\|test}.csv | ENTRANCES_AVG                | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 54  | application_{train\\|test}.csv | FLOORSMAX_AVG                | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 55  | application_{train\\|test}.csv | FLOORSMIN_AVG                | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 56  | application_{train\\|test}.csv | LANDAREA_AVG                 | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 57  | application_{train\\|test}.csv | LIVINGAPARTMENTS_AVG         | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 58  | application_{train\\|test}.csv | LIVINGAREA_AVG               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 59  | application_{train\\|test}.csv | NONLIVINGAPARTMENTS_AVG      | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 60  | application_{train\\|test}.csv | NONLIVINGAREA_AVG            | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 61  | application_{train\\|test}.csv | APARTMENTS_MODE              | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 62  | application_{train\\|test}.csv | BASEMENTAREA_MODE            | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 63  | application_{train\\|test}.csv | YEARS_BEGINEXPLUATATION_MODE | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 64  | application_{train\\|test}.csv | YEARS_BUILD_MODE             | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 65  | application_{train\\|test}.csv | COMMONAREA_MODE              | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 66  | application_{train\\|test}.csv | ELEVATORS_MODE               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 67  | application_{train\\|test}.csv | ENTRANCES_MODE               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 68  | application_{train\\|test}.csv | FLOORSMAX_MODE               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 69  | application_{train\\|test}.csv | FLOORSMIN_MODE               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 70  | application_{train\\|test}.csv | LANDAREA_MODE                | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 71  | application_{train\\|test}.csv | LIVINGAPARTMENTS_MODE        | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 72  | application_{train\\|test}.csv | LIVINGAREA_MODE              | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 73  | application_{train\\|test}.csv | NONLIVINGAPARTMENTS_MODE     | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 74  | application_{train\\|test}.csv | NONLIVINGAREA_MODE           | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 75  | application_{train\\|test}.csv | APARTMENTS_MEDI              | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 76  | application_{train\\|test}.csv | BASEMENTAREA_MEDI            | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 77  | application_{train\\|test}.csv | YEARS_BEGINEXPLUATATION_MEDI | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 78  | application_{train\\|test}.csv | YEARS_BUILD_MEDI             | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 79  | application_{train\\|test}.csv | COMMONAREA_MEDI              | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 80  | application_{train\\|test}.csv | ELEVATORS_MEDI               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 81  | application_{train\\|test}.csv | ENTRANCES_MEDI               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 82  | application_{train\\|test}.csv | FLOORSMAX_MEDI               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 83  | application_{train\\|test}.csv | FLOORSMIN_MEDI               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 84  | application_{train\\|test}.csv | LANDAREA_MEDI                | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 85  | application_{train\\|test}.csv | LIVINGAPARTMENTS_MEDI        | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 86  | application_{train\\|test}.csv | LIVINGAREA_MEDI              | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 87  | application_{train\\|test}.csv | NONLIVINGAPARTMENTS_MEDI     | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 88  | application_{train\\|test}.csv | NONLIVINGAREA_MEDI           | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 89  | application_{train\\|test}.csv | FONDKAPREMONT_MODE           | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 90  | application_{train\\|test}.csv | HOUSETYPE_MODE               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 91  | application_{train\\|test}.csv | TOTALAREA_MODE               | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 92  | application_{train\\|test}.csv | WALLSMATERIAL_MODE           | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 93  | application_{train\\|test}.csv | EMERGENCYSTATE_MODE          | \"Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor\" | normalized                            | \n",
    "| 94  | application_{train\\|test}.csv | OBS_30_CNT_SOCIAL_CIRCLE     | How many observation of client's social surroundings with observable 30 DPD (days past due) default                                                                                                                                                                                    |                                       | \n",
    "| 95  | application_{train\\|test}.csv | DEF_30_CNT_SOCIAL_CIRCLE     | How many observation of client's social surroundings defaulted on 30 DPD (days past due)                                                                                                                                                                                               |                                       | \n",
    "| 96  | application_{train\\|test}.csv | OBS_60_CNT_SOCIAL_CIRCLE     | How many observation of client's social surroundings with observable 60 DPD (days past due) default                                                                                                                                                                                    |                                       | \n",
    "| 97  | application_{train\\|test}.csv | DEF_60_CNT_SOCIAL_CIRCLE     | How many observation of client's social surroundings defaulted on 60 (days past due) DPD                                                                                                                                                                                               |                                       | \n",
    "| 98  | application_{train\\|test}.csv | DAYS_LAST_PHONE_CHANGE       | How many days before application did client change phone                                                                                                                                                                                                                               |                                       | \n",
    "| 99  | application_{train\\|test}.csv | FLAG_DOCUMENT_2              | Did client provide document 2                                                                                                                                                                                                                                                          |                                       | \n",
    "| 100 | application_{train\\|test}.csv | FLAG_DOCUMENT_3              | Did client provide document 3                                                                                                                                                                                                                                                          |                                       | \n",
    "| 101 | application_{train\\|test}.csv | FLAG_DOCUMENT_4              | Did client provide document 4                                                                                                                                                                                                                                                          |                                       | \n",
    "| 102 | application_{train\\|test}.csv | FLAG_DOCUMENT_5              | Did client provide document 5                                                                                                                                                                                                                                                          |                                       | \n",
    "| 103 | application_{train\\|test}.csv | FLAG_DOCUMENT_6              | Did client provide document 6                                                                                                                                                                                                                                                          |                                       | \n",
    "| 104 | application_{train\\|test}.csv | FLAG_DOCUMENT_7              | Did client provide document 7                                                                                                                                                                                                                                                          |                                       | \n",
    "| 105 | application_{train\\|test}.csv | FLAG_DOCUMENT_8              | Did client provide document 8                                                                                                                                                                                                                                                          |                                       | \n",
    "| 106 | application_{train\\|test}.csv | FLAG_DOCUMENT_9              | Did client provide document 9                                                                                                                                                                                                                                                          |                                       | \n",
    "| 107 | application_{train\\|test}.csv | FLAG_DOCUMENT_10             | Did client provide document 10                                                                                                                                                                                                                                                         |                                       | \n",
    "| 108 | application_{train\\|test}.csv | FLAG_DOCUMENT_11             | Did client provide document 11                                                                                                                                                                                                                                                         |                                       | \n",
    "| 109 | application_{train\\|test}.csv | FLAG_DOCUMENT_12             | Did client provide document 12                                                                                                                                                                                                                                                         |                                       | \n",
    "| 110 | application_{train\\|test}.csv | FLAG_DOCUMENT_13             | Did client provide document 13                                                                                                                                                                                                                                                         |                                       | \n",
    "| 111 | application_{train\\|test}.csv | FLAG_DOCUMENT_14             | Did client provide document 14                                                                                                                                                                                                                                                         |                                       | \n",
    "| 112 | application_{train\\|test}.csv | FLAG_DOCUMENT_15             | Did client provide document 15                                                                                                                                                                                                                                                         |                                       | \n",
    "| 113 | application_{train\\|test}.csv | FLAG_DOCUMENT_16             | Did client provide document 16                                                                                                                                                                                                                                                         |                                       | \n",
    "| 114 | application_{train\\|test}.csv | FLAG_DOCUMENT_17             | Did client provide document 17                                                                                                                                                                                                                                                         |                                       | \n",
    "| 115 | application_{train\\|test}.csv | FLAG_DOCUMENT_18             | Did client provide document 18                                                                                                                                                                                                                                                         |                                       | \n",
    "| 116 | application_{train\\|test}.csv | FLAG_DOCUMENT_19             | Did client provide document 19                                                                                                                                                                                                                                                         |                                       | \n",
    "| 117 | application_{train\\|test}.csv | FLAG_DOCUMENT_20             | Did client provide document 20                                                                                                                                                                                                                                                         |                                       | \n",
    "| 118 | application_{train\\|test}.csv | FLAG_DOCUMENT_21             | Did client provide document 21                                                                                                                                                                                                                                                         |                                       | \n",
    "| 119 | application_{train\\|test}.csv | AMT_REQ_CREDIT_BUREAU_HOUR   | Number of enquiries to Credit Bureau about the client one hour before application                                                                                                                                                                                                      |                                       | \n",
    "| 120 | application_{train\\|test}.csv | AMT_REQ_CREDIT_BUREAU_DAY    | Number of enquiries to Credit Bureau about the client one day before application (excluding one hour before application)                                                                                                                                                               |                                       | \n",
    "| 121 | application_{train\\|test}.csv | AMT_REQ_CREDIT_BUREAU_WEEK   | Number of enquiries to Credit Bureau about the client one week before application (excluding one day before application)                                                                                                                                                               |                                       | \n",
    "| 122 | application_{train\\|test}.csv | AMT_REQ_CREDIT_BUREAU_MON    | Number of enquiries to Credit Bureau about the client one month before application (excluding one week before application)                                                                                                                                                             |                                       | \n",
    "| 123 | application_{train\\|test}.csv | AMT_REQ_CREDIT_BUREAU_QRT    | Number of enquiries to Credit Bureau about the client 3 month before application (excluding one month before application)                                                                                                                                                              |                                       | \n",
    "| 124 | application_{train\\|test}.csv | AMT_REQ_CREDIT_BUREAU_YEAR   | Number of enquiries to Credit Bureau about the client one day year (excluding last 3 months before application)                                                                                                                                                                        |                                       | \n",
    "| 125 | bureau.csv                    | SK_ID_CURR                   | \"ID of loan in our sample - one loan in our sample can have 0,1,2 or more related previous credits in credit bureau \"                                                                                                                                                                  | hashed                                | \n",
    "| 126 | bureau.csv                    | SK_BUREAU_ID                 | Recoded ID of previous Credit Bureau credit related to our loan (unique coding for each loan application)                                                                                                                                                                              | hashed                                | \n",
    "| 127 | bureau.csv                    | CREDIT_ACTIVE                | Status of the Credit Bureau (CB) reported credits                                                                                                                                                                                                                                      |                                       | \n",
    "| 128 | bureau.csv                    | CREDIT_CURRENCY              | Recoded currency of the Credit Bureau credit                                                                                                                                                                                                                                           | recoded                               | \n",
    "| 129 | bureau.csv                    | DAYS_CREDIT                  | How many days before current application did client apply for Credit Bureau credit                                                                                                                                                                                                     | time only relative to the application | \n",
    "| 130 | bureau.csv                    | CREDIT_DAY_OVERDUE           | Number of days past due on CB credit at the time of application for related loan in our sample                                                                                                                                                                                         |                                       | \n",
    "| 131 | bureau.csv                    | DAYS_CREDIT_ENDDATE          | Remaining duration of CB credit (in days) at the time of application in Home Credit                                                                                                                                                                                                    | time only relative to the application | \n",
    "| 132 | bureau.csv                    | DAYS_ENDDATE_FACT            | Days since CB credit ended at the time of application in Home Credit (only for closed credit)                                                                                                                                                                                          | time only relative to the application | \n",
    "| 133 | bureau.csv                    | AMT_CREDIT_MAX_OVERDUE       | Maximal amount overdue on the Credit Bureau credit so far (at application date of loan in our sample)                                                                                                                                                                                  |                                       | \n",
    "| 134 | bureau.csv                    | CNT_CREDIT_PROLONG           | How many times was the Credit Bureau credit prolonged                                                                                                                                                                                                                                  |                                       | \n",
    "| 135 | bureau.csv                    | AMT_CREDIT_SUM               | Current credit amount for the Credit Bureau credit                                                                                                                                                                                                                                     |                                       | \n",
    "| 136 | bureau.csv                    | AMT_CREDIT_SUM_DEBT          | Current debt on Credit Bureau credit                                                                                                                                                                                                                                                   |                                       | \n",
    "| 137 | bureau.csv                    | AMT_CREDIT_SUM_LIMIT         | Current credit limit of credit card reported in Credit Bureau                                                                                                                                                                                                                          |                                       | \n",
    "| 138 | bureau.csv                    | AMT_CREDIT_SUM_OVERDUE       | Current amount overdue on Credit Bureau credit                                                                                                                                                                                                                                         |                                       | \n",
    "| 139 | bureau.csv                    | CREDIT_TYPE                  | \"Type of Credit Bureau credit (Car, cash,...)\"                                                                                                                                                                                                                                         |                                       | \n",
    "| 140 | bureau.csv                    | DAYS_CREDIT_UPDATE           | How many days before loan application did last information about the Credit Bureau credit come                                                                                                                                                                                         | time only relative to the application | \n",
    "| 141 | bureau.csv                    | AMT_ANNUITY                  | Annuity of the Credit Bureau credit                                                                                                                                                                                                                                                    |                                       | \n",
    "| 142 | bureau_balance.csv            | SK_BUREAU_ID                 | Recoded ID of Credit Bureau credit (unique coding for each application) - use this to join to CREDIT_BUREAU table                                                                                                                                                                      | hashed                                | \n",
    "| 143 | bureau_balance.csv            | MONTHS_BALANCE               | Month of balance relative to application date (-1 means the freshest balance date)                                                                                                                                                                                                     | time only relative to the application | \n",
    "| 144 | bureau_balance.csv            | STATUS                       | \"Status of Credit Bureau loan during the month (active, closed, DPD0-30,",
    " [C means closed, X means status unknown, 0 means no DPD, 1 means maximal did during month between 1-30, 2 means DPD 31-60,",
    " 5 means DPD 120+ or sold or written off ] )\"                                     |                                       | \n",
    "| 145 | POS_CASH_balance.csv          | SK_ID_PREV                   | \"ID of previous credit in Home Credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)\"                                                                                                                                    |                                       | \n",
    "| 146 | POS_CASH_balance.csv          | SK_ID_CURR                   | ID of loan in our sample                                                                                                                                                                                                                                                               |                                       | \n",
    "| 147 | POS_CASH_balance.csv          | MONTHS_BALANCE               | \"Month of balance relative to application date (-1 means the information to the freshest monthly snapshot, 0 means the information at application - often it will be the same as -1 as many banks are not updating the information to Credit Bureau regularly )\"                       | time only relative to the application | \n",
    "| 148 | POS_CASH_balance.csv          | CNT_INSTALMENT               | Term of previous credit (can change over time)                                                                                                                                                                                                                                         |                                       | \n",
    "| 149 | POS_CASH_balance.csv          | CNT_INSTALMENT_FUTURE        | Installments left to pay on the previous credit                                                                                                                                                                                                                                        |                                       | \n",
    "| 150 | POS_CASH_balance.csv          | NAME_CONTRACT_STATUS         | Contract status during the month                                                                                                                                                                                                                                                       |                                       | \n",
    "| 151 | POS_CASH_balance.csv          | SK_DPD                       | DPD (days past due) during the month of previous credit                                                                                                                                                                                                                                |                                       | \n",
    "| 152 | POS_CASH_balance.csv          | SK_DPD_DEF                   | DPD during the month with tolerance (debts with low loan amounts are ignored) of the previous credit                                                                                                                                                                                   |                                       | \n",
    "| 153 | credit_card_balance.csv       | SK_ID_PREV                   | \"ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)\"                                                                                                                                    | hashed                                | \n",
    "| 154 | credit_card_balance.csv       | SK_ID_CURR                   | ID of loan in our sample                                                                                                                                                                                                                                                               | hashed                                | \n",
    "| 155 | credit_card_balance.csv       | MONTHS_BALANCE               | Month of balance relative to application date (-1 means the freshest balance date)                                                                                                                                                                                                     | time only relative to the application | \n",
    "| 156 | credit_card_balance.csv       | AMT_BALANCE                  | Balance during the month of previous credit                                                                                                                                                                                                                                            |                                       | \n",
    "| 157 | credit_card_balance.csv       | AMT_CREDIT_LIMIT_ACTUAL      | Credit card limit during the month of the previous credit                                                                                                                                                                                                                              |                                       | \n",
    "| 158 | credit_card_balance.csv       | AMT_DRAWINGS_ATM_CURRENT     | Amount drawing at ATM during the month of the previous credit                                                                                                                                                                                                                          |                                       | \n",
    "| 159 | credit_card_balance.csv       | AMT_DRAWINGS_CURRENT         | Amount drawing during the month of the previous credit                                                                                                                                                                                                                                 |                                       | \n",
    "| 160 | credit_card_balance.csv       | AMT_DRAWINGS_OTHER_CURRENT   | Amount of other drawings during the month of the previous credit                                                                                                                                                                                                                       |                                       | \n",
    "| 161 | credit_card_balance.csv       | AMT_DRAWINGS_POS_CURRENT     | Amount drawing or buying goods during the month of the previous credit                                                                                                                                                                                                                 |                                       | \n",
    "| 162 | credit_card_balance.csv       | AMT_INST_MIN_REGULARITY      | Minimal installment for this month of the previous credit                                                                                                                                                                                                                              |                                       | \n",
    "| 163 | credit_card_balance.csv       | AMT_PAYMENT_CURRENT          | How much did the client pay during the month on the previous credit                                                                                                                                                                                                                    |                                       | \n",
    "| 164 | credit_card_balance.csv       | AMT_PAYMENT_TOTAL_CURRENT    | How much did the client pay during the month in total on the previous credit                                                                                                                                                                                                           |                                       | \n",
    "| 165 | credit_card_balance.csv       | AMT_RECEIVABLE_PRINCIPAL     | Amount receivable for principal on the previous credit                                                                                                                                                                                                                                 |                                       | \n",
    "| 166 | credit_card_balance.csv       | AMT_RECIVABLE                | Amount receivable on the previous credit                                                                                                                                                                                                                                               |                                       | \n",
    "| 167 | credit_card_balance.csv       | AMT_TOTAL_RECEIVABLE         | Total amount receivable on the previous credit                                                                                                                                                                                                                                         |                                       | \n",
    "| 168 | credit_card_balance.csv       | CNT_DRAWINGS_ATM_CURRENT     | Number of drawings at ATM during this month on the previous credit                                                                                                                                                                                                                     |                                       | \n",
    "| 169 | credit_card_balance.csv       | CNT_DRAWINGS_CURRENT         | Number of drawings during this month on the previous credit                                                                                                                                                                                                                            |                                       | \n",
    "| 170 | credit_card_balance.csv       | CNT_DRAWINGS_OTHER_CURRENT   | Number of other drawings during this month on the previous credit                                                                                                                                                                                                                      |                                       | \n",
    "| 171 | credit_card_balance.csv       | CNT_DRAWINGS_POS_CURRENT     | Number of drawings for goods during this month on the previous credit                                                                                                                                                                                                                  |                                       | \n",
    "| 172 | credit_card_balance.csv       | CNT_INSTALMENT_MATURE_CUM    | Number of paid installments on the previous credit                                                                                                                                                                                                                                     |                                       | \n",
    "| 173 | credit_card_balance.csv       | NAME_CONTRACT_STATUS         | \"Contract status (active signed,...) on the previous credit\"                                                                                                                                                                                                                           |                                       | \n",
    "| 174 | credit_card_balance.csv       | SK_DPD                       | DPD (Days past due) during the month on the previous credit                                                                                                                                                                                                                            |                                       | \n",
    "| 175 | credit_card_balance.csv       | SK_DPD_DEF                   | DPD (Days past due) during the month with tolerance (debts with low loan amounts are ignored) of the previous credit                                                                                                                                                                   |                                       | \n",
    "| 176 | previous_application.csv      | SK_ID_PREV                   | \"ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loan applications in Home Credit, previous application could, but not necessarily have to lead to credit) \"                                               | hashed                                | \n",
    "| 177 | previous_application.csv      | SK_ID_CURR                   | ID of loan in our sample                                                                                                                                                                                                                                                               | hashed                                | \n",
    "| 178 | previous_application.csv      | NAME_CONTRACT_TYPE           | \"Contract product type (Cash loan, consumer loan [POS] ,...) of the previous application\"                                                                                                                                                                                              |                                       | \n",
    "| 179 | previous_application.csv      | AMT_ANNUITY                  | Annuity of previous application                                                                                                                                                                                                                                                        |                                       | \n",
    "| 180 | previous_application.csv      | AMT_APPLICATION              | For how much credit did client ask on the previous application                                                                                                                                                                                                                         |                                       | \n",
    "| 181 | previous_application.csv      | AMT_CREDIT                   | \"Final credit amount on the previous application. This differs from AMT_APPLICATION in a way that the AMT_APPLICATION is the amount for which the client initially applied for, but during our approval process he could have received different amount - AMT_CREDIT\"                  |                                       | \n",
    "| 182 | previous_application.csv      | AMT_DOWN_PAYMENT             | Down payment on the previous application                                                                                                                                                                                                                                               |                                       | \n",
    "| 183 | previous_application.csv      | AMT_GOODS_PRICE              | Goods price of good that client asked for (if applicable) on the previous application                                                                                                                                                                                                  |                                       | \n",
    "| 184 | previous_application.csv      | WEEKDAY_APPR_PROCESS_START   | On which day of the week did the client apply for previous application                                                                                                                                                                                                                 |                                       | \n",
    "| 185 | previous_application.csv      | HOUR_APPR_PROCESS_START      | Approximately at what day hour did the client apply for the previous application                                                                                                                                                                                                       | rounded                               | \n",
    "| 186 | previous_application.csv      | FLAG_LAST_APPL_PER_CONTRACT  | Flag if it was last application for the previous contract. Sometimes by mistake of client or our clerk there could be more applications for one single contract                                                                                                                        |                                       | \n",
    "| 187 | previous_application.csv      | NFLAG_LAST_APPL_IN_DAY       | Flag if the application was the last application per day of the client. Sometimes clients apply for more applications a day. Rarely it could also be error in our system that one application is in the database twice                                                                 |                                       | \n",
    "| 188 | previous_application.csv      | NFLAG_MICRO_CASH             | Flag Micro finance loan                                                                                                                                                                                                                                                                |                                       | \n",
    "| 189 | previous_application.csv      | RATE_DOWN_PAYMENT            | Down payment rate normalized on previous credit                                                                                                                                                                                                                                        | normalized                            | \n",
    "| 190 | previous_application.csv      | RATE_INTEREST_PRIMARY        | Interest rate normalized on previous credit                                                                                                                                                                                                                                            | normalized                            | \n",
    "| 191 | previous_application.csv      | RATE_INTEREST_PRIVILEGED     | Interest rate normalized on previous credit                                                                                                                                                                                                                                            | normalized                            | \n",
    "| 192 | previous_application.csv      | NAME_CASH_LOAN_PURPOSE       | Purpose of the cash loan                                                                                                                                                                                                                                                               |                                       | \n",
    "| 193 | previous_application.csv      | NAME_CONTRACT_STATUS         | \"Contract status (approved, cancelled, ...) of previous application\"                                                                                                                                                                                                                   |                                       | \n",
    "| 194 | previous_application.csv      | DAYS_DECISION                | Relative to current application when was the decision about previous application made                                                                                                                                                                                                  | time only relative to the application | \n",
    "| 195 | previous_application.csv      | NAME_PAYMENT_TYPE            | Payment method that client chose to pay for the previous application                                                                                                                                                                                                                   |                                       | \n",
    "| 196 | previous_application.csv      | CODE_REJECT_REASON           | Why was the previous application rejected                                                                                                                                                                                                                                              |                                       | \n",
    "| 197 | previous_application.csv      | NAME_TYPE_SUITE              | Who accompanied client when applying for the previous application                                                                                                                                                                                                                      |                                       | \n",
    "| 198 | previous_application.csv      | NAME_CLIENT_TYPE             | Was the client old or new client when applying for the previous application                                                                                                                                                                                                            |                                       | \n",
    "| 199 | previous_application.csv      | NAME_GOODS_CATEGORY          | What kind of goods did the client apply for in the previous application                                                                                                                                                                                                                |                                       | \n",
    "| 200 | previous_application.csv      | NAME_PORTFOLIO               | \"Was the previous application for CASH, POS, CAR, ",
    "\"                                                                                                                                                                                                                                   |                                       | \n",
    "| 201 | previous_application.csv      | NAME_PRODUCT_TYPE            | Was the previous application x-sell o walk-in                                                                                                                                                                                                                                          |                                       | \n",
    "| 202 | previous_application.csv      | CHANNEL_TYPE                 | Through which channel we acquired the client on the previous application                                                                                                                                                                                                               |                                       | \n",
    "| 203 | previous_application.csv      | SELLERPLACE_AREA             | Selling area of seller place of the previous application                                                                                                                                                                                                                               |                                       | \n",
    "| 204 | previous_application.csv      | NAME_SELLER_INDUSTRY         | The industry of the seller                                                                                                                                                                                                                                                             |                                       | \n",
    "| 205 | previous_application.csv      | CNT_PAYMENT                  | Term of previous credit at application of the previous application                                                                                                                                                                                                                     |                                       | \n",
    "| 206 | previous_application.csv      | NAME_YIELD_GROUP             | Grouped interest rate into small medium and high of the previous application                                                                                                                                                                                                           | grouped                               | \n",
    "| 207 | previous_application.csv      | PRODUCT_COMBINATION          | Detailed product combination of the previous application                                                                                                                                                                                                                               |                                       | \n",
    "| 208 | previous_application.csv      | DAYS_FIRST_DRAWING           | Relative to application date of current application when was the first disbursement of the previous application                                                                                                                                                                        | time only relative to the application | \n",
    "| 209 | previous_application.csv      | DAYS_FIRST_DUE               | Relative to application date of current application when was the first due supposed to be of the previous application                                                                                                                                                                  | time only relative to the application | \n",
    "| 210 | previous_application.csv      | DAYS_LAST_DUE_1ST_VERSION    | Relative to application date of current application when was the first due of the previous application                                                                                                                                                                                 | time only relative to the application | \n",
    "| 211 | previous_application.csv      | DAYS_LAST_DUE                | Relative to application date of current application when was the last due date of the previous application                                                                                                                                                                             | time only relative to the application | \n",
    "| 212 | previous_application.csv      | DAYS_TERMINATION             | Relative to application date of current application when was the expected termination of the previous application                                                                                                                                                                      | time only relative to the application | \n",
    "| 213 | previous_application.csv      | NFLAG_INSURED_ON_APPROVAL    | Did the client requested insurance during the previous application                                                                                                                                                                                                                     |                                       | \n",
    "| 214 | installments_payments.csv     | SK_ID_PREV                   | \"ID of previous credit in Home credit related to loan in our sample. (One loan in our sample can have 0,1,2 or more previous loans in Home Credit)\"                                                                                                                                    | hashed                                | \n",
    "| 215 | installments_payments.csv     | SK_ID_CURR                   | ID of loan in our sample                                                                                                                                                                                                                                                               | hashed                                | \n",
    "| 216 | installments_payments.csv     | NUM_INSTALMENT_VERSION       | Version of installment calendar (0 is for credit card) of previous credit. Change of installment version from month to month signifies that some parameter of payment calendar has changed                                                                                             |                                       | \n",
    "| 217 | installments_payments.csv     | NUM_INSTALMENT_NUMBER        | On which installment we observe payment                                                                                                                                                                                                                                                |                                       | \n",
    "| 218 | installments_payments.csv     | DAYS_INSTALMENT              | When the installment of previous credit was supposed to be paid (relative to application date of current loan)                                                                                                                                                                         | time only relative to the application | \n",
    "| 219 | installments_payments.csv     | DAYS_ENTRY_PAYMENT           | When was the installments of previous credit paid actually (relative to application date of current loan)                                                                                                                                                                              | time only relative to the application | \n",
    "| 220 | installments_payments.csv     | AMT_INSTALMENT               | What was the prescribed installment amount of previous credit on this installment                                                                                                                                                                                                      |                                       | \n",
    "| 221 | installments_payments.csv     | AMT_PAYMENT                  | What the client actually paid on previous credit on this installment                                                                                                                                                                                                                   |                                       | \n",
    "|     |                               |                              |                                                                                                                                                                                                                                                                                        |                                       | \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329.625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
